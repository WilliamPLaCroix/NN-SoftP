{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "#from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizerFast\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# authentication with huggingface\n",
    "# create token at https://huggingface.co/settings/tokens (create as read)\n",
    "# token should be stored locally, so technically login is only needed one time\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download specific dataset files; entire archive is only ~250 mb\n",
    "# articles = load_dataset(\"Cofacts/line-msg-fact-check-tw\", \"articles\") # article contents and some meta info\n",
    "# article_replies = load_dataset(\"Cofacts/line-msg-fact-check-tw\", \"article_replies\") # 'join table' for articles and replies with added meta info\n",
    "\n",
    "# open from csv\n",
    "articles = pd.read_csv('./data/articles.csv',lineterminator='\\n')\n",
    "article_replies = pd.read_csv('./data/article_replies.csv')\n",
    "art_rep_df = pd.merge(articles, article_replies, left_on=\"id\", right_on=\"articleId\", how=\"left\")\n",
    "art_rep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(texts: list):\n",
    "    max_length = 0\n",
    "    for t in texts:\n",
    "        if t:\n",
    "            max_length = max(max_length, len(t))\n",
    "    return max_length\n",
    "\n",
    "def inspect_dist(col: list, threshold = 0):\n",
    "    if threshold == 0:\n",
    "        plt.hist(col, bins=100)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.hist(col[col <= threshold], bins=100)\n",
    "        plt.show()\n",
    "        print(f\"Remaining datapoints: {len(col[col <= threshold]) / len(col)}\")\n",
    "\n",
    "def remove_url(text: str):\n",
    "    text = re.sub(r\"https?://(?:www\\.)?\\w+(?:\\.\\w+)+(?:/\\S*)?\", \"\", text)\n",
    "    if len(text) == 0:\n",
    "        text = pd.NA\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>articleType</th>\n",
       "      <th>status_x</th>\n",
       "      <th>text</th>\n",
       "      <th>normalArticleReplyCount</th>\n",
       "      <th>createdAt_x</th>\n",
       "      <th>updatedAt_x</th>\n",
       "      <th>lastRequestedAt</th>\n",
       "      <th>userIdsha256_x</th>\n",
       "      <th>appId_x</th>\n",
       "      <th>...</th>\n",
       "      <th>articleId</th>\n",
       "      <th>replyId</th>\n",
       "      <th>userIdsha256_y</th>\n",
       "      <th>negativeFeedbackCount</th>\n",
       "      <th>positiveFeedbackCount</th>\n",
       "      <th>replyType</th>\n",
       "      <th>appId_y</th>\n",
       "      <th>status_y</th>\n",
       "      <th>createdAt_y</th>\n",
       "      <th>updatedAt_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>326xjpkjbf01i</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://youtu.be/xK9NzL3PkdE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-08T12:34:27.602Z</td>\n",
       "      <td>2022-09-08T12:34:27.602Z</td>\n",
       "      <td>2022-09-08T12:34:27.634Z</td>\n",
       "      <td>243b5897c14f02fb5b92a9e4f4cc39d5fb84ff16173add...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2ihx6b0hy6reh</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://fb.watch/fqyCukx77O/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-09T08:30:45.294Z</td>\n",
       "      <td>2022-09-09T08:30:45.294Z</td>\n",
       "      <td>2022-09-09T08:30:45.321Z</td>\n",
       "      <td>67cb002cbd4884b64f03aca3400e67a867b35e3838caa3...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2zvytlna89voa</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://youtu.be/EHvxmzP7Mfw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-11T00:24:14.167Z</td>\n",
       "      <td>2022-09-11T00:24:14.167Z</td>\n",
       "      <td>2022-09-11T00:24:14.197Z</td>\n",
       "      <td>1b8841d66c76e5ebbce83a71ddfa61fea29c32a736218f...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>32745wxu27y2q</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://youtu.be/BSh2Lj_OrDs</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-11T06:24:59.763Z</td>\n",
       "      <td>2022-09-11T06:24:59.763Z</td>\n",
       "      <td>2022-09-11T06:24:59.791Z</td>\n",
       "      <td>9fdd5c5952dbbe145c4f00ca018fe0979e5875fbac45fe...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1ppbai7d2l96o</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://youtu.be/3nX_ctdukOI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-11T12:29:43.754Z</td>\n",
       "      <td>2022-09-11T12:29:43.754Z</td>\n",
       "      <td>2022-09-11T12:29:43.786Z</td>\n",
       "      <td>5e19fe6083cf02075ff2e022e526a4b09d4958780174d3...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168960</th>\n",
       "      <td>h9io4zcc5wbg</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>了解 先自我介紹一下 我是淞瑋包裝材料行徵工代表：葉佳倩</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-01-20T06:44:26.971Z</td>\n",
       "      <td>2024-01-20T06:44:26.971Z</td>\n",
       "      <td>2024-01-25T08:35:51.328Z</td>\n",
       "      <td>17295e1f0761fd28c74ff00632d2dd29dbf37baf42f8f0...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>h9io4zcc5wbg</td>\n",
       "      <td>qPc9Qo0BAjOeMOklzqEu</td>\n",
       "      <td>7708d900e0a9d4a92ad69381b1e0782338787e027954ce...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RUMOR</td>\n",
       "      <td>WEBSITE</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>2024-01-25T20:09:23.249Z</td>\n",
       "      <td>2024-01-25T20:09:23.249Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169040</th>\n",
       "      <td>3dl8g74u3exnz</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>要去app sotre搜尋COINCENTEL載到桌布喔</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-26T12:11:53.760Z</td>\n",
       "      <td>2024-01-26T12:11:53.760Z</td>\n",
       "      <td>2024-01-26T12:11:53.777Z</td>\n",
       "      <td>8ab6707dd0821b0605969c1bc0481d3ca0bbc87953c9ee...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169313</th>\n",
       "      <td>22vlxdwzs7l7d</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://txp.rs/v/ACr5NfeCEn2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-26T06:04:22.281Z</td>\n",
       "      <td>2024-01-26T06:04:22.281Z</td>\n",
       "      <td>2024-01-26T06:04:22.293Z</td>\n",
       "      <td>63318d9d6b88b75fac18d8bdb41a5b38ffa759576210f3...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169315</th>\n",
       "      <td>ok8jtnk1pj9g</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>https://youtu.be/UwtXzwckeWQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-05-18T04:27:21.707Z</td>\n",
       "      <td>2023-05-18T04:27:21.707Z</td>\n",
       "      <td>2024-01-26T07:19:52.516Z</td>\n",
       "      <td>e87d917e34b761d233273ebd5cf339be6c92c0920ceafc...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169445</th>\n",
       "      <td>2d0n253o8cv7d</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>先介紹一下   我是唐三手工代工有限公司的徵工專員吳詩婷</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-11-20T07:14:29.037Z</td>\n",
       "      <td>2023-11-20T07:14:29.037Z</td>\n",
       "      <td>2024-01-11T11:47:14.051Z</td>\n",
       "      <td>8d2225d5e4f4fe878722f1fa65d221e54e2b57bca5b792...</td>\n",
       "      <td>RUMORS_LINE_BOT</td>\n",
       "      <td>...</td>\n",
       "      <td>2d0n253o8cv7d</td>\n",
       "      <td>vfdEQo0BAjOeMOklcqG4</td>\n",
       "      <td>7708d900e0a9d4a92ad69381b1e0782338787e027954ce...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RUMOR</td>\n",
       "      <td>WEBSITE</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>2024-01-25T20:16:38.587Z</td>\n",
       "      <td>2024-01-25T20:16:38.587Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5161 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id articleType status_x                          text  \\\n",
       "1       326xjpkjbf01i        TEXT   NORMAL  https://youtu.be/xK9NzL3PkdE   \n",
       "50      2ihx6b0hy6reh        TEXT   NORMAL  https://fb.watch/fqyCukx77O/   \n",
       "115     2zvytlna89voa        TEXT   NORMAL  https://youtu.be/EHvxmzP7Mfw   \n",
       "132     32745wxu27y2q        TEXT   NORMAL  https://youtu.be/BSh2Lj_OrDs   \n",
       "160     1ppbai7d2l96o        TEXT   NORMAL  https://youtu.be/3nX_ctdukOI   \n",
       "...               ...         ...      ...                           ...   \n",
       "168960   h9io4zcc5wbg        TEXT   NORMAL  了解 先自我介紹一下 我是淞瑋包裝材料行徵工代表：葉佳倩   \n",
       "169040  3dl8g74u3exnz        TEXT   NORMAL  要去app sotre搜尋COINCENTEL載到桌布喔   \n",
       "169313  22vlxdwzs7l7d        TEXT   NORMAL  https://txp.rs/v/ACr5NfeCEn2   \n",
       "169315   ok8jtnk1pj9g        TEXT   NORMAL  https://youtu.be/UwtXzwckeWQ   \n",
       "169445  2d0n253o8cv7d        TEXT   NORMAL  先介紹一下   我是唐三手工代工有限公司的徵工專員吳詩婷   \n",
       "\n",
       "        normalArticleReplyCount               createdAt_x  \\\n",
       "1                           0.0  2022-09-08T12:34:27.602Z   \n",
       "50                          0.0  2022-09-09T08:30:45.294Z   \n",
       "115                         0.0  2022-09-11T00:24:14.167Z   \n",
       "132                         0.0  2022-09-11T06:24:59.763Z   \n",
       "160                         0.0  2022-09-11T12:29:43.754Z   \n",
       "...                         ...                       ...   \n",
       "168960                      1.0  2024-01-20T06:44:26.971Z   \n",
       "169040                      0.0  2024-01-26T12:11:53.760Z   \n",
       "169313                      0.0  2024-01-26T06:04:22.281Z   \n",
       "169315                      0.0  2023-05-18T04:27:21.707Z   \n",
       "169445                      1.0  2023-11-20T07:14:29.037Z   \n",
       "\n",
       "                     updatedAt_x           lastRequestedAt  \\\n",
       "1       2022-09-08T12:34:27.602Z  2022-09-08T12:34:27.634Z   \n",
       "50      2022-09-09T08:30:45.294Z  2022-09-09T08:30:45.321Z   \n",
       "115     2022-09-11T00:24:14.167Z  2022-09-11T00:24:14.197Z   \n",
       "132     2022-09-11T06:24:59.763Z  2022-09-11T06:24:59.791Z   \n",
       "160     2022-09-11T12:29:43.754Z  2022-09-11T12:29:43.786Z   \n",
       "...                          ...                       ...   \n",
       "168960  2024-01-20T06:44:26.971Z  2024-01-25T08:35:51.328Z   \n",
       "169040  2024-01-26T12:11:53.760Z  2024-01-26T12:11:53.777Z   \n",
       "169313  2024-01-26T06:04:22.281Z  2024-01-26T06:04:22.293Z   \n",
       "169315  2023-05-18T04:27:21.707Z  2024-01-26T07:19:52.516Z   \n",
       "169445  2023-11-20T07:14:29.037Z  2024-01-11T11:47:14.051Z   \n",
       "\n",
       "                                           userIdsha256_x          appId_x  \\\n",
       "1       243b5897c14f02fb5b92a9e4f4cc39d5fb84ff16173add...  RUMORS_LINE_BOT   \n",
       "50      67cb002cbd4884b64f03aca3400e67a867b35e3838caa3...  RUMORS_LINE_BOT   \n",
       "115     1b8841d66c76e5ebbce83a71ddfa61fea29c32a736218f...  RUMORS_LINE_BOT   \n",
       "132     9fdd5c5952dbbe145c4f00ca018fe0979e5875fbac45fe...  RUMORS_LINE_BOT   \n",
       "160     5e19fe6083cf02075ff2e022e526a4b09d4958780174d3...  RUMORS_LINE_BOT   \n",
       "...                                                   ...              ...   \n",
       "168960  17295e1f0761fd28c74ff00632d2dd29dbf37baf42f8f0...  RUMORS_LINE_BOT   \n",
       "169040  8ab6707dd0821b0605969c1bc0481d3ca0bbc87953c9ee...  RUMORS_LINE_BOT   \n",
       "169313  63318d9d6b88b75fac18d8bdb41a5b38ffa759576210f3...  RUMORS_LINE_BOT   \n",
       "169315  e87d917e34b761d233273ebd5cf339be6c92c0920ceafc...  RUMORS_LINE_BOT   \n",
       "169445  8d2225d5e4f4fe878722f1fa65d221e54e2b57bca5b792...  RUMORS_LINE_BOT   \n",
       "\n",
       "        ...      articleId               replyId  \\\n",
       "1       ...            NaN                   NaN   \n",
       "50      ...            NaN                   NaN   \n",
       "115     ...            NaN                   NaN   \n",
       "132     ...            NaN                   NaN   \n",
       "160     ...            NaN                   NaN   \n",
       "...     ...            ...                   ...   \n",
       "168960  ...   h9io4zcc5wbg  qPc9Qo0BAjOeMOklzqEu   \n",
       "169040  ...            NaN                   NaN   \n",
       "169313  ...            NaN                   NaN   \n",
       "169315  ...            NaN                   NaN   \n",
       "169445  ...  2d0n253o8cv7d  vfdEQo0BAjOeMOklcqG4   \n",
       "\n",
       "                                           userIdsha256_y  \\\n",
       "1                                                     NaN   \n",
       "50                                                    NaN   \n",
       "115                                                   NaN   \n",
       "132                                                   NaN   \n",
       "160                                                   NaN   \n",
       "...                                                   ...   \n",
       "168960  7708d900e0a9d4a92ad69381b1e0782338787e027954ce...   \n",
       "169040                                                NaN   \n",
       "169313                                                NaN   \n",
       "169315                                                NaN   \n",
       "169445  7708d900e0a9d4a92ad69381b1e0782338787e027954ce...   \n",
       "\n",
       "       negativeFeedbackCount  positiveFeedbackCount  replyType  appId_y  \\\n",
       "1                        NaN                    NaN        NaN      NaN   \n",
       "50                       NaN                    NaN        NaN      NaN   \n",
       "115                      NaN                    NaN        NaN      NaN   \n",
       "132                      NaN                    NaN        NaN      NaN   \n",
       "160                      NaN                    NaN        NaN      NaN   \n",
       "...                      ...                    ...        ...      ...   \n",
       "168960                   0.0                    0.0      RUMOR  WEBSITE   \n",
       "169040                   NaN                    NaN        NaN      NaN   \n",
       "169313                   NaN                    NaN        NaN      NaN   \n",
       "169315                   NaN                    NaN        NaN      NaN   \n",
       "169445                   0.0                    0.0      RUMOR  WEBSITE   \n",
       "\n",
       "       status_y               createdAt_y               updatedAt_y  \n",
       "1           NaN                       NaN                       NaN  \n",
       "50          NaN                       NaN                       NaN  \n",
       "115         NaN                       NaN                       NaN  \n",
       "132         NaN                       NaN                       NaN  \n",
       "160         NaN                       NaN                       NaN  \n",
       "...         ...                       ...                       ...  \n",
       "168960   NORMAL  2024-01-25T20:09:23.249Z  2024-01-25T20:09:23.249Z  \n",
       "169040      NaN                       NaN                       NaN  \n",
       "169313      NaN                       NaN                       NaN  \n",
       "169315      NaN                       NaN                       NaN  \n",
       "169445   NORMAL  2024-01-25T20:16:38.587Z  2024-01-25T20:16:38.587Z  \n",
       "\n",
       "[5161 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_nans = art_rep_df.dropna(subset=[\"text\", \"replyType\"])\n",
    "filter_articles = no_nans[no_nans[\"replyType\"] != \"NOT_ARTICLE\"]\n",
    "no_urls = filter_articles.copy()\n",
    "no_urls[\"text\"] = no_urls[\"text\"].apply(remove_url)\n",
    "no_urls = no_urls[no_urls['text'] != \"<NA>\"]\n",
    "#no_urls = no_urls.dropna(subset=[\"text\"])\n",
    "no_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5111 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "listed = no_urls[['text','replyType']].dropna().copy()\n",
    "listed['replyType'] = listed['replyType'].map(lambda x: x.replace(\"NOT_RUMOR\", \"0\"))\n",
    "listed['replyType'] = listed['replyType'].map(lambda x: x.replace(\"OPINIONATED\", \"1\"))\n",
    "listed['replyType'] = listed['replyType'].map(lambda x: x.replace(\"RUMOR\", \"2\"))\n",
    "listed['replyType'] = listed['replyType'].astype(int)\n",
    "listed['text'] = listed['text'].map(lambda x: torch.Tensor(tokenizer(x)['input_ids']))\n",
    "listed['replyType'] = listed['replyType'].map(lambda x: torch.tensor(x))\n",
    "padded = torch.nn.utils.rnn.pad_sequence(listed['text'], batch_first=True, padding_value=0)\n",
    "zipped = list(zip(padded, listed['replyType']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def k_fold_split_data(dataset, batch_size, k=5):\n",
    "#     n = len(dataset)\n",
    "#     fold_size = n // k\n",
    "#     folds = []\n",
    "\n",
    "#     def collate_fn(data):\n",
    "#         tensors, targets = zip(*data)\n",
    "#         features = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "#         targets = torch.stack(targets)\n",
    "#         return features, targets\n",
    "\n",
    "#     for i in range(k):\n",
    "#         start = i * fold_size\n",
    "#         end = (i + 1) * fold_size if i < k - 1 else n\n",
    "#         folds.append(torch.utils.data.Subset(dataset, range(start, end)))\n",
    "\n",
    "#     dataloaders = []\n",
    "#     for i in range(k):\n",
    "#         validation_dataset = folds[i]\n",
    "#         train_folds = [folds[j] for j in range(k) if j != i]\n",
    "#         train_dataset = torch.utils.data.ConcatDataset(train_folds)\n",
    "\n",
    "#         y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "\n",
    "#         global class_weights\n",
    "#         class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "#         class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "#         train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "#         validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "#         dataloaders.append((train_dataloader, validation_dataloader))\n",
    "\n",
    "#     return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_split_data(dataset, batch_size):\n",
    "    \n",
    "\n",
    "    #generator = torch.Generator().manual_seed(42)\n",
    "    #train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1], generator=generator)\n",
    "    train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "    y = torch.tensor([label for _, label in dataset], dtype=torch.long)\n",
    "\n",
    "    def collate_fn(data):\n",
    "        tensors, targets = zip(*data)\n",
    "        features = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "        targets = torch.stack(targets)\n",
    "        return features, targets\n",
    "\n",
    "    global class_weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneableModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, layer_size, dropout_rate, n_layers):\n",
    "        super(TuneableModel, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=layer_size, bidirectional=True, \n",
    "                                  num_layers=n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.output_layer = torch.nn.Linear(layer_size, 3)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(layer_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(layer_size*2, layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lstm(x)\n",
    "        x = self.activation(x[0])\n",
    "        x = self.linear(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, dataloader, optimizer, training=\"train\"):\n",
    "   \n",
    "    loss_function = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "    if training == \"train\":\n",
    "        model.train()\n",
    "    elif training == \"validation\":\n",
    "        model.eval()\n",
    "    elif training == \"test\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise ValueError(\"training argument must be either 'train', 'validation' or 'test'\")\n",
    "        \n",
    "    cumulative_loss = 0\n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    for sample in tqdm(dataloader):\n",
    "        input, targets = sample[0].float().to(device), sample[1].type(torch.LongTensor).to(device)\n",
    "        output = model(input).to(device)\n",
    "        loss_value = loss_function(output, targets)\n",
    "        cumulative_loss += loss_value.item()\n",
    "\n",
    "        if training == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.sum().backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        predictions = output.to('cpu').detach().numpy().argmax(axis=1)\n",
    "        target_labels = sample[1]\n",
    "        prediction_list.extend(predictions)\n",
    "        label_list.extend(target_labels)\n",
    "    #f1 = f1_score(label_list, prediction_list)\n",
    "    accuracy = accuracy_score(label_list, prediction_list)\n",
    "    #confusion = confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "    return cumulative_loss, accuracy#, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "def evaluate(params, dataset):\n",
    "    dropout, hidden_size, learning_rate, batch_size, n_hidden = params\n",
    "\n",
    "    max_epochs = 1000\n",
    "    max_patience = 5\n",
    "    seed = 42\n",
    "\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    train_dataloader, validation_dataloader, test_dataloader = randomly_split_data(dataset, batch_size)\n",
    "    #dataloaders = k_fold_split_data(dataset, batch_size, k=5)\n",
    "    # train_dataloader, validation_dataloader = dataloader[0], dataloader[1]\n",
    "    # test_dataloader = dataloader[1]\n",
    "    PATH = \"model_.pt\"\n",
    "    last_loss = 1000000\n",
    "    torch.manual_seed(seed)\n",
    "    input_size = train_dataloader.dataset[0][0].size()[0]\n",
    "    model = TuneableModel(input_size, hidden_size, dropout, n_hidden)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # training\n",
    "        train_loss, train_accuracy = train_test(model, train_dataloader, optimizer, training=\"train\")\n",
    "        train_loss, train_accuracy = train_loss, round(train_accuracy, 4)\n",
    "        # validation at end of epoch\n",
    "        validation_loss, validation_accuracy = train_test(model, validation_dataloader, optimizer, training=\"validation\")\n",
    "        validation_loss, validation_accuracy = validation_loss, round(validation_accuracy, 4)\n",
    "        if validation_loss < last_loss:\n",
    "            last_loss = validation_loss\n",
    "            current_patience = 0\n",
    "        else:\n",
    "            if current_patience == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': last_loss,\n",
    "                    }, PATH)\n",
    "            current_patience += 1\n",
    "        if current_patience == max_patience:\n",
    "            break   \n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch} validation loss: {validation_loss} validation accuracy: {validation_accuracy*100}%\")\n",
    "    # Testing once patience is reached\n",
    "    torch.manual_seed(seed)\n",
    "    model = TuneableModel(input_size, hidden_size, dropout, n_hidden)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    test_loss, test_accuracy = train_test(model, test_dataloader, optimizer, training=\"test\")\n",
    "    test_loss, test_accuracy = test_loss, test_accuracy\n",
    "    #print(f\"Model {i} at epoch {checkpoint['epoch']} test results: accuracy: {test_accuracy*100}% f1: {test_f1}\")\n",
    "    # accuracies.append(test_accuracy)\n",
    "    # f1s.append(test_f1)\n",
    "    #print(test_confusion)\n",
    "\n",
    "    return round(test_accuracy*100, )\n",
    "    # print(f\"Average accuracy: {round(np.mean(accuracies), 2)}%\")\n",
    "    # print(f\"Average f1: {round(np.mean(f1s), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:40<00:00,  5.22it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 validation loss: 29.66617774963379 validation accuracy: 22.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:39<00:00,  5.27it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation loss: 29.675812244415283 validation accuracy: 14.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:46<00:00,  4.49it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation loss: 29.692907094955444 validation accuracy: 62.86000000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:41<00:00,  5.07it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 17.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation loss: 29.75939631462097 validation accuracy: 15.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:39<00:00,  5.28it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation loss: 29.795060634613037 validation accuracy: 22.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:39<00:00,  5.27it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 17.81it/s]\n",
      "100%|██████████| 27/27 [00:01<00:00, 16.30it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m accuracy, f1 \u001b[38;5;241m=\u001b[39m evaluate(params)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-fold average accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "dataset = zipped\n",
    "params = (0.0, 256, 0.1, 256, 4)\n",
    "accuracy= evaluate(params, dataset)\n",
    "print(f\"final test accuracy: {accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
