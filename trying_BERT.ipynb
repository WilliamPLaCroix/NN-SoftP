{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\William\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "API_TOKEN = \"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\"\n",
    "login(token=API_TOKEN)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb00ff812054a32a26544273d846cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f73a5f0b4b14612918492e39540ab75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506015720ca54162b02cdd9f6dc1d4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17c7ebd083c42cbaf41c88d9f10f649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cea91c9f101420188ed281251ddf6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f7e1052a8e48b89bdc3ffd1173f335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fd20f77bef438e87597eabdafb984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207a978528a04fdfb55500d3b58920da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\").to(device)\n",
    "\n",
    "input = \"this is a sample input\"\n",
    "\n",
    "\n",
    "# send input to tensor\n",
    "tokenized_input = tokenizer(input, return_tensors='pt').to(device)\n",
    "print(\"tokenize input\")\n",
    "print(tokenized_input)\n",
    "embeddings = model(**tokenized_input)[0]\n",
    "print(\"get bert embeddings\")\n",
    "print(\"\\t\", embeddings.shape)\n",
    "suprisal_values = torch.Tensor(np.random.uniform(0, 1, (1, embeddings.shape[1]))).to(device)\n",
    "print(\"get suprisal values\")\n",
    "print(\"\\t\", suprisal_values.shape)\n",
    "input_features = torch.cat((embeddings, suprisal_values.unsqueeze(2)), dim=2)\n",
    "print(\"add suprisial values to embeddings\")\n",
    "print(\"\\t\", input_features.shape)\n",
    "input_size = input_features.shape[2]\n",
    "\n",
    "hidden_size = 100\n",
    "dropout = 0\n",
    "classes = 2\n",
    "num_layers = 1\n",
    "\n",
    "lstm_layer = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=False, \n",
    "                                  num_layers=num_layers, batch_first=True, dropout=dropout, proj_size=1).to(device)\n",
    "lstm_output = lstm_layer(input_features)[0].squeeze(2)\n",
    "print(\"run input through lstm\")\n",
    "print(\"\\t\", lstm_output.shape)\n",
    "sentiment_score = torch.Tensor(np.random.uniform(0, 1, (1, 3))).to(device)\n",
    "print(\"run input through sentiment classifier\")\n",
    "print(\"\\t\", sentiment_score.shape)\n",
    "\n",
    "# add sentiment score to lstm output\n",
    "combined_output = torch.cat((lstm_output, sentiment_score), dim=1)\n",
    "print(\"add sentiment score to lstm output\")\n",
    "print(\"\\t\", combined_output.shape)\n",
    "\n",
    "linear_layer = torch.nn.Linear(combined_output.shape[1], classes).to(device)\n",
    "linear_output = linear_layer(combined_output)\n",
    "print(\"run combined output through linear layer\")\n",
    "print(\"\\t\", linear_output.shape)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "probabilities = softmax(linear_output)\n",
    "print(\"get probabilities\")\n",
    "print(\"\\t\", probabilities)\n",
    "prediction = torch.argmax(probabilities, dim=1)\n",
    "print(\"get prediction\")\n",
    "print(f\"label:\", prediction.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
