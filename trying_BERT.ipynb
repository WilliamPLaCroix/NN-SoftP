{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0JLGz7_MbmX",
        "outputId": "c556448a-2845-4706-b26a-5714e334ec4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to C:\\Users\\William\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel, DataCollatorWithPadding\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "API_TOKEN = \"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\"\n",
        "login(token=API_TOKEN)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "BERT = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataset = load_dataset(\"liar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z_WdA5gFMbmb"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    CustomDataset is a class for creating a dataset in PyTorch, inheriting from the PyTorch Dataset class.\n",
        "    This dataset is designed to handle tabular data provided as pandas DataFrames.\n",
        "\n",
        "    Attributes:\n",
        "        features (pd.DataFrame): A DataFrame containing the features of the dataset.\n",
        "        labels (pd.Series or pd.DataFrame): A Series or DataFrame containing the labels of the dataset.\n",
        "    Methods:\n",
        "        __getitem__(self, index): Returns the features and label for a given index.\n",
        "        __len__(self): Returns the total number of samples in the dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, features, labels):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            features (pd.DataFrame): The features of the dataset.\n",
        "            labels (pd.Series or pd.DataFrame): The labels of the dataset.\n",
        "        \"\"\"\n",
        "        self.features = pd.DataFrame(features)\n",
        "        self.labels = pd.DataFrame(labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            index (int): The index of the item to retrieve.\n",
        "        Returns:\n",
        "            tuple: A tuple containing the features as a numpy array and the label.\n",
        "        \"\"\"\n",
        "        features = self.features.iloc[index].to_numpy()\n",
        "        label = [self.labels.iloc[index]]\n",
        "        return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: The total number of samples.\n",
        "        \"\"\"\n",
        "        return len(self.features)\n",
        "\n",
        "def tokenize(data):\n",
        "    return tokenizer(data[\"statement\"], truncation=True, max_length=512, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXJInw3phqVj",
        "outputId": "55c0a4c7-adb9-4a61-879f-450b89e8aa57"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c133c0d654314bf0a55dc81a2a21dfe5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/759 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "653210e7f66d4a83831430be0230d2fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ba4bce40cd74cf080f0316e97263d10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10eeb55a47c44b10beb813b8156cacd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a6d05b948ef40f1b2e685d515f5b2ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a33fa346b0a34838b78396ef41b2e42e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\William\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "distilled_student_sentiment_classifier = pipeline(\n",
        "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
        "    return_all_scores=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUyj-JMhMbmd",
        "outputId": "87a04ee4-b8f0-4667-89b9-b2b3356733ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1284 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1284/1284 [01:42<00:00, 12.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1284 1284\n",
            "Dataset({\n",
            "    features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment'],\n",
            "    num_rows: 1284\n",
            "})\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecfbc6542e8f41f4a42b4a8bd69e2013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1284 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train = dataset[\"validation\"]\n",
        "\n",
        "sentiments_list = []\n",
        "\n",
        "for statement in tqdm(train[\"statement\"]):\n",
        "    scores = distilled_student_sentiment_classifier(statement)[0]\n",
        "    sentiments = [sentiment[\"score\"] for sentiment in scores]\n",
        "    sentiments_list.append(sentiments)\n",
        "\n",
        "print(len(sentiments_list), len(train[\"statement\"]))\n",
        "\n",
        "train = train.add_column(\"sentiment\", sentiments_list)\n",
        "\n",
        "print(train)\n",
        "\n",
        "\n",
        "tokenized_dataset = train.map(tokenize, batch_size=batch_size, batched=True)\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'sentiment'])\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "train_dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMVt6cIeMbmd",
        "outputId": "1d159a36-57d7-4303-8854-d9abecea63fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['sentiment', 'input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "print(next(iter(train_dataloader)).keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e5SqpD90Mbme"
      },
      "outputs": [],
      "source": [
        "# custom NN model with BERT embeddings\n",
        "\n",
        "class BERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.requires_grad_(False)\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.proj_size = 20\n",
        "        self.hidden_size = 100\n",
        "        self.lstm = torch.nn.LSTM(input_size=768, hidden_size=self.hidden_size, num_layers=2, batch_first=True, bidirectional=False, proj_size=self.proj_size)\n",
        "        self.classifier = torch.nn.Linear(self.proj_size+3, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, sentiment):\n",
        "        # dummy forward pass, not real architecture\n",
        "        outputs = self.bert(input_ids, attention_mask).last_hidden_state\n",
        "        outputs = self.lstm(outputs)[0][:,-1]\n",
        "        # insert classification layers here\n",
        "        # surprisal, sentiment, etc.\n",
        "        outputs = self.classifier(torch.cat((outputs, sentiment), dim=1))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3GJvhG6Mbme",
        "outputId": "6f0efe5c-7c6f-4786-e5f5-6a51764514cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11it [01:43,  9.38s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# this is not working\u001b[39;00m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     29\u001b[0m targets\u001b[38;5;241m.\u001b[39mextend(labels)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# simple training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = BERTClassifier(6).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "for i in range(100):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, batch in tqdm(enumerate(train_dataloader)):\n",
        "        batch.to(device)\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        sentiment = batch[\"sentiment\"]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, sentiment)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward() # this is not working\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        predictions.extend(outputs.detach().argmax(dim=1))\n",
        "        targets.extend(labels)\n",
        "        # for sample in zip(batch[\"labels\"], outputs.detach().argmax(dim=1)):\n",
        "        #     total += 1\n",
        "        #     if sample[0] == sample[1]:\n",
        "        #         correct += 1\n",
        "        batch.to('cpu')\n",
        "    total = len(targets)\n",
        "    correct = np.sum(np.array(predictions) == np.array(targets))\n",
        "    print(correct/total*100, np.mean(losses))\n",
        "    print(predictions)\n",
        "model.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hca_6mWRMbmf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# input = \"this is a sample input\"\n",
        "\n",
        "\n",
        "# # send input to tensor\n",
        "# tokenized_input = tokenizer(input, return_tensors='pt').to(device)\n",
        "# print(\"tokenize input\")\n",
        "# print(tokenized_input)\n",
        "# embeddings = BERT(**tokenized_input)[0]\n",
        "# print(\"get bert embeddings\")\n",
        "# print(\"\\t\", embeddings.shape)\n",
        "# suprisal_values = torch.Tensor(np.random.uniform(0, 1, (1, embeddings.shape[1]))).to(device)\n",
        "# print(\"get suprisal values\")\n",
        "# print(\"\\t\", suprisal_values.shape)\n",
        "# input_features = torch.cat((embeddings, suprisal_values.unsqueeze(2)), dim=2)\n",
        "# print(\"add suprisial values to embeddings\")\n",
        "# print(\"\\t\", input_features.shape)\n",
        "# input_size = input_features.shape[2]\n",
        "\n",
        "# hidden_size = 100\n",
        "# dropout = 0\n",
        "# classes = 2\n",
        "# num_layers = 1\n",
        "\n",
        "# lstm_layer = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=False,\n",
        "#                                   num_layers=num_layers, batch_first=True, dropout=dropout, proj_size=1).to(device)\n",
        "# lstm_output = lstm_layer(input_features)[0].squeeze(2)\n",
        "# print(\"run input through lstm\")\n",
        "# print(\"\\t\", lstm_output.shape)\n",
        "# sentiment_score = torch.Tensor(np.random.uniform(0, 1, (1, 3))).to(device)\n",
        "# print(\"run input through sentiment classifier\")\n",
        "# print(\"\\t\", sentiment_score.shape)\n",
        "\n",
        "# # add sentiment score to lstm output\n",
        "# combined_output = torch.cat((lstm_output, sentiment_score), dim=1)\n",
        "# print(\"add sentiment score to lstm output\")\n",
        "# print(\"\\t\", combined_output.shape)\n",
        "\n",
        "# linear_layer = torch.nn.Linear(combined_output.shape[1], classes).to(device)\n",
        "# linear_output = linear_layer(combined_output)\n",
        "# print(\"run combined output through linear layer\")\n",
        "# print(\"\\t\", linear_output.shape)\n",
        "# softmax = torch.nn.Softmax(dim=1)\n",
        "# probabilities = softmax(linear_output)\n",
        "# print(\"get probabilities\")\n",
        "# print(\"\\t\", probabilities)\n",
        "# prediction = torch.argmax(probabilities, dim=1)\n",
        "# print(\"get prediction\")\n",
        "# print(f\"label:\", prediction.item())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
