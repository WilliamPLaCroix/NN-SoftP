{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0JLGz7_MbmX",
        "outputId": "c556448a-2845-4706-b26a-5714e334ec4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to C:\\Users\\William\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\William\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, DataCollatorWithPadding, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "API_TOKEN = \"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\"\n",
        "login(token=API_TOKEN)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "distilled_student_sentiment_classifier = pipeline(\n",
        "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUyj-JMhMbmd",
        "outputId": "87a04ee4-b8f0-4667-89b9-b2b3356733ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# def add_sentiment_and_pickle(dataset, split):\n",
        "#     data = dataset[split]\n",
        "#     sentiments_list = []\n",
        "\n",
        "#     for statement in tqdm(data[\"statement\"]):\n",
        "#         scores = distilled_student_sentiment_classifier(statement)[0]\n",
        "#         sentiments = [sentiment[\"score\"] for sentiment in scores]\n",
        "#         sentiments_list.append(sentiments)\n",
        "\n",
        "#     data = data.add_column(\"sentiment\", sentiments_list)\n",
        "#     dataframe = pd.DataFrame(data)\n",
        "#     dataframe.to_pickle(f\"./data/{split}.pkl\")\n",
        "#     return\n",
        "\n",
        "\n",
        "# dataset = load_dataset(\"liar\")\n",
        "\n",
        "# add_sentiment_and_pickle(dataset, \"train\")\n",
        "# add_sentiment_and_pickle(dataset, \"validation\")\n",
        "# add_sentiment_and_pickle(dataset, \"test\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n",
            "(30522, 768)\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "print(model.config.hidden_size)\n",
        "embedding_matrix = model.embeddings.word_embeddings.weight\n",
        "embedding_matrix = embedding_matrix.detach().numpy()\n",
        "print(embedding_matrix.shape)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b758d36b34a24086adc1c565e0254620",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10269 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8eb0a7c70034777aa6f5a07fbb4abd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1284 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c0ec6811ce44cec8846da06880ceaa7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1283 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['sentiment', 'input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "def tokenize(data):\n",
        "    return tokenizer(data[\"statement\"], truncation=True, max_length=512, padding=True)\n",
        "\n",
        "def dataloader_from_pickle(split):\n",
        "    dataframe = pd.read_pickle(f\"./pickle_files/{split}.pkl\")\n",
        "    dataset = Dataset.from_pandas(dataframe)\n",
        "    tokenized_dataset = dataset.map(tokenize, batch_size=batch_size, batched=True)\n",
        "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'sentiment'])\n",
        "    return DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "\n",
        "train_dataloader = dataloader_from_pickle(\"train\")\n",
        "val_dataloader = dataloader_from_pickle(\"validation\")\n",
        "test_dataloader = dataloader_from_pickle(\"test\")\n",
        "\n",
        "print(next(iter(train_dataloader)).keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e5SqpD90Mbme"
      },
      "outputs": [],
      "source": [
        "# custom NN model with BERT embeddings\n",
        "\n",
        "class BERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.requires_grad_(False)\n",
        "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.proj_size = 20\n",
        "        self.hidden_size = 100\n",
        "        self.lstm = torch.nn.LSTM(input_size=768, hidden_size=self.hidden_size, num_layers=2, batch_first=True, bidirectional=False, proj_size=self.proj_size)\n",
        "        #self.classifier = torch.nn.Linear(self.proj_size+3, num_classes)\n",
        "        self.classifier = torch.nn.Linear(768+3, num_classes)\n",
        "        self.condenser = torch.nn.Linear(768, self.proj_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, sentiment):\n",
        "        # dummy forward pass, not real architecture\n",
        "        outputs = self.bert(input_ids, attention_mask).last_hidden_state\n",
        "        outputs = torch.mean(outputs, dim=1)\n",
        "        #outputs = self.condenser(outputs)\n",
        "        #outputs = self.lstm(outputs)[0][:,-1]\n",
        "        # insert classification layers here\n",
        "        # surprisal, sentiment, etc.\n",
        "        outputs = self.classifier(torch.cat((outputs, sentiment), dim=1))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3GJvhG6Mbme",
        "outputId": "6f0efe5c-7c6f-4786-e5f5-6a51764514cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22it [03:45, 10.25s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# this is not working\u001b[39;00m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     27\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     28\u001b[0m targets\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# simple training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = BERTClassifier(6).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "\n",
        "for i in range(100):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, batch in tqdm(enumerate(val_dataloader)):\n",
        "        print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "        print(torch.cuda.max_memory_allocated())\n",
        "        print(torch.cuda.memory_allocated())\n",
        "        batch.to(device)\n",
        "        continue\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        sentiment = batch[\"sentiment\"]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, sentiment)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward() # this is not working\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        predictions.extend(outputs.detach().argmax(dim=1).to('cpu').tolist())\n",
        "        targets.extend(labels.to('cpu').tolist())\n",
        "    total = len(targets)\n",
        "    correct = np.sum(np.array(predictions) == np.array(targets))\n",
        "    print(\"acc:\", correct/total*100, \"loss:\", np.mean(losses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hca_6mWRMbmf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# input = \"this is a sample input\"\n",
        "\n",
        "\n",
        "# # send input to tensor\n",
        "# tokenized_input = tokenizer(input, return_tensors='pt').to(device)\n",
        "# print(\"tokenize input\")\n",
        "# print(tokenized_input)\n",
        "# embeddings = BERT(**tokenized_input)[0]\n",
        "# print(\"get bert embeddings\")\n",
        "# print(\"\\t\", embeddings.shape)\n",
        "# suprisal_values = torch.Tensor(np.random.uniform(0, 1, (1, embeddings.shape[1]))).to(device)\n",
        "# print(\"get suprisal values\")\n",
        "# print(\"\\t\", suprisal_values.shape)\n",
        "# input_features = torch.cat((embeddings, suprisal_values.unsqueeze(2)), dim=2)\n",
        "# print(\"add suprisial values to embeddings\")\n",
        "# print(\"\\t\", input_features.shape)\n",
        "# input_size = input_features.shape[2]\n",
        "\n",
        "# hidden_size = 100\n",
        "# dropout = 0\n",
        "# classes = 2\n",
        "# num_layers = 1\n",
        "\n",
        "# lstm_layer = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=False,\n",
        "#                                   num_layers=num_layers, batch_first=True, dropout=dropout, proj_size=1).to(device)\n",
        "# lstm_output = lstm_layer(input_features)[0].squeeze(2)\n",
        "# print(\"run input through lstm\")\n",
        "# print(\"\\t\", lstm_output.shape)\n",
        "# sentiment_score = torch.Tensor(np.random.uniform(0, 1, (1, 3))).to(device)\n",
        "# print(\"run input through sentiment classifier\")\n",
        "# print(\"\\t\", sentiment_score.shape)\n",
        "\n",
        "# # add sentiment score to lstm output\n",
        "# combined_output = torch.cat((lstm_output, sentiment_score), dim=1)\n",
        "# print(\"add sentiment score to lstm output\")\n",
        "# print(\"\\t\", combined_output.shape)\n",
        "\n",
        "# linear_layer = torch.nn.Linear(combined_output.shape[1], classes).to(device)\n",
        "# linear_output = linear_layer(combined_output)\n",
        "# print(\"run combined output through linear layer\")\n",
        "# print(\"\\t\", linear_output.shape)\n",
        "# softmax = torch.nn.Softmax(dim=1)\n",
        "# probabilities = softmax(linear_output)\n",
        "# print(\"get probabilities\")\n",
        "# print(\"\\t\", probabilities)\n",
        "# prediction = torch.argmax(probabilities, dim=1)\n",
        "# print(\"get prediction\")\n",
        "# print(f\"label:\", prediction.item())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
