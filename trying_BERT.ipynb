{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0JLGz7_MbmX",
        "outputId": "c556448a-2845-4706-b26a-5714e334ec4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to C:\\Users\\William\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\William\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, DataCollatorWithPadding, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "API_TOKEN = \"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\"\n",
        "login(token=API_TOKEN)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "distilled_student_sentiment_classifier = pipeline(\n",
        "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUyj-JMhMbmd",
        "outputId": "87a04ee4-b8f0-4667-89b9-b2b3356733ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "100%|██████████| 10269/10269 [25:33<00:00,  6.70it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id  label                                          statement  \\\n",
            "0   2635.json      0  Says the Annies List political group supports ...   \n",
            "1  10540.json      1  When did the decline of coal start? It started...   \n",
            "2    324.json      2  Hillary Clinton agrees with John McCain \"by vo...   \n",
            "3   1123.json      0  Health care reform legislation is likely to ma...   \n",
            "4   9028.json      1  The economic turnaround started at the end of ...   \n",
            "5  12465.json      3  The Chicago Bears have had more starting quart...   \n",
            "6   2342.json      4  Jim Dunnam has not lived in the district he re...   \n",
            "7    153.json      1  \"I'm the only person on this stage who has wor...   \n",
            "8   5602.json      1  However, it took $19.5 million in Oregon Lotte...   \n",
            "9   9741.json      2  Says GOP primary opponents Glenn Grothman and ...   \n",
            "\n",
            "                                     subject                 speaker  \\\n",
            "0                                   abortion            dwayne-bohac   \n",
            "1         energy,history,job-accomplishments          scott-surovell   \n",
            "2                             foreign-policy            barack-obama   \n",
            "3                                health-care            blog-posting   \n",
            "4                               economy,jobs           charlie-crist   \n",
            "5                                  education               robin-vos   \n",
            "6                       candidates-biography  republican-party-texas   \n",
            "7                                     ethics            barack-obama   \n",
            "8                                       jobs          oregon-lottery   \n",
            "9  energy,message-machine-2014,voting-record           duey-stroebel   \n",
            "\n",
            "                    job_title state_info party_affiliation  \\\n",
            "0        State representative      Texas        republican   \n",
            "1              State delegate   Virginia          democrat   \n",
            "2                   President   Illinois          democrat   \n",
            "3                                                     none   \n",
            "4                                Florida          democrat   \n",
            "5  Wisconsin Assembly speaker  Wisconsin        republican   \n",
            "6                                  Texas        republican   \n",
            "7                   President   Illinois          democrat   \n",
            "8                                             organization   \n",
            "9        State representative  Wisconsin        republican   \n",
            "\n",
            "   barely_true_counts  false_counts  half_true_counts  mostly_true_counts  \\\n",
            "0                 0.0           1.0               0.0                 0.0   \n",
            "1                 0.0           0.0               1.0                 1.0   \n",
            "2                70.0          71.0             160.0               163.0   \n",
            "3                 7.0          19.0               3.0                 5.0   \n",
            "4                15.0           9.0              20.0                19.0   \n",
            "5                 0.0           3.0               2.0                 5.0   \n",
            "6                 3.0           1.0               1.0                 3.0   \n",
            "7                70.0          71.0             160.0               163.0   \n",
            "8                 0.0           0.0               1.0                 0.0   \n",
            "9                 0.0           0.0               0.0                 1.0   \n",
            "\n",
            "   pants_on_fire_counts                                   context  \\\n",
            "0                   0.0                                  a mailer   \n",
            "1                   0.0                           a floor speech.   \n",
            "2                   9.0                                    Denver   \n",
            "3                  44.0                            a news release   \n",
            "4                   2.0                       an interview on CNN   \n",
            "5                   1.0                 a an online opinion-piece   \n",
            "6                   1.0                          a press release.   \n",
            "7                   9.0  a Democratic debate in Philadelphia, Pa.   \n",
            "8                   1.0                                a website    \n",
            "9                   0.0                           an online video   \n",
            "\n",
            "                                           sentiment  perplexity  \n",
            "0  [0.494022935628891, 0.1540677547454834, 0.3519...    2.338595  \n",
            "1  [0.23761452734470367, 0.18118640780448914, 0.5...    1.668094  \n",
            "2  [0.5235663652420044, 0.14535528421401978, 0.33...    1.924924  \n",
            "3  [0.4653804898262024, 0.1953166276216507, 0.339...    2.172579  \n",
            "4  [0.43884366750717163, 0.20763611793518066, 0.3...    3.380251  \n",
            "5  [0.3803189694881439, 0.24844682216644287, 0.37...    1.553483  \n",
            "6  [0.27129918336868286, 0.44253605604171753, 0.2...    2.554948  \n",
            "7  [0.2751220762729645, 0.2210664004087448, 0.503...    1.495040  \n",
            "8  [0.4187851846218109, 0.27245083451271057, 0.30...    1.585191  \n",
            "9  [0.3940470814704895, 0.14908328652381897, 0.45...    1.648126  \n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loss_model = AutoModelForCausalLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "loss_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "def add_loss_and_pickle(split):\n",
        "\n",
        "    \n",
        "    dataframe = pd.read_pickle(f\"./pickle_files/{split}.pkl\")\n",
        "    data = Dataset.from_pandas(dataframe)\n",
        "    perplexity_list = []\n",
        "\n",
        "    for sample in tqdm(data):\n",
        "        lm_out = loss_model(input_ids, attention_mask, output_hidden_states=True)\n",
        "        outputs = lm_out.hidden_states[-1]\n",
        "        #print(outputs)\n",
        "        # print(\"lm output\", outputs.shape, outputs.dtype)\n",
        "        # print(\"outputs\", outputs)\n",
        "        #outputs = self.lstm(outputs)[0][:,-1]\n",
        "        logits = torch.nn.functional.softmax(lm_out.logits, dim=-1)\n",
        "        #print(\"logits\", logits.shape, logits.dtype)\n",
        "        probs = torch.gather(logits, dim=2, index=input_ids.unsqueeze(dim=2)).squeeze(-1)\n",
        "        #print(\"probs\", probs.shape, probs.dtype)\n",
        "        subword_surp = -1 * torch.log2(probs) * attention_mask\n",
        "        #print(\"subword_surp\", subword_surp.shape, subword_surp.dtype)\n",
        "        #print(\"subword_surp\", subword_surp.shape, subword_surp.dtype)\n",
        "        mean_surprisal = subword_surp.sum(dim=1) / attention_mask.sum(dim=1)\n",
        "    data = data.add_column(\"perplexity\", perplexity_list)\n",
        "    dataframe = pd.DataFrame(data)\n",
        "    print(dataframe.head(10))\n",
        "    dataframe.to_pickle(f\"./data/{split}.pkl\")\n",
        "    return\n",
        "\n",
        "#add_loss_and_pickle(\"train\")\n",
        "#add_loss_and_pickle(\"validation\")\n",
        "#add_loss_and_pickle(\"test\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "768\n",
            "(30522, 768)\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "print(model.config.hidden_size)\n",
        "embedding_matrix = model.embeddings.word_embeddings.weight\n",
        "embedding_matrix = embedding_matrix.detach().numpy()\n",
        "print(embedding_matrix.shape)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df354bc7deb34191b29161d1876f6558",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10269 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "268c7ddb3772490b83574bf6ec49fcf7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1284 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "798a60bdd80746f88b7b44a9d57e8a0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1283 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['sentiment', 'input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "def tokenize(data):\n",
        "    return tokenizer(data[\"statement\"], truncation=True, max_length=512, padding=True)\n",
        "\n",
        "def dataloader_from_pickle(split):\n",
        "    dataframe = pd.read_pickle(f\"./pickle_files/{split}.pkl\")\n",
        "    dataset = Dataset.from_pandas(dataframe)\n",
        "    tokenized_dataset = dataset.map(tokenize, batch_size=batch_size, batched=True)\n",
        "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label', 'sentiment', 'perplexity'])\n",
        "    return DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "\n",
        "train_dataloader = dataloader_from_pickle(\"train\")\n",
        "val_dataloader = dataloader_from_pickle(\"validation\")\n",
        "test_dataloader = dataloader_from_pickle(\"test\")\n",
        "\n",
        "print(next(iter(train_dataloader)).keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5SqpD90Mbme"
      },
      "outputs": [],
      "source": [
        "# custom NN model with BERT embeddings\n",
        "\n",
        "class BERTClassifier(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.requires_grad_(False)\n",
        "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.proj_size = 20\n",
        "        self.hidden_size = 100\n",
        "        self.lstm = torch.nn.LSTM(input_size=768, hidden_size=self.hidden_size, num_layers=2, batch_first=True, bidirectional=False, proj_size=self.proj_size)\n",
        "        #self.classifier = torch.nn.Linear(self.proj_size+3, num_classes)\n",
        "        self.classifier = torch.nn.Linear(768+3, num_classes)\n",
        "        self.condenser = torch.nn.Linear(768, self.proj_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, sentiment):\n",
        "        # dummy forward pass, not real architecture\n",
        "        outputs = self.bert(input_ids, attention_mask).last_hidden_state\n",
        "        outputs = torch.mean(outputs, dim=1)\n",
        "        #outputs = self.condenser(outputs)\n",
        "        #outputs = self.lstm(outputs)[0][:,-1]\n",
        "        # insert classification layers here\n",
        "        # surprisal, sentiment, etc.\n",
        "        outputs = self.classifier(torch.cat((outputs, sentiment), dim=1))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "print(bnb_config.bnb_4bit_compute_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3GJvhG6Mbme",
        "outputId": "6f0efe5c-7c6f-4786-e5f5-6a51764514cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3168100864\n",
            "1792331776\n",
            "<class 'list'> <class 'list'>\n",
            "acc: 18.69158878504673 loss: 2.4384174288772957\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_dataloader):\n\u001b[0;32m     15\u001b[0m     batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\data\\data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3283\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3278\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_element\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unknown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of a python, numpy, pytorch or tensorflow object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3280\u001b[0m         )\n\u001b[0;32m   3282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m encoded_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m-> 3283\u001b[0m         encoded_inputs[key] \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3285\u001b[0m \u001b[38;5;66;03m# Convert padding_strategy in PaddingStrategy\u001b[39;00m\n\u001b[0;32m   3286\u001b[0m padding_strategy, _, max_length, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3287\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding, max_length\u001b[38;5;241m=\u001b[39mmax_length, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[0;32m   3288\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:249\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:255\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[1;32m--> 255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mframework_to_py_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mnumber):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:240\u001b[0m, in \u001b[0;36mto_py_obj.<locals>.<lambda>\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_py_obj\u001b[39m(obj):\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     framework_to_py_obj \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np\u001b[38;5;241m.\u001b[39masarray(obj)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    244\u001b[0m     }\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mdict\u001b[39m, UserDict)):\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# simple training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model = BERTClassifier(6).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "\n",
        "for i in range(10):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        \n",
        "        batch.to(device)\n",
        "        \n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        sentiment = batch[\"sentiment\"]\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, sentiment)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward() # this is not working\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        predictions.extend(outputs.detach().argmax(dim=1).to('cpu').tolist())\n",
        "        targets.extend(labels.to('cpu').tolist())\n",
        "    print(torch.cuda.max_memory_allocated())\n",
        "    print(torch.cuda.memory_allocated())\n",
        "    print(type(predictions), type(targets))\n",
        "    total = len(targets)\n",
        "    correct = np.sum(np.array(predictions) == np.array(targets))\n",
        "    print(\"acc:\", correct/total*100, \"loss:\", np.mean(losses))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hca_6mWRMbmf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# input = \"this is a sample input\"\n",
        "\n",
        "\n",
        "# # send input to tensor\n",
        "# tokenized_input = tokenizer(input, return_tensors='pt').to(device)\n",
        "# print(\"tokenize input\")\n",
        "# print(tokenized_input)\n",
        "# embeddings = BERT(**tokenized_input)[0]\n",
        "# print(\"get bert embeddings\")\n",
        "# print(\"\\t\", embeddings.shape)\n",
        "# suprisal_values = torch.Tensor(np.random.uniform(0, 1, (1, embeddings.shape[1]))).to(device)\n",
        "# print(\"get suprisal values\")\n",
        "# print(\"\\t\", suprisal_values.shape)\n",
        "# input_features = torch.cat((embeddings, suprisal_values.unsqueeze(2)), dim=2)\n",
        "# print(\"add suprisial values to embeddings\")\n",
        "# print(\"\\t\", input_features.shape)\n",
        "# input_size = input_features.shape[2]\n",
        "\n",
        "# hidden_size = 100\n",
        "# dropout = 0\n",
        "# classes = 2\n",
        "# num_layers = 1\n",
        "\n",
        "# lstm_layer = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=False,\n",
        "#                                   num_layers=num_layers, batch_first=True, dropout=dropout, proj_size=1).to(device)\n",
        "# lstm_output = lstm_layer(input_features)[0].squeeze(2)\n",
        "# print(\"run input through lstm\")\n",
        "# print(\"\\t\", lstm_output.shape)\n",
        "# sentiment_score = torch.Tensor(np.random.uniform(0, 1, (1, 3))).to(device)\n",
        "# print(\"run input through sentiment classifier\")\n",
        "# print(\"\\t\", sentiment_score.shape)\n",
        "\n",
        "# # add sentiment score to lstm output\n",
        "# combined_output = torch.cat((lstm_output, sentiment_score), dim=1)\n",
        "# print(\"add sentiment score to lstm output\")\n",
        "# print(\"\\t\", combined_output.shape)\n",
        "\n",
        "# linear_layer = torch.nn.Linear(combined_output.shape[1], classes).to(device)\n",
        "# linear_output = linear_layer(combined_output)\n",
        "# print(\"run combined output through linear layer\")\n",
        "# print(\"\\t\", linear_output.shape)\n",
        "# softmax = torch.nn.Softmax(dim=1)\n",
        "# probabilities = softmax(linear_output)\n",
        "# print(\"get probabilities\")\n",
        "# print(\"\\t\", probabilities)\n",
        "# prediction = torch.argmax(probabilities, dim=1)\n",
        "# print(\"get prediction\")\n",
        "# print(f\"label:\", prediction.item())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
