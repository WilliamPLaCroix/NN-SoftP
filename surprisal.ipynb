{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfingten/.python_environments/ML/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wolfingten/.python_environments/ML/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wolfingten/.python_environments/ML/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, XLMRobertaTokenizerFast, XLMRobertaXLForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type xlm-roberta-xl. This is not supported for all configurations of models and can yield errors.\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of XLMRobertaXLForCausalLM were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.encoder.layer.1.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.3.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.6.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.11.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.0.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self_attn_layer_norm.weight', 'roberta.encoder.LayerNorm.weight', 'roberta.encoder.layer.1.LayerNorm.weight', 'roberta.encoder.layer.5.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.4.LayerNorm.weight', 'roberta.encoder.layer.4.LayerNorm.bias', 'roberta.encoder.layer.10.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.9.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.7.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.3.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.5.LayerNorm.bias', 'roberta.encoder.layer.9.LayerNorm.weight', 'roberta.encoder.layer.7.LayerNorm.bias', 'roberta.encoder.layer.10.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.0.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.9.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.0.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.2.LayerNorm.bias', 'roberta.encoder.layer.1.LayerNorm.bias', 'roberta.encoder.LayerNorm.bias', 'roberta.encoder.layer.2.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.4.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.6.LayerNorm.weight', 'roberta.encoder.layer.3.LayerNorm.weight', 'roberta.encoder.layer.8.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.6.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.11.LayerNorm.weight', 'roberta.encoder.layer.11.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.10.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.8.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.4.attention.self_attn_layer_norm.weight', 'roberta.encoder.layer.5.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.1.attention.self_attn_layer_norm.bias', 'roberta.encoder.layer.9.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaXLForCausalLM.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"I am bald unbelievable.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = tokens[\"input_ids\"].squeeze(0)[1:-1]\n",
    "index = torch.arange(0, output_ids.shape[0])\n",
    "surp = -1 * torch.log2(torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze(0)[index, output_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:44.09819030761719\n",
      "1:27.66463851928711\n",
      "2:30.628446578979492\n",
      "3:18.88300323486328\n",
      "3:22.08664321899414\n",
      "3:20.027633666992188\n",
      "3:32.8770866394043\n",
      "3:13.566536903381348\n"
     ]
    }
   ],
   "source": [
    "ids = tokens.word_ids()[1:-1]\n",
    "\n",
    "for i, surp in zip(ids, surp):\n",
    "    \n",
    "    print(f\"{i}:{surp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object of too small depth for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(ids)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m surprisal_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(surp)\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m word_surprisal_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mbincount(ids, weights\u001b[39m=\u001b[39;49msurprisal_values)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m word_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(ids)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m word_surprisal_avg \u001b[39m=\u001b[39m word_surprisal_sum \u001b[39m/\u001b[39m word_counts\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: object of too small depth for desired array"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ids = np.array(ids)\n",
    "surprisal_values = np.array(surp)\n",
    "\n",
    "word_surprisal_sum = np.bincount(ids, weights=surprisal_values)\n",
    "word_counts = np.bincount(ids)\n",
    "word_surprisal_avg = word_surprisal_sum / word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   87,   444, 33984,     5,     2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = tokens[\"input_ids\"].squeeze(0)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4304e+01, -6.9474e-02,  2.8266e+01,  ...,  1.4371e+01,\n",
       "           4.7493e+00,  1.1140e+01],\n",
       "         [ 1.0702e+01,  3.4034e-03,  2.1005e+01,  ...,  1.6348e+01,\n",
       "           6.6278e+00,  9.6904e+00],\n",
       "         [ 1.0795e+01,  4.1587e-02,  2.0379e+01,  ...,  1.5365e+01,\n",
       "           5.9377e+00,  9.3574e+00],\n",
       "         [ 1.1029e+01,  1.0021e-01,  2.0498e+01,  ...,  1.6095e+01,\n",
       "           6.1641e+00,  9.0450e+00],\n",
       "         [ 8.6666e+00,  3.4548e-02,  2.0787e+01,  ...,  1.5092e+01,\n",
       "           6.0019e+00,  9.5058e+00],\n",
       "         [ 1.2237e+01,  2.2618e-02,  2.0028e+01,  ...,  1.5195e+01,\n",
       "           5.9527e+00,  9.4392e+00]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
