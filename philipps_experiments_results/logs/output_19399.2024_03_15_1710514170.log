
=============
== PyTorch ==
=============

NVIDIA Release 24.01 (build 80741402)
PyTorch Version 2.2.0a0+81ea7a4

Container image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

Copyright (c) 2014-2023 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: CUDA Forward Compatibility mode ENABLED.
  Using CUDA 12.3 driver version 545.23.08 with kernel driver version 525.85.12.
  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.

Language Model has hidden_size: 4096
freezing Model... (AutoModel)
Running on device: cuda
Epoch [1/1000] took 95.35818409919739s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4365913355461906, train accuracy: 0.45544843704352905
Val mean loss: 1.6713359704831752, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2758 			 1998 			 1128
2210 			 2123 			 999
2155 			 1966 			 977
1479 			 1683 			 721
1249 			 1657 			 597
418 			 842 			 255
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
363 			 263 			 111
220 			 248 			 60
241 			 251 			 78
212 			 169 			 53
195 			 237 			 54
53 			 116 			 24
Max memory allocated: 8076390912; Memory allocated: 3806592000
Epoch [2/1000] took 96.48454117774963s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.435703537545843, train accuracy: 0.45583795890544354
Val mean loss: 1.6641664388703137, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2758 			 1998 			 1109
2148 			 2123 			 977
2125 			 1966 			 979
1531 			 1683 			 741
1270 			 1657 			 603
437 			 842 			 272
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 102
242 			 248 			 64
202 			 251 			 71
236 			 169 			 59
209 			 237 			 60
53 			 116 			 21
Max memory allocated: 8200616960; Memory allocated: 3849487872
Epoch [3/1000] took 96.69725275039673s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.43506834737237, train accuracy: 0.4578829486804947
Val mean loss: 1.6771347202905795, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2700 			 1998 			 1120
2124 			 2123 			 983
2077 			 1966 			 965
1571 			 1683 			 744
1380 			 1657 			 634
417 			 842 			 256
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
369 			 263 			 111
292 			 248 			 77
237 			 251 			 78
181 			 169 			 45
148 			 237 			 45
57 			 116 			 25
Max memory allocated: 8305229312; Memory allocated: 3932557824
Epoch [4/1000] took 97.22984623908997s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4341967206134973, train accuracy: 0.45651962216379394
Val mean loss: 1.6727812406493396, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2789 			 1998 			 1128
2203 			 2123 			 1010
2141 			 1966 			 987
1458 			 1683 			 705
1245 			 1657 			 594
433 			 842 			 264
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 97
309 			 248 			 76
183 			 251 			 65
224 			 169 			 60
171 			 237 			 50
59 			 116 			 27
Max memory allocated: 8305229312; Memory allocated: 3891726848
Epoch [5/1000] took 97.0781581401825s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4360991326447958, train accuracy: 0.4553510565780504
Val mean loss: 1.6911757835527745, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2693 			 1998 			 1103
2247 			 2123 			 1007
2083 			 1966 			 968
1514 			 1683 			 726
1308 			 1657 			 609
424 			 842 			 263
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 98
306 			 248 			 76
207 			 251 			 70
222 			 169 			 58
160 			 237 			 47
54 			 116 			 25
Max memory allocated: 8319195648; Memory allocated: 3806888448
Epoch [6/1000] took 97.28613471984863s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4336421797951433, train accuracy: 0.45135845749342685
Val mean loss: 1.6662590765371554, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2728 			 1998 			 1100
2177 			 2123 			 983
2104 			 1966 			 957
1560 			 1683 			 738
1261 			 1657 			 595
439 			 842 			 262
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 93
338 			 248 			 83
224 			 251 			 76
203 			 169 			 52
157 			 237 			 48
53 			 116 			 24
Max memory allocated: 8319195648; Memory allocated: 3855877632
Epoch [7/1000] took 97.36052918434143s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.433626692243083, train accuracy: 0.4601226993865031
Val mean loss: 1.6723827472547206, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2642 			 1998 			 1115
2347 			 2123 			 1052
2066 			 1966 			 969
1496 			 1683 			 717
1297 			 1657 			 609
421 			 842 			 263
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 102
233 			 248 			 62
267 			 251 			 83
221 			 169 			 56
131 			 237 			 42
66 			 116 			 27
Max memory allocated: 8319195648; Memory allocated: 3891661312
Epoch [8/1000] took 97.25642561912537s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4338740218094206, train accuracy: 0.4578829486804947
Val mean loss: 1.681969165802002, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2781 			 1998 			 1137
2141 			 2123 			 979
2142 			 1966 			 991
1554 			 1683 			 745
1234 			 1657 			 592
417 			 842 			 258
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
361 			 263 			 106
260 			 248 			 67
232 			 251 			 73
199 			 169 			 49
170 			 237 			 52
62 			 116 			 28
Max memory allocated: 8347215360; Memory allocated: 3846080000
Epoch [9/1000] took 96.69633340835571s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.43250511145666, train accuracy: 0.45700652449118706
Val mean loss: 1.6696376567933617, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2726 			 1998 			 1113
2235 			 2123 			 1012
2115 			 1966 			 983
1504 			 1683 			 733
1258 			 1657 			 590
431 			 842 			 262
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
361 			 263 			 107
258 			 248 			 65
228 			 251 			 74
201 			 169 			 52
168 			 237 			 49
68 			 116 			 25
Max memory allocated: 8347215360; Memory allocated: 3892021760
Epoch [10/1000] took 97.20625877380371s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4323560348552336, train accuracy: 0.45739604635310155
Val mean loss: 1.6770553472565441, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2689 			 1998 			 1109
2229 			 2123 			 1000
2111 			 1966 			 973
1475 			 1683 			 719
1308 			 1657 			 619
457 			 842 			 277
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
409 			 263 			 114
283 			 248 			 71
186 			 251 			 66
236 			 169 			 60
111 			 237 			 33
59 			 116 			 25
Max memory allocated: 8347215360; Memory allocated: 3815408128
Epoch [11/1000] took 97.07461452484131s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4327597146465028, train accuracy: 0.4575908072840588
Val mean loss: 1.6742982864379883, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2823 			 1998 			 1142
2141 			 2123 			 979
2042 			 1966 			 962
1568 			 1683 			 748
1253 			 1657 			 601
442 			 842 			 267
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 104
302 			 248 			 73
266 			 251 			 81
187 			 169 			 47
143 			 237 			 45
62 			 116 			 27
Max memory allocated: 8347215360; Memory allocated: 3819667968
Epoch [12/1000] took 96.78828430175781s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4303115613735353, train accuracy: 0.45856461193884507
Val mean loss: 1.7000730212141828, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2743 			 1998 			 1133
2219 			 2123 			 1004
2054 			 1966 			 961
1514 			 1683 			 728
1318 			 1657 			 615
421 			 842 			 268
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 101
263 			 248 			 71
285 			 251 			 85
186 			 169 			 46
152 			 237 			 45
65 			 116 			 28
Max memory allocated: 8431965696; Memory allocated: 3823927808
Epoch [13/1000] took 97.41305446624756s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4331644327841073, train accuracy: 0.4577855682150161
Val mean loss: 1.6704960945175915, val accuracy: 0.30062305295950154

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2699 			 1998 			 1117
2200 			 2123 			 1002
2177 			 1966 			 993
1470 			 1683 			 719
1280 			 1657 			 601
443 			 842 			 269
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
370 			 263 			 113
248 			 248 			 66
265 			 251 			 83
207 			 169 			 55
142 			 237 			 45
52 			 116 			 24
Max memory allocated: 8431965696; Memory allocated: 3846538752
Epoch [14/1000] took 96.94649696350098s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.430407221814925, train accuracy: 0.45729866588762297
Val mean loss: 1.665691614151001, val accuracy: 0.2998442367601246

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2760 			 1998 			 1130
2161 			 2123 			 987
2152 			 1966 			 993
1573 			 1683 			 752
1227 			 1657 			 582
396 			 842 			 252
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 104
308 			 248 			 78
220 			 251 			 74
188 			 169 			 51
164 			 237 			 54
63 			 116 			 24
Max memory allocated: 8431965696; Memory allocated: 3845883392
Epoch [15/1000] took 97.18428754806519s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.430299490411705, train accuracy: 0.45983055799006717
Val mean loss: 1.6745445786452875, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2750 			 1998 			 1138
2230 			 2123 			 1020
2097 			 1966 			 977
1477 			 1683 			 715
1282 			 1657 			 611
433 			 842 			 261
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 101
274 			 248 			 72
253 			 251 			 80
192 			 169 			 50
161 			 237 			 49
58 			 116 			 26
Max memory allocated: 8431965696; Memory allocated: 3892414976
Epoch [16/1000] took 97.26635694503784s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4312263288973277, train accuracy: 0.4599279384555458
Val mean loss: 1.676950414006303, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2672 			 1998 			 1110
2221 			 2123 			 1022
2118 			 1966 			 981
1503 			 1683 			 735
1328 			 1657 			 617
427 			 842 			 258
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 103
242 			 248 			 63
201 			 251 			 69
248 			 169 			 63
162 			 237 			 46
65 			 116 			 27
Max memory allocated: 8431965696; Memory allocated: 3846800896
Epoch [17/1000] took 96.86135387420654s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4282409428064697, train accuracy: 0.4567143830947512
Val mean loss: 1.6759050968216687, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2718 			 1998 			 1119
2216 			 2123 			 993
2068 			 1966 			 959
1529 			 1683 			 731
1255 			 1657 			 601
483 			 842 			 287
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
420 			 263 			 119
259 			 248 			 65
221 			 251 			 73
189 			 169 			 47
149 			 237 			 45
46 			 116 			 25
Max memory allocated: 8431965696; Memory allocated: 3846047232
Epoch [18/1000] took 97.2016236782074s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4283220753120114, train accuracy: 0.4556431979744863
Val mean loss: 1.6967532809187726, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2788 			 1998 			 1132
2115 			 2123 			 964
2189 			 1966 			 999
1475 			 1683 			 724
1288 			 1657 			 605
414 			 842 			 255
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 93
316 			 248 			 79
189 			 251 			 65
226 			 169 			 57
167 			 237 			 48
70 			 116 			 26
Max memory allocated: 8499783168; Memory allocated: 3806888448
Epoch [19/1000] took 97.18581819534302s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4296273137921485, train accuracy: 0.45875937286980234
Val mean loss: 1.671390745697952, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2667 			 1998 			 1089
2295 			 2123 			 1028
2077 			 1966 			 980
1515 			 1683 			 739
1243 			 1657 			 595
472 			 842 			 280
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 96
256 			 248 			 67
208 			 251 			 73
222 			 169 			 56
206 			 237 			 59
54 			 116 			 22
Max memory allocated: 8499783168; Memory allocated: 3813278208
Epoch [20/1000] took 97.05796480178833s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4288406468626123, train accuracy: 0.45768818774953746
Val mean loss: 1.6810052045961705, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2700 			 1998 			 1119
2129 			 2123 			 980
2117 			 1966 			 971
1524 			 1683 			 727
1382 			 1657 			 641
417 			 842 			 262
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 102
292 			 248 			 77
208 			 251 			 67
210 			 169 			 54
165 			 237 			 50
62 			 116 			 25
Max memory allocated: 8499783168; Memory allocated: 3846768128
Epoch [21/1000] took 97.06182551383972s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4279460335074927, train accuracy: 0.45934365566267404
Val mean loss: 1.673282364519631, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2686 			 1998 			 1109
2233 			 2123 			 1014
2068 			 1966 			 964
1521 			 1683 			 742
1312 			 1657 			 619
449 			 842 			 269
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
359 			 263 			 105
286 			 248 			 72
226 			 251 			 76
223 			 169 			 58
123 			 237 			 41
67 			 116 			 28
Max memory allocated: 8499783168; Memory allocated: 3846768128
Epoch [22/1000] took 97.6650960445404s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4297481637877467, train accuracy: 0.45729866588762297
Val mean loss: 1.6699076949096308, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2781 			 1998 			 1152
2176 			 2123 			 973
2129 			 1966 			 988
1560 			 1683 			 736
1190 			 1657 			 575
433 			 842 			 272
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 103
316 			 248 			 79
207 			 251 			 71
189 			 169 			 51
163 			 237 			 46
60 			 116 			 25
Max memory allocated: 8499783168; Memory allocated: 3846047232
Epoch [23/1000] took 96.74096488952637s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.42830003013492, train accuracy: 0.4595384165936313
Val mean loss: 1.6629255137792447, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2720 			 1998 			 1121
2183 			 2123 			 1000
2159 			 1966 			 994
1484 			 1683 			 722
1297 			 1657 			 618
426 			 842 			 264
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
358 			 263 			 108
300 			 248 			 80
190 			 251 			 66
214 			 169 			 56
154 			 237 			 45
68 			 116 			 29
Max memory allocated: 8499783168; Memory allocated: 3819667968
Epoch [24/1000] took 97.49651002883911s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4255833306416543, train accuracy: 0.4603174603174603
Val mean loss: 1.6688155924401633, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2752 			 1998 			 1129
2179 			 2123 			 1001
2126 			 1966 			 983
1474 			 1683 			 729
1303 			 1657 			 620
435 			 842 			 265
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 92
305 			 248 			 76
192 			 251 			 68
274 			 169 			 68
154 			 237 			 51
67 			 116 			 29
Max memory allocated: 8635561472; Memory allocated: 3851617792
Epoch [25/1000] took 96.74170589447021s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4249343114478565, train accuracy: 0.4594410361281527
Val mean loss: 1.686035569121198, val accuracy: 0.30218068535825543

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2651 			 1998 			 1114
2336 			 2123 			 1034
2037 			 1966 			 954
1516 			 1683 			 738
1279 			 1657 			 605
450 			 842 			 273
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 103
242 			 248 			 65
225 			 251 			 75
259 			 169 			 66
161 			 237 			 52
59 			 116 			 27
Max memory allocated: 8635561472; Memory allocated: 3846145536
Epoch [26/1000] took 97.20856881141663s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4258602345101188, train accuracy: 0.46265459148894733
Val mean loss: 1.663406616303979, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2707 			 1998 			 1125
2124 			 2123 			 985
2107 			 1966 			 989
1608 			 1683 			 772
1302 			 1657 			 618
421 			 842 			 262
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
357 			 263 			 106
297 			 248 			 76
256 			 251 			 83
160 			 169 			 45
154 			 237 			 46
60 			 116 			 27
Max memory allocated: 8635561472; Memory allocated: 3855877632
Epoch [27/1000] took 96.95959830284119s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4263945138343026, train accuracy: 0.4623624500925114
Val mean loss: 1.6587551919425405, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2689 			 1998 			 1126
2229 			 2123 			 1023
2084 			 1966 			 973
1518 			 1683 			 737
1323 			 1657 			 619
426 			 842 			 270
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
396 			 263 			 117
258 			 248 			 67
246 			 251 			 76
186 			 169 			 47
136 			 237 			 43
62 			 116 			 25
Max memory allocated: 8635561472; Memory allocated: 3823927808
Epoch [28/1000] took 97.31503486633301s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.424318101175849, train accuracy: 0.4583698510078878
Val mean loss: 1.7006511746383295, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2742 			 1998 			 1124
2176 			 2123 			 989
2175 			 1966 			 999
1469 			 1683 			 723
1265 			 1657 			 601
442 			 842 			 271
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
405 			 263 			 116
281 			 248 			 73
199 			 251 			 66
209 			 169 			 55
143 			 237 			 42
47 			 116 			 24
Max memory allocated: 8635561472; Memory allocated: 3851617792
Epoch [29/1000] took 97.1434416770935s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4260116938118623, train accuracy: 0.4610965040412893
Val mean loss: 1.6725288193400314, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2701 			 1998 			 1122
2186 			 2123 			 1008
2124 			 1966 			 983
1526 			 1683 			 734
1298 			 1657 			 621
434 			 842 			 267
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
387 			 263 			 112
291 			 248 			 73
194 			 251 			 70
209 			 169 			 54
150 			 237 			 46
53 			 116 			 26
Max memory allocated: 8635561472; Memory allocated: 3819667968
Epoch [30/1000] took 96.90561652183533s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.423292985957731, train accuracy: 0.46119388450676796
Val mean loss: 1.695559585966715, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2734 			 1998 			 1125
2114 			 2123 			 986
2128 			 1966 			 999
1519 			 1683 			 730
1329 			 1657 			 625
445 			 842 			 271
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
428 			 263 			 122
285 			 248 			 72
179 			 251 			 61
198 			 169 			 52
145 			 237 			 44
49 			 116 			 22
Max memory allocated: 8635561472; Memory allocated: 3847259648
Epoch [31/1000] took 96.94219541549683s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4251288777199862, train accuracy: 0.46070698217937484
Val mean loss: 1.6887839683672277, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2809 			 1998 			 1152
2272 			 2123 			 1035
2054 			 1966 			 967
1457 			 1683 			 716
1262 			 1657 			 601
415 			 842 			 260
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 102
251 			 248 			 65
213 			 251 			 73
249 			 169 			 62
176 			 237 			 51
59 			 116 			 25
Max memory allocated: 8635561472; Memory allocated: 3891628544
Epoch [32/1000] took 97.16665625572205s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4242150311900819, train accuracy: 0.46353101567825494
Val mean loss: 1.6817601512118083, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2696 			 1998 			 1119
2173 			 2123 			 993
2133 			 1966 			 1004
1499 			 1683 			 742
1324 			 1657 			 626
444 			 842 			 276
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 97
337 			 248 			 77
198 			 251 			 70
215 			 169 			 57
155 			 237 			 46
67 			 116 			 28
Max memory allocated: 8635561472; Memory allocated: 3846899200
Epoch [33/1000] took 97.11094999313354s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4240526740796098, train accuracy: 0.4621676891615542
Val mean loss: 1.670237512123294, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2745 			 1998 			 1138
2270 			 2123 			 1034
2073 			 1966 			 968
1488 			 1683 			 735
1242 			 1657 			 596
451 			 842 			 275
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 89
240 			 248 			 58
272 			 251 			 85
255 			 169 			 63
157 			 237 			 47
74 			 116 			 29
Max memory allocated: 8635561472; Memory allocated: 3847194112
Epoch [34/1000] took 96.92723059654236s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4229348834801314, train accuracy: 0.4657707663842633
Val mean loss: 1.6641037958424265, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2643 			 1998 			 1109
2126 			 2123 			 986
2077 			 1966 			 983
1635 			 1683 			 784
1339 			 1657 			 642
449 			 842 			 279
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
363 			 263 			 108
257 			 248 			 70
240 			 251 			 75
204 			 169 			 49
160 			 237 			 48
60 			 116 			 25
Max memory allocated: 8635561472; Memory allocated: 3845719552
Epoch [35/1000] took 97.36758494377136s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.423986332067448, train accuracy: 0.4616807868341611
Val mean loss: 1.6835051280696218, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2729 			 1998 			 1132
2158 			 2123 			 987
2175 			 1966 			 1002
1450 			 1683 			 728
1312 			 1657 			 616
445 			 842 			 276
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
348 			 263 			 103
291 			 248 			 72
220 			 251 			 73
230 			 169 			 60
141 			 237 			 47
54 			 116 			 26
Max memory allocated: 8635561472; Memory allocated: 3821797888
Epoch [36/1000] took 97.25669980049133s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4226903146672472, train accuracy: 0.4638231570746908
Val mean loss: 1.6695415711984403, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2767 			 1998 			 1145
2192 			 2123 			 1015
2132 			 1966 			 1001
1528 			 1683 			 748
1218 			 1657 			 585
432 			 842 			 269
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
273 			 263 			 90
304 			 248 			 72
210 			 251 			 73
246 			 169 			 64
185 			 237 			 51
66 			 116 			 29
Max memory allocated: 8635561472; Memory allocated: 3892578816
Epoch [37/1000] took 97.64848446846008s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4243264090606356, train accuracy: 0.46421267893660534
Val mean loss: 1.6795607369120529, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2644 			 1998 			 1111
2238 			 2123 			 1021
2109 			 1966 			 982
1533 			 1683 			 749
1308 			 1657 			 625
437 			 842 			 279
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
379 			 263 			 111
264 			 248 			 69
185 			 251 			 66
236 			 169 			 64
161 			 237 			 46
59 			 116 			 26
Max memory allocated: 8635561472; Memory allocated: 3804758528
Epoch [38/1000] took 97.00810217857361s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4217134955516113, train accuracy: 0.4638231570746908
Val mean loss: 1.6589303103888906, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2712 			 1998 			 1119
2222 			 2123 			 1031
2053 			 1966 			 972
1520 			 1683 			 743
1343 			 1657 			 630
419 			 842 			 268
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
372 			 263 			 110
262 			 248 			 68
232 			 251 			 73
213 			 169 			 56
130 			 237 			 41
75 			 116 			 29
Max memory allocated: 8635561472; Memory allocated: 3846211072
Epoch [39/1000] took 96.60859799385071s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4196872874583781, train accuracy: 0.4629467328853832
Val mean loss: 1.6779440699554071, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2740 			 1998 			 1123
2201 			 2123 			 1025
2079 			 1966 			 983
1545 			 1683 			 743
1256 			 1657 			 605
448 			 842 			 275
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
375 			 263 			 107
261 			 248 			 70
218 			 251 			 71
202 			 169 			 51
165 			 237 			 49
63 			 116 			 26
Max memory allocated: 8635561472; Memory allocated: 3846014464
Epoch [40/1000] took 96.94133257865906s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4217172786825543, train accuracy: 0.4623624500925114
Val mean loss: 1.685193087996506, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2735 			 1998 			 1130
2141 			 2123 			 990
2171 			 1966 			 1007
1534 			 1683 			 733
1267 			 1657 			 621
421 			 842 			 267
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
374 			 263 			 107
278 			 248 			 74
198 			 251 			 68
195 			 169 			 50
178 			 237 			 53
61 			 116 			 28
Max memory allocated: 8635561472; Memory allocated: 3815408128
Epoch [41/1000] took 97.10863614082336s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4207113758425847, train accuracy: 0.46158340636868245
Val mean loss: 1.6732141593607461, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2743 			 1998 			 1138
2263 			 2123 			 1031
2005 			 1966 			 953
1496 			 1683 			 719
1314 			 1657 			 624
448 			 842 			 275
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 95
244 			 248 			 64
282 			 251 			 88
226 			 169 			 55
159 			 237 			 49
62 			 116 			 28
Max memory allocated: 8635561472; Memory allocated: 3846374912
Epoch [42/1000] took 97.19031310081482s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4206001561750132, train accuracy: 0.4630441133508618
Val mean loss: 1.6666309368319627, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2651 			 1998 			 1115
2108 			 2123 			 986
2207 			 1966 			 1006
1547 			 1683 			 742
1310 			 1657 			 624
446 			 842 			 282
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
380 			 263 			 109
275 			 248 			 69
211 			 251 			 68
212 			 169 			 53
155 			 237 			 48
51 			 116 			 22
Max memory allocated: 8635561472; Memory allocated: 3811148288
Epoch [43/1000] took 97.3232216835022s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4213918503199783, train accuracy: 0.46440743986756255
Val mean loss: 1.686114878189273, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2700 			 1998 			 1121
2256 			 2123 			 1034
2068 			 1966 			 970
1494 			 1683 			 739
1317 			 1657 			 633
434 			 842 			 272
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
388 			 263 			 109
270 			 248 			 73
238 			 251 			 76
199 			 169 			 51
137 			 237 			 40
52 			 116 			 23
Max memory allocated: 8635561472; Memory allocated: 3819667968
Epoch [44/1000] took 96.92602181434631s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.419940192751424, train accuracy: 0.4630441133508618
Val mean loss: 1.6712289554316824, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2823 			 1998 			 1152
2109 			 2123 			 992
2149 			 1966 			 1006
1521 			 1683 			 744
1249 			 1657 			 599
418 			 842 			 262
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 94
334 			 248 			 82
213 			 251 			 71
194 			 169 			 51
162 			 237 			 50
70 			 116 			 27
Max memory allocated: 8635561472; Memory allocated: 3821797888
Epoch [45/1000] took 96.90675139427185s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4179500066602713, train accuracy: 0.4636283961437336
Val mean loss: 1.6890456909086646, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2638 			 1998 			 1115
2274 			 2123 			 1036
2093 			 1966 			 973
1497 			 1683 			 737
1331 			 1657 			 630
436 			 842 			 270
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
363 			 263 			 110
261 			 248 			 66
239 			 251 			 77
216 			 169 			 55
141 			 237 			 42
64 			 116 			 30
Max memory allocated: 8763005440; Memory allocated: 3891956224
Epoch [46/1000] took 96.84560632705688s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4168172981880165, train accuracy: 0.46664719057357096
Val mean loss: 1.6738702058792114, val accuracy: 0.30062305295950154

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2694 			 1998 			 1132
2185 			 2123 			 1017
2080 			 1966 			 985
1602 			 1683 			 776
1262 			 1657 			 608
446 			 842 			 274
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 99
269 			 248 			 73
277 			 251 			 89
171 			 169 			 45
165 			 237 			 52
62 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3819667968
Epoch [47/1000] took 97.40375876426697s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4166834469524872, train accuracy: 0.4630441133508618
Val mean loss: 1.695309240643571, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2748 			 1998 			 1132
2209 			 2123 			 1023
2114 			 1966 			 982
1455 			 1683 			 723
1317 			 1657 			 628
426 			 842 			 267
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 102
261 			 248 			 64
264 			 251 			 82
213 			 169 			 52
136 			 237 			 41
66 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3845555712
Epoch [48/1000] took 97.07290768623352s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.417288467148754, train accuracy: 0.4673288538319213
Val mean loss: 1.6744520228083541, val accuracy: 0.30218068535825543

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2703 			 1998 			 1140
2184 			 2123 			 1013
2210 			 1966 			 1017
1444 			 1683 			 728
1277 			 1657 			 615
451 			 842 			 286
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
277 			 248 			 74
211 			 251 			 75
242 			 169 			 63
172 			 237 			 51
65 			 116 			 29
Max memory allocated: 8763005440; Memory allocated: 3891956224
Epoch [49/1000] took 97.28539538383484s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4178002731079624, train accuracy: 0.46937384360697243
Val mean loss: 1.6703536248788602, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2637 			 1998 			 1119
2199 			 2123 			 1024
2108 			 1966 			 997
1599 			 1683 			 772
1272 			 1657 			 623
454 			 842 			 285
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
384 			 263 			 112
292 			 248 			 74
213 			 251 			 67
192 			 169 			 52
148 			 237 			 46
55 			 116 			 26
Max memory allocated: 8763005440; Memory allocated: 3846374912
Epoch [50/1000] took 97.23063564300537s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.417371567907363, train accuracy: 0.4651864835913916
Val mean loss: 1.6835650205612183, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2799 			 1998 			 1154
2161 			 2123 			 1002
2138 			 1966 			 998
1463 			 1683 			 733
1269 			 1657 			 611
439 			 842 			 279
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
360 			 263 			 108
273 			 248 			 73
207 			 251 			 72
223 			 169 			 57
161 			 237 			 47
60 			 116 			 27
Max memory allocated: 8763005440; Memory allocated: 3815408128
Epoch [51/1000] took 97.23519277572632s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.417092191467404, train accuracy: 0.4667445710390496
Val mean loss: 1.6956393020909006, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2679 			 1998 			 1114
2224 			 2123 			 1028
2040 			 1966 			 977
1554 			 1683 			 762
1327 			 1657 			 629
445 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 95
311 			 248 			 76
243 			 251 			 79
204 			 169 			 51
156 			 237 			 46
62 			 116 			 27
Max memory allocated: 8763005440; Memory allocated: 3846768128
Epoch [52/1000] took 97.24896097183228s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4148451899442347, train accuracy: 0.46742623429739993
Val mean loss: 1.6671120335416096, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2691 			 1998 			 1127
2195 			 2123 			 1019
2118 			 1966 			 998
1498 			 1683 			 745
1306 			 1657 			 628
461 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
378 			 263 			 112
303 			 248 			 77
194 			 251 			 68
206 			 169 			 56
155 			 237 			 46
48 			 116 			 23
Max memory allocated: 8763005440; Memory allocated: 3849487872
Epoch [53/1000] took 97.51677513122559s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4154200802710941, train accuracy: 0.47054240919271595
Val mean loss: 1.671346350413997, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2637 			 1998 			 1118
2226 			 2123 			 1026
2099 			 1966 			 995
1557 			 1683 			 770
1307 			 1657 			 645
443 			 842 			 278
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
383 			 263 			 111
257 			 248 			 64
216 			 251 			 69
206 			 169 			 51
158 			 237 			 46
64 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3815408128
Epoch [54/1000] took 97.52303647994995s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4158050397474817, train accuracy: 0.4664524296426137
Val mean loss: 1.6898230052575833, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2651 			 1998 			 1136
2231 			 2123 			 1026
2107 			 1966 			 980
1517 			 1683 			 743
1314 			 1657 			 625
449 			 842 			 280
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
418 			 263 			 121
250 			 248 			 66
222 			 251 			 72
189 			 169 			 48
154 			 237 			 46
51 			 116 			 24
Max memory allocated: 8763005440; Memory allocated: 3851617792
Epoch [55/1000] took 97.64483165740967s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4157992846497864, train accuracy: 0.4691790826760152
Val mean loss: 1.6760654740217256, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2727 			 1998 			 1147
2227 			 2123 			 1031
2046 			 1966 			 986
1549 			 1683 			 761
1302 			 1657 			 627
418 			 842 			 266
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
373 			 263 			 109
247 			 248 			 67
280 			 251 			 86
179 			 169 			 48
145 			 237 			 45
60 			 116 			 27
Max memory allocated: 8763005440; Memory allocated: 3847357952
Epoch [56/1000] took 97.25421738624573s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4157865857038172, train accuracy: 0.46830265848670755
Val mean loss: 1.677429850508527, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2740 			 1998 			 1148
2223 			 2123 			 1031
2136 			 1966 			 1010
1448 			 1683 			 727
1280 			 1657 			 616
442 			 842 			 277
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 103
237 			 248 			 63
214 			 251 			 70
241 			 169 			 61
182 			 237 			 51
64 			 116 			 29
Max memory allocated: 8763005440; Memory allocated: 3815408128
Epoch [57/1000] took 96.9328293800354s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4131522791407933, train accuracy: 0.4692764631414938
Val mean loss: 1.669348004387646, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2756 			 1998 			 1156
2065 			 2123 			 988
2143 			 1966 			 1012
1576 			 1683 			 759
1281 			 1657 			 627
448 			 842 			 277
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 91
341 			 248 			 82
210 			 251 			 70
209 			 169 			 52
167 			 237 			 51
66 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3823927808
Epoch [58/1000] took 97.09386444091797s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4139460789451719, train accuracy: 0.46878956081410067
Val mean loss: 1.6822624206542969, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2609 			 1998 			 1107
2308 			 2123 			 1055
2107 			 1966 			 997
1506 			 1683 			 748
1287 			 1657 			 624
452 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
372 			 263 			 109
229 			 248 			 63
245 			 251 			 78
216 			 169 			 54
163 			 237 			 48
59 			 116 			 24
Max memory allocated: 8763005440; Memory allocated: 3846669824
Epoch [59/1000] took 97.01536154747009s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4133475302164429, train accuracy: 0.4655760054533061
Val mean loss: 1.6788961771057873, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2719 			 1998 			 1124
2127 			 2123 			 992
2166 			 1966 			 1020
1484 			 1683 			 745
1307 			 1657 			 619
466 			 842 			 281
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 97
289 			 248 			 72
223 			 251 			 72
245 			 169 			 62
159 			 237 			 46
55 			 116 			 26
Max memory allocated: 8763005440; Memory allocated: 3849487872
Epoch [60/1000] took 97.46395301818848s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.413488994877658, train accuracy: 0.47180835524393805
Val mean loss: 1.6845860801092007, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2655 			 1998 			 1126
2222 			 2123 			 1040
2097 			 1966 			 998
1526 			 1683 			 749
1323 			 1657 			 646
446 			 842 			 286
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
357 			 263 			 100
272 			 248 			 72
219 			 251 			 72
239 			 169 			 64
136 			 237 			 42
61 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3796238848
Epoch [61/1000] took 97.2761652469635s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4139416563176663, train accuracy: 0.46859479988314345
Val mean loss: 1.6697452213706039, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2717 			 1998 			 1130
2176 			 2123 			 1030
2093 			 1966 			 978
1579 			 1683 			 777
1271 			 1657 			 625
433 			 842 			 272
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 101
325 			 248 			 82
220 			 251 			 75
195 			 169 			 51
147 			 237 			 43
55 			 116 			 27
Max memory allocated: 8763005440; Memory allocated: 3809018368
Epoch [62/1000] took 96.70688056945801s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4117384970745193, train accuracy: 0.4688869412795793
Val mean loss: 1.6794887374087077, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2695 			 1998 			 1142
2265 			 2123 			 1046
2072 			 1966 			 981
1515 			 1683 			 754
1248 			 1657 			 602
474 			 842 			 290
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
365 			 263 			 108
254 			 248 			 68
273 			 251 			 85
184 			 169 			 45
156 			 237 			 46
52 			 116 			 25
Max memory allocated: 8763005440; Memory allocated: 3849487872
Epoch [63/1000] took 97.12976098060608s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4122225703480087, train accuracy: 0.46947122407245107
Val mean loss: 1.6761136694652279, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2692 			 1998 			 1131
2143 			 2123 			 1019
2162 			 1966 			 1017
1515 			 1683 			 739
1332 			 1657 			 640
425 			 842 			 275
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 106
295 			 248 			 71
248 			 251 			 76
190 			 169 			 47
153 			 237 			 49
69 			 116 			 30
Max memory allocated: 8763005440; Memory allocated: 3846997504
Epoch [64/1000] took 96.86410093307495s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4103594691582557, train accuracy: 0.47161359431298083
Val mean loss: 1.6660188727262544, val accuracy: 0.30218068535825543

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2694 			 1998 			 1136
2232 			 2123 			 1040
2161 			 1966 			 1014
1464 			 1683 			 746
1274 			 1657 			 624
444 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 99
259 			 248 			 67
221 			 251 			 76
255 			 169 			 65
162 			 237 			 51
66 			 116 			 30
Max memory allocated: 8763005440; Memory allocated: 3891726848
Epoch [65/1000] took 97.10452699661255s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4114617521517745, train accuracy: 0.4720031161748953
Val mean loss: 1.6724894337537812, val accuracy: 0.30218068535825543

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2664 			 1998 			 1128
2126 			 2123 			 1007
2137 			 1966 			 1012
1543 			 1683 			 763
1361 			 1657 			 658
438 			 842 			 279
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 96
309 			 248 			 79
201 			 251 			 72
240 			 169 			 64
153 			 237 			 49
69 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3811148288
Epoch [66/1000] took 97.44934868812561s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4114420057457184, train accuracy: 0.4719057357094167
Val mean loss: 1.6776650795122472, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2693 			 1998 			 1139
2265 			 2123 			 1039
2049 			 1966 			 990
1539 			 1683 			 761
1294 			 1657 			 640
429 			 842 			 277
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 101
280 			 248 			 70
225 			 251 			 74
214 			 169 			 56
173 			 237 			 51
71 			 116 			 28
Max memory allocated: 8763005440; Memory allocated: 3849487872
Epoch [67/1000] took 97.4066367149353s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4105091488621317, train accuracy: 0.47093193105463044
Val mean loss: 1.678748558207256, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2664 			 1998 			 1132
2214 			 2123 			 1036
2140 			 1966 			 1007
1517 			 1683 			 755
1272 			 1657 			 618
462 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
387 			 263 			 108
239 			 248 			 65
248 			 251 			 78
199 			 169 			 51
159 			 237 			 48
52 			 116 			 23
Max memory allocated: 8763005440; Memory allocated: 3846080000
Epoch [68/1000] took 97.01248598098755s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4073248366329159, train accuracy: 0.47687213944882656
Val mean loss: 1.681097271965771, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2589 			 1998 			 1120
2173 			 2123 			 1041
2200 			 1966 			 1043
1521 			 1683 			 765
1334 			 1657 			 642
452 			 842 			 286
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
412 			 263 			 118
261 			 248 			 73
185 			 251 			 64
237 			 169 			 63
135 			 237 			 43
54 			 116 			 23
Max memory allocated: 8763005440; Memory allocated: 3891595776
Epoch [69/1000] took 97.29607057571411s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4108667340234062, train accuracy: 0.47015288733080146
Val mean loss: 1.6811236114036747, val accuracy: 0.30062305295950154

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2796 			 1998 			 1167
2143 			 2123 			 1014
2106 			 1966 			 999
1546 			 1683 			 759
1276 			 1657 			 630
402 			 842 			 259
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 98
291 			 248 			 75
234 			 251 			 78
212 			 169 			 56
161 			 237 			 49
63 			 116 			 30
Max memory allocated: 8763005440; Memory allocated: 3846047232
Epoch [70/1000] took 97.11519646644592s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4084481026896063, train accuracy: 0.4732690622261174
Val mean loss: 1.6986641593095733, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2569 			 1998 			 1109
2239 			 2123 			 1036
2104 			 1966 			 1007
1552 			 1683 			 768
1325 			 1657 			 643
480 			 842 			 297
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
405 			 263 			 114
285 			 248 			 75
210 			 251 			 70
195 			 169 			 53
133 			 237 			 42
56 			 116 			 24
Max memory allocated: 8763005440; Memory allocated: 3845391872
Epoch [71/1000] took 97.52239513397217s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.409286273602756, train accuracy: 0.4747297692082968
Val mean loss: 1.709243588331269, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2737 			 1998 			 1166
2135 			 2123 			 1019
2121 			 1966 			 1005
1526 			 1683 			 756
1317 			 1657 			 650
433 			 842 			 279
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
360 			 263 			 105
322 			 248 			 81
206 			 251 			 72
184 			 169 			 47
154 			 237 			 44
58 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3846997504
Epoch [72/1000] took 96.99023962020874s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4079326502630645, train accuracy: 0.47404810594994645
Val mean loss: 1.6774555968075264, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2648 			 1998 			 1129
2262 			 2123 			 1038
2093 			 1966 			 1000
1493 			 1683 			 759
1315 			 1657 			 646
458 			 842 			 296
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
420 			 263 			 118
273 			 248 			 67
214 			 251 			 72
181 			 169 			 47
136 			 237 			 38
60 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3892283904
Epoch [73/1000] took 97.20785880088806s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.407204565600814, train accuracy: 0.4699581263998442
Val mean loss: 1.7011399472632058, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2752 			 1998 			 1156
2254 			 2123 			 1041
2050 			 1966 			 984
1466 			 1683 			 737
1294 			 1657 			 625
453 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 103
260 			 248 			 67
236 			 251 			 76
243 			 169 			 61
143 			 237 			 43
68 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3819667968
Epoch [74/1000] took 97.56561231613159s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4089079066600383, train accuracy: 0.4720031161748953
Val mean loss: 1.6626280458962046, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2650 			 1998 			 1127
2150 			 2123 			 1010
2159 			 1966 			 1021
1542 			 1683 			 765
1324 			 1657 			 644
444 			 842 			 280
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 98
273 			 248 			 73
201 			 251 			 70
255 			 169 			 65
162 			 237 			 49
62 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3846374912
Epoch [75/1000] took 97.3963713645935s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4080676479131633, train accuracy: 0.47297692082968157
Val mean loss: 1.6753125481489228, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2673 			 1998 			 1141
2157 			 2123 			 1028
2181 			 1966 			 1024
1527 			 1683 			 753
1283 			 1657 			 630
448 			 842 			 281
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 107
283 			 248 			 72
186 			 251 			 67
214 			 169 			 57
182 			 237 			 50
66 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3847357952
Epoch [76/1000] took 97.09146285057068s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4078616130389157, train accuracy: 0.4722952575713312
Val mean loss: 1.668107774199509, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2682 			 1998 			 1137
2283 			 2123 			 1057
2034 			 1966 			 978
1529 			 1683 			 755
1297 			 1657 			 636
444 			 842 			 287
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
359 			 263 			 103
266 			 248 			 67
232 			 251 			 72
202 			 169 			 50
161 			 237 			 44
64 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846735360
Epoch [77/1000] took 97.82590913772583s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4085535984544368, train accuracy: 0.47619047619047616
Val mean loss: 1.6829102330091523, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2624 			 1998 			 1130
2212 			 2123 			 1041
2090 			 1966 			 1005
1575 			 1683 			 782
1318 			 1657 			 644
450 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
419 			 263 			 120
235 			 248 			 64
243 			 251 			 74
192 			 169 			 49
144 			 237 			 44
51 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846080000
Epoch [78/1000] took 97.38639330863953s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4067043385401694, train accuracy: 0.47287954036420293
Val mean loss: 1.706281757936245, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2772 			 1998 			 1158
2159 			 2123 			 1021
2150 			 1966 			 1020
1459 			 1683 			 749
1285 			 1657 			 625
444 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 96
266 			 248 			 66
234 			 251 			 78
240 			 169 			 61
174 			 237 			 52
60 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3845883392
Epoch [79/1000] took 97.11558723449707s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4057047660477064, train accuracy: 0.4750219106047327
Val mean loss: 1.700022156645612, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2664 			 1998 			 1139
2199 			 2123 			 1035
2100 			 1966 			 1007
1542 			 1683 			 767
1317 			 1657 			 649
447 			 842 			 281
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 98
287 			 248 			 74
205 			 251 			 70
243 			 169 			 59
159 			 237 			 48
63 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3892087296
Epoch [80/1000] took 97.284343957901s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4054537495348685, train accuracy: 0.4741454864154251
Val mean loss: 1.67056760264606, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2636 			 1998 			 1127
2297 			 2123 			 1062
2089 			 1966 			 999
1537 			 1683 			 770
1250 			 1657 			 622
460 			 842 			 289
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
355 			 263 			 107
263 			 248 			 69
199 			 251 			 69
221 			 169 			 57
188 			 237 			 54
58 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3892349440
Epoch [81/1000] took 97.3774573802948s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4044680896206436, train accuracy: 0.47424286688090367
Val mean loss: 1.6912676764697563, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2695 			 1998 			 1141
2108 			 2123 			 1005
2104 			 1966 			 1003
1534 			 1683 			 767
1371 			 1657 			 667
457 			 842 			 287
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 100
299 			 248 			 78
219 			 251 			 75
223 			 169 			 56
141 			 237 			 45
64 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3847521792
Epoch [82/1000] took 97.06753182411194s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4034730311123382, train accuracy: 0.4746323887428182
Val mean loss: 1.6932826710910331, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2737 			 1998 			 1151
2191 			 2123 			 1034
2165 			 1966 			 1027
1495 			 1683 			 756
1239 			 1657 			 622
442 			 842 			 284
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 105
284 			 248 			 76
183 			 251 			 64
251 			 169 			 65
160 			 237 			 47
53 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3892349440
Epoch [83/1000] took 97.42973756790161s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4043445446038172, train accuracy: 0.4758983347940403
Val mean loss: 1.6811948811135642, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2753 			 1998 			 1166
2156 			 2123 			 1030
2047 			 1966 			 983
1578 			 1683 			 785
1309 			 1657 			 643
426 			 842 			 280
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 94
325 			 248 			 82
242 			 251 			 78
196 			 169 			 52
150 			 237 			 46
70 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3847357952
Epoch [84/1000] took 96.90589237213135s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4036455496077969, train accuracy: 0.47989093387866394
Val mean loss: 1.674863396621332, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2557 			 1998 			 1120
2220 			 2123 			 1059
2122 			 1966 			 1013
1520 			 1683 			 760
1357 			 1657 			 667
493 			 842 			 309
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
404 			 263 			 117
254 			 248 			 66
227 			 251 			 68
207 			 169 			 56
141 			 237 			 43
51 			 116 			 22
Max memory allocated: 8772412928; Memory allocated: 3806888448
Epoch [85/1000] took 96.79642486572266s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4015424262697451, train accuracy: 0.47287954036420293
Val mean loss: 1.6678294495838444, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2690 			 1998 			 1136
2288 			 2123 			 1064
2108 			 1966 			 991
1517 			 1683 			 772
1233 			 1657 			 615
433 			 842 			 278
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
396 			 263 			 112
196 			 248 			 51
229 			 251 			 72
204 			 169 			 51
204 			 237 			 55
55 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3845719552
Epoch [86/1000] took 96.48995685577393s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4004269916320515, train accuracy: 0.4782354659655273
Val mean loss: 1.6881724508797251, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2652 			 1998 			 1135
2134 			 2123 			 1029
2149 			 1966 			 1024
1492 			 1683 			 759
1390 			 1657 			 679
452 			 842 			 285
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
406 			 263 			 115
250 			 248 			 68
201 			 251 			 65
218 			 169 			 58
144 			 237 			 45
65 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846080000
Epoch [87/1000] took 97.32989263534546s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4032263183890845, train accuracy: 0.47716428084526247
Val mean loss: 1.6698999579359846, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2722 			 1998 			 1158
2208 			 2123 			 1043
2026 			 1966 			 986
1573 			 1683 			 786
1268 			 1657 			 634
472 			 842 			 293
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 101
213 			 248 			 55
289 			 251 			 88
181 			 169 			 46
209 			 237 			 59
51 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3855877632
Epoch [88/1000] took 97.2699339389801s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4053098182440547, train accuracy: 0.4758009543285617
Val mean loss: 1.6818384222868012, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2627 			 1998 			 1115
2125 			 2123 			 1016
2190 			 1966 			 1040
1486 			 1683 			 758
1385 			 1657 			 669
456 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
387 			 263 			 104
311 			 248 			 81
217 			 251 			 70
184 			 169 			 50
127 			 237 			 40
58 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3847357952
Epoch [89/1000] took 97.05324220657349s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4017063267877168, train accuracy: 0.47862498782744184
Val mean loss: 1.6815006122356508, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2661 			 1998 			 1139
2219 			 2123 			 1056
2101 			 1966 			 1010
1539 			 1683 			 771
1276 			 1657 			 641
473 			 842 			 298
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 103
269 			 248 			 70
223 			 251 			 75
213 			 169 			 52
187 			 237 			 54
52 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3849487872
Epoch [90/1000] took 97.39550805091858s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4019323787956595, train accuracy: 0.47843022689648457
Val mean loss: 1.7032478524417412, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2685 			 1998 			 1148
2211 			 2123 			 1048
2130 			 1966 			 1020
1540 			 1683 			 773
1288 			 1657 			 646
415 			 842 			 278
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
352 			 263 			 101
246 			 248 			 64
205 			 251 			 71
223 			 169 			 60
184 			 237 			 48
74 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3847357952
Epoch [91/1000] took 97.26616430282593s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4023414723969694, train accuracy: 0.4747297692082968
Val mean loss: 1.6937134265899658, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2701 			 1998 			 1147
2205 			 2123 			 1038
2040 			 1966 			 998
1578 			 1683 			 774
1282 			 1657 			 630
463 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 94
266 			 248 			 70
230 			 251 			 77
193 			 169 			 48
206 			 237 			 54
58 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3823927808
Epoch [92/1000] took 97.38608193397522s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.401609337961191, train accuracy: 0.4780407050345701
Val mean loss: 1.6849997072685055, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2658 			 1998 			 1139
2204 			 2123 			 1052
2103 			 1966 			 1020
1497 			 1683 			 762
1368 			 1657 			 659
439 			 842 			 277
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 100
239 			 248 			 64
317 			 251 			 100
191 			 169 			 47
139 			 237 			 43
72 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3809018368
Epoch [93/1000] took 97.68888354301453s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4023706515630086, train accuracy: 0.47755380270717696
Val mean loss: 1.687189387112129, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2706 			 1998 			 1157
2145 			 2123 			 1026
2176 			 1966 			 1034
1522 			 1683 			 769
1258 			 1657 			 624
462 			 842 			 294
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 99
269 			 248 			 68
245 			 251 			 78
209 			 169 			 55
168 			 237 			 52
57 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846997504
Epoch [94/1000] took 96.8904139995575s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4004139254026324, train accuracy: 0.47794332456909144
Val mean loss: 1.6793303780439424, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2623 			 1998 			 1133
2128 			 2123 			 1025
2160 			 1966 			 1029
1511 			 1683 			 760
1400 			 1657 			 678
447 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 100
349 			 248 			 83
216 			 251 			 73
197 			 169 			 54
119 			 237 			 38
66 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3819667968
Epoch [95/1000] took 97.43182516098022s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4001517039593134, train accuracy: 0.48144902132632195
Val mean loss: 1.6883968958040563, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2651 			 1998 			 1140
2245 			 2123 			 1059
2069 			 1966 			 1013
1571 			 1683 			 800
1261 			 1657 			 631
472 			 842 			 301
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
419 			 263 			 123
263 			 248 			 71
227 			 251 			 71
181 			 169 			 45
138 			 237 			 42
56 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3845883392
Epoch [96/1000] took 97.04221200942993s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4001494582939742, train accuracy: 0.47687213944882656
Val mean loss: 1.6850638680341767, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2782 			 1998 			 1167
2146 			 2123 			 1019
2084 			 1966 			 1015
1496 			 1683 			 758
1309 			 1657 			 648
452 			 842 			 290
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 93
274 			 248 			 70
265 			 251 			 84
216 			 169 			 55
150 			 237 			 47
67 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3892316672
Epoch [97/1000] took 97.12074589729309s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.400598087414774, train accuracy: 0.4785276073619632
Val mean loss: 1.6840504698637055, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2651 			 1998 			 1134
2177 			 2123 			 1042
2165 			 1966 			 1035
1496 			 1683 			 762
1343 			 1657 			 663
437 			 842 			 278
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 101
343 			 248 			 81
203 			 251 			 71
215 			 169 			 59
128 			 237 			 38
60 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [98/1000] took 97.09501504898071s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.4000724854870377, train accuracy: 0.4794040315512708
Val mean loss: 1.673499889490081, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2635 			 1998 			 1141
2234 			 2123 			 1061
2079 			 1966 			 1000
1527 			 1683 			 770
1319 			 1657 			 646
475 			 842 			 305
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
369 			 263 			 103
287 			 248 			 74
223 			 251 			 70
207 			 169 			 51
137 			 237 			 41
61 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3845883392
Epoch [99/1000] took 97.07363367080688s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3980313305542849, train accuracy: 0.4793066510857922
Val mean loss: 1.65801833751725, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2632 			 1998 			 1145
2267 			 2123 			 1069
2107 			 1966 			 1017
1552 			 1683 			 773
1255 			 1657 			 636
456 			 842 			 282
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
427 			 263 			 126
221 			 248 			 60
241 			 251 			 76
190 			 169 			 49
153 			 237 			 43
52 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3813278208
Epoch [100/1000] took 97.0756893157959s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3983092530865535, train accuracy: 0.4809621189989288
Val mean loss: 1.679676733365873, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2705 			 1998 			 1156
2131 			 2123 			 1029
2115 			 1966 			 1025
1560 			 1683 			 787
1306 			 1657 			 640
452 			 842 			 302
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
386 			 263 			 109
266 			 248 			 66
236 			 251 			 76
206 			 169 			 54
140 			 237 			 44
50 			 116 			 21
Max memory allocated: 8772412928; Memory allocated: 3891825152
Epoch [101/1000] took 97.55691623687744s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3981336254939856, train accuracy: 0.4797935534131853
Val mean loss: 1.6830062255626772, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2656 			 1998 			 1156
2186 			 2123 			 1046
2144 			 1966 			 1027
1478 			 1683 			 758
1349 			 1657 			 653
456 			 842 			 287
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
370 			 263 			 107
302 			 248 			 73
221 			 251 			 76
221 			 169 			 58
119 			 237 			 40
51 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3845883392
Epoch [102/1000] took 97.30646181106567s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.399999337032948, train accuracy: 0.4793066510857922
Val mean loss: 1.6809499292838863, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2682 			 1998 			 1145
2242 			 2123 			 1068
2093 			 1966 			 1017
1543 			 1683 			 783
1249 			 1657 			 624
460 			 842 			 285
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 104
295 			 248 			 76
234 			 251 			 77
180 			 169 			 45
158 			 237 			 48
64 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846899200
Epoch [103/1000] took 97.0747230052948s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3956498910333508, train accuracy: 0.48057259713701433
Val mean loss: 1.6783656928597428, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2763 			 1998 			 1172
2132 			 2123 			 1029
2129 			 1966 			 1035
1510 			 1683 			 778
1293 			 1657 			 635
442 			 842 			 286
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 96
290 			 248 			 70
228 			 251 			 75
205 			 169 			 50
206 			 237 			 57
62 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [104/1000] took 96.94307613372803s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3960407656672587, train accuracy: 0.47950141201674945
Val mean loss: 1.6842841229787686, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2539 			 1998 			 1117
2253 			 2123 			 1054
2124 			 1966 			 1030
1503 			 1683 			 759
1391 			 1657 			 670
459 			 842 			 294
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
407 			 263 			 120
254 			 248 			 65
228 			 251 			 76
217 			 169 			 55
128 			 237 			 40
50 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3845883392
Epoch [105/1000] took 97.26966762542725s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.39640392692661, train accuracy: 0.47969617294770667
Val mean loss: 1.6925995960468199, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2750 			 1998 			 1161
2150 			 2123 			 1039
2081 			 1966 			 1018
1514 			 1683 			 770
1349 			 1657 			 664
425 			 842 			 274
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 94
309 			 248 			 70
259 			 251 			 83
228 			 169 			 56
116 			 237 			 38
72 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3891694080
Epoch [106/1000] took 97.03948593139648s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3946614302578746, train accuracy: 0.4800856948096212
Val mean loss: 1.6823045684070121, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2678 			 1998 			 1144
2211 			 2123 			 1069
2107 			 1966 			 1024
1560 			 1683 			 784
1259 			 1657 			 621
454 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 100
243 			 248 			 65
301 			 251 			 91
189 			 169 			 49
154 			 237 			 48
64 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3846047232
Epoch [107/1000] took 97.4253363609314s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3974762649922357, train accuracy: 0.4808647385334502
Val mean loss: 1.6775685345254294, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2613 			 1998 			 1140
2180 			 2123 			 1047
2227 			 1966 			 1057
1476 			 1683 			 755
1318 			 1657 			 652
455 			 842 			 287
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 103
293 			 248 			 79
239 			 251 			 78
196 			 169 			 51
159 			 237 			 45
59 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3851617792
Epoch [108/1000] took 96.95783138275146s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3941980492660189, train accuracy: 0.48388353296328757
Val mean loss: 1.6796865259728782, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2656 			 1998 			 1148
2229 			 2123 			 1065
2104 			 1966 			 1029
1510 			 1683 			 778
1326 			 1657 			 666
444 			 842 			 283
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
354 			 263 			 105
287 			 248 			 72
248 			 251 			 79
190 			 169 			 49
144 			 237 			 46
61 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3845391872
Epoch [109/1000] took 97.33148169517517s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.395068557091591, train accuracy: 0.48144902132632195
Val mean loss: 1.6925955138555386, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2701 			 1998 			 1151
2167 			 2123 			 1040
2115 			 1966 			 1024
1540 			 1683 			 797
1289 			 1657 			 644
457 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 96
288 			 248 			 69
238 			 251 			 75
198 			 169 			 51
189 			 237 			 55
68 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3846080000
Epoch [110/1000] took 97.21912217140198s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3947292031528793, train accuracy: 0.4801830752750998
Val mean loss: 1.6905549851859487, val accuracy: 0.3029595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2668 			 1998 			 1151
2142 			 2123 			 1029
2189 			 1966 			 1035
1494 			 1683 			 762
1327 			 1657 			 660
449 			 842 			 294
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
375 			 263 			 114
292 			 248 			 74
194 			 251 			 68
236 			 169 			 63
134 			 237 			 43
53 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3891561984
Epoch [111/1000] took 97.42809510231018s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.394229130581532, train accuracy: 0.4824228259811082
Val mean loss: 1.686600528112272, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2662 			 1998 			 1147
2225 			 2123 			 1055
2077 			 1966 			 1008
1533 			 1683 			 786
1287 			 1657 			 650
485 			 842 			 308
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
394 			 263 			 117
258 			 248 			 65
220 			 251 			 70
210 			 169 			 54
150 			 237 			 45
52 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3846047232
Epoch [112/1000] took 97.01887369155884s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3921015685963853, train accuracy: 0.48388353296328757
Val mean loss: 1.6913639655927333, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2708 			 1998 			 1165
2203 			 2123 			 1060
2075 			 1966 			 1017
1505 			 1683 			 781
1306 			 1657 			 647
472 			 842 			 299
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 92
251 			 248 			 64
264 			 251 			 86
212 			 169 			 51
191 			 237 			 52
64 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846964736
Epoch [113/1000] took 96.94204068183899s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3945644536865092, train accuracy: 0.48339663063589444
Val mean loss: 1.6678237202690869, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2661 			 1998 			 1157
2135 			 2123 			 1042
2162 			 1966 			 1039
1507 			 1683 			 767
1367 			 1657 			 677
437 			 842 			 282
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 100
265 			 248 			 66
252 			 251 			 78
216 			 169 			 53
142 			 237 			 45
71 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3846964736
Epoch [114/1000] took 97.06016325950623s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.393401436345228, train accuracy: 0.4832992501704158
Val mean loss: 1.6740435681691983, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2595 			 1998 			 1128
2191 			 2123 			 1058
2125 			 1966 			 1032
1559 			 1683 			 788
1321 			 1657 			 655
478 			 842 			 302
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
378 			 263 			 109
242 			 248 			 62
231 			 251 			 74
178 			 169 			 46
194 			 237 			 53
61 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3892447744
Epoch [115/1000] took 97.14053225517273s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3939856584198378, train accuracy: 0.4834940111013731
Val mean loss: 1.6725444008664387, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2704 			 1998 			 1174
2153 			 2123 			 1042
2077 			 1966 			 1014
1567 			 1683 			 781
1323 			 1657 			 666
445 			 842 			 288
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 90
319 			 248 			 78
239 			 251 			 80
193 			 169 			 49
178 			 237 			 54
65 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3892218368
Epoch [116/1000] took 97.21262264251709s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3921598600821332, train accuracy: 0.48427305482520205
Val mean loss: 1.6666753815441597, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2687 			 1998 			 1168
2224 			 2123 			 1069
2130 			 1966 			 1028
1479 			 1683 			 758
1288 			 1657 			 653
461 			 842 			 297
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 98
264 			 248 			 65
252 			 251 			 83
194 			 169 			 51
188 			 237 			 53
58 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [117/1000] took 97.47466945648193s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3933348633418574, train accuracy: 0.48193592365371507
Val mean loss: 1.6932399825351994, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2661 			 1998 			 1148
2116 			 2123 			 1030
2162 			 1966 			 1039
1511 			 1683 			 770
1360 			 1657 			 667
459 			 842 			 295
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 98
305 			 248 			 76
225 			 251 			 73
198 			 169 			 51
169 			 237 			 50
52 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3845391872
Epoch [118/1000] took 96.92633986473083s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3908737344533855, train accuracy: 0.4850520985490311
Val mean loss: 1.6955592603218266, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2598 			 1998 			 1144
2303 			 2123 			 1087
2064 			 1966 			 1015
1523 			 1683 			 781
1329 			 1657 			 658
452 			 842 			 296
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
354 			 263 			 106
260 			 248 			 67
229 			 251 			 76
221 			 169 			 53
157 			 237 			 47
63 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3849487872
Epoch [119/1000] took 97.21313714981079s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.390213884668558, train accuracy: 0.48602590320381733
Val mean loss: 1.6987781844488004, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2618 			 1998 			 1145
2187 			 2123 			 1051
2107 			 1966 			 1032
1543 			 1683 			 789
1357 			 1657 			 677
457 			 842 			 297
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
362 			 263 			 107
289 			 248 			 74
222 			 251 			 73
210 			 169 			 57
131 			 237 			 40
70 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3845555712
Epoch [120/1000] took 97.00868821144104s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3908991902788108, train accuracy: 0.4822280650501509
Val mean loss: 1.6899310757474202, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2708 			 1998 			 1159
2200 			 2123 			 1061
1992 			 1966 			 988
1598 			 1683 			 798
1286 			 1657 			 646
485 			 842 			 300
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
345 			 263 			 102
275 			 248 			 71
275 			 251 			 85
180 			 169 			 47
151 			 237 			 47
58 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846080000
Epoch [121/1000] took 96.71061706542969s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3898942270011545, train accuracy: 0.4841756743597234
Val mean loss: 1.6837589798904047, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2676 			 1998 			 1159
2139 			 2123 			 1038
2219 			 1966 			 1062
1466 			 1683 			 759
1329 			 1657 			 664
440 			 842 			 290
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 99
278 			 248 			 71
209 			 251 			 71
252 			 169 			 66
152 			 237 			 44
66 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3846702592
Epoch [122/1000] took 96.9716260433197s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3889810533909783, train accuracy: 0.48378615249780893
Val mean loss: 1.690086931717105, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2568 			 1998 			 1123
2258 			 2123 			 1073
2061 			 1966 			 1014
1623 			 1683 			 807
1286 			 1657 			 649
473 			 842 			 302
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 102
246 			 248 			 62
278 			 251 			 87
202 			 169 			 49
168 			 237 			 49
62 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3846374912
Epoch [123/1000] took 97.3294723033905s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3913476474559938, train accuracy: 0.4834940111013731
Val mean loss: 1.6915201065016956, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2696 			 1998 			 1154
2126 			 2123 			 1042
2133 			 1966 			 1040
1549 			 1683 			 784
1315 			 1657 			 658
450 			 842 			 287
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 105
284 			 248 			 72
233 			 251 			 76
197 			 169 			 52
159 			 237 			 46
65 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3847357952
Epoch [124/1000] took 96.76953530311584s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3881027208310421, train accuracy: 0.48252020644658683
Val mean loss: 1.6904149840517741, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2661 			 1998 			 1149
2168 			 2123 			 1048
2135 			 1966 			 1026
1528 			 1683 			 776
1298 			 1657 			 658
479 			 842 			 298
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 99
246 			 248 			 63
231 			 251 			 77
215 			 169 			 54
196 			 237 			 58
61 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846047232
Epoch [125/1000] took 97.16601395606995s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3897143331272208, train accuracy: 0.4841756743597234
Val mean loss: 1.680567485530202, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2558 			 1998 			 1129
2210 			 2123 			 1069
2130 			 1966 			 1025
1530 			 1683 			 777
1378 			 1657 			 673
463 			 842 			 299
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
419 			 263 			 116
253 			 248 			 62
197 			 251 			 64
213 			 169 			 58
135 			 237 			 42
67 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846833664
Epoch [126/1000] took 96.57035565376282s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3876710179438843, train accuracy: 0.4835913915668517
Val mean loss: 1.6850316379128434, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2706 			 1998 			 1169
2136 			 2123 			 1039
2065 			 1966 			 1011
1589 			 1683 			 800
1309 			 1657 			 657
464 			 842 			 290
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
371 			 263 			 109
285 			 248 			 74
248 			 251 			 80
181 			 169 			 49
139 			 237 			 43
60 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3846768128
Epoch [127/1000] took 97.4611930847168s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3896831028929382, train accuracy: 0.4841756743597234
Val mean loss: 1.7031027340307467, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2718 			 1998 			 1163
2180 			 2123 			 1061
2116 			 1966 			 1029
1486 			 1683 			 768
1308 			 1657 			 652
461 			 842 			 299
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 94
302 			 248 			 72
249 			 251 			 80
211 			 169 			 54
152 			 237 			 46
66 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3847259648
Epoch [128/1000] took 96.9704077243805s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3872397740681965, train accuracy: 0.48738922972051807
Val mean loss: 1.6761353597408388, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2590 			 1998 			 1142
2270 			 2123 			 1088
2066 			 1966 			 1019
1542 			 1683 			 776
1326 			 1657 			 676
475 			 842 			 304
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 110
254 			 248 			 67
261 			 251 			 82
197 			 169 			 48
147 			 237 			 45
59 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3845555712
Epoch [129/1000] took 97.02007579803467s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3892856825177915, train accuracy: 0.48456519622163796
Val mean loss: 1.6918370665573492, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2673 			 1998 			 1164
2134 			 2123 			 1036
2145 			 1966 			 1044
1513 			 1683 			 775
1334 			 1657 			 659
470 			 842 			 298
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 97
262 			 248 			 69
247 			 251 			 79
204 			 169 			 52
159 			 237 			 45
62 			 116 			 23
Max memory allocated: 8772412928; Memory allocated: 3846407680
Epoch [130/1000] took 96.82184147834778s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.387533265482228, train accuracy: 0.4855390008764242
Val mean loss: 1.6809949467821819, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2622 			 1998 			 1157
2179 			 2123 			 1054
2134 			 1966 			 1033
1559 			 1683 			 781
1305 			 1657 			 660
470 			 842 			 301
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
359 			 263 			 103
283 			 248 			 70
246 			 251 			 76
182 			 169 			 44
157 			 237 			 49
57 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3809018368
Epoch [131/1000] took 97.23824858665466s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3883678905689085, train accuracy: 0.48378615249780893
Val mean loss: 1.690896955932059, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2709 			 1998 			 1164
2243 			 2123 			 1075
2068 			 1966 			 1008
1525 			 1683 			 779
1276 			 1657 			 647
448 			 842 			 295
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 100
257 			 248 			 68
247 			 251 			 80
225 			 169 			 57
159 			 237 			 47
67 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3891561984
Epoch [132/1000] took 96.82198286056519s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3860992412329463, train accuracy: 0.4871944687895608
Val mean loss: 1.6808291673660278, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2583 			 1998 			 1147
2189 			 2123 			 1063
2164 			 1966 			 1051
1517 			 1683 			 775
1345 			 1657 			 672
471 			 842 			 295
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
374 			 263 			 110
234 			 248 			 64
241 			 251 			 77
227 			 169 			 58
150 			 237 			 44
58 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3852600832
Epoch [133/1000] took 96.93111896514893s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3858503794001642, train accuracy: 0.48738922972051807
Val mean loss: 1.6956661677942044, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2678 			 1998 			 1165
2094 			 2123 			 1041
2105 			 1966 			 1026
1581 			 1683 			 803
1349 			 1657 			 672
462 			 842 			 298
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 102
297 			 248 			 76
233 			 251 			 73
231 			 169 			 58
119 			 237 			 37
62 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846047232
Epoch [134/1000] took 97.16724729537964s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3868672684345662, train accuracy: 0.48738922972051807
Val mean loss: 1.6964476341154517, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2657 			 1998 			 1160
2226 			 2123 			 1067
2106 			 1966 			 1029
1549 			 1683 			 795
1267 			 1657 			 649
464 			 842 			 305
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 92
274 			 248 			 69
224 			 251 			 74
225 			 169 			 58
181 			 237 			 55
69 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [135/1000] took 97.19276189804077s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.386967657511108, train accuracy: 0.4890446976336547
Val mean loss: 1.6854112700718205, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2656 			 1998 			 1164
2169 			 2123 			 1062
2068 			 1966 			 1024
1547 			 1683 			 796
1377 			 1657 			 683
452 			 842 			 293
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 103
285 			 248 			 73
218 			 251 			 72
224 			 169 			 58
141 			 237 			 43
70 			 116 			 31
Max memory allocated: 8772412928; Memory allocated: 3846538752
Epoch [136/1000] took 97.06706309318542s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3875314799424643, train accuracy: 0.48602590320381733
Val mean loss: 1.688385736651537, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2694 			 1998 			 1177
2181 			 2123 			 1050
2086 			 1966 			 1021
1516 			 1683 			 775
1330 			 1657 			 668
462 			 842 			 300
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 92
299 			 248 			 74
221 			 251 			 73
232 			 169 			 58
166 			 237 			 51
65 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3821797888
Epoch [137/1000] took 97.39750456809998s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3871719161298044, train accuracy: 0.48758399065147534
Val mean loss: 1.675865874057863, val accuracy: 0.30218068535825543

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2662 			 1998 			 1147
2142 			 2123 			 1061
2145 			 1966 			 1050
1546 			 1683 			 794
1310 			 1657 			 654
464 			 842 			 301
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 96
270 			 248 			 74
203 			 251 			 73
249 			 169 			 65
172 			 237 			 50
66 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3851617792
Epoch [138/1000] took 97.34282088279724s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3849595796282046, train accuracy: 0.4868049469276463
Val mean loss: 1.6857984647518252, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2544 			 1998 			 1122
2187 			 2123 			 1058
2111 			 1966 			 1016
1529 			 1683 			 794
1400 			 1657 			 692
498 			 842 			 317
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
363 			 263 			 107
328 			 248 			 85
172 			 251 			 62
253 			 169 			 64
116 			 237 			 37
52 			 116 			 23
Max memory allocated: 8772412928; Memory allocated: 3892414976
Epoch [139/1000] took 97.31246829032898s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.386140266311503, train accuracy: 0.4863180446002532
Val mean loss: 1.6703640484228366, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2666 			 1998 			 1159
2265 			 2123 			 1079
2009 			 1966 			 994
1616 			 1683 			 822
1278 			 1657 			 656
435 			 842 			 284
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 102
269 			 248 			 67
262 			 251 			 85
199 			 169 			 50
162 			 237 			 50
63 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3819667968
Epoch [140/1000] took 96.99632668495178s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3834714284195706, train accuracy: 0.4887525562372188
Val mean loss: 1.6603754613457657, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2612 			 1998 			 1160
2189 			 2123 			 1058
2141 			 1966 			 1044
1572 			 1683 			 800
1292 			 1657 			 656
463 			 842 			 301
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 93
296 			 248 			 76
205 			 251 			 70
216 			 169 			 57
201 			 237 			 59
63 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846374912
Epoch [141/1000] took 97.37762641906738s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3850929012922484, train accuracy: 0.4889473171681761
Val mean loss: 1.703169799432522, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2556 			 1998 			 1139
2223 			 2123 			 1073
2133 			 1966 			 1034
1536 			 1683 			 796
1357 			 1657 			 681
464 			 842 			 298
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 101
277 			 248 			 72
197 			 251 			 65
219 			 169 			 57
181 			 237 			 49
61 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3809018368
Epoch [142/1000] took 97.1632034778595s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3837241030927758, train accuracy: 0.48797351251338983
Val mean loss: 1.6760419868841403, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2623 			 1998 			 1151
2173 			 2123 			 1059
2068 			 1966 			 1014
1583 			 1683 			 800
1361 			 1657 			 684
461 			 842 			 303
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 99
272 			 248 			 69
248 			 251 			 79
204 			 169 			 49
156 			 237 			 46
68 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3900608000
Epoch [143/1000] took 97.00467157363892s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3837145726628763, train accuracy: 0.4861232836692959
Val mean loss: 1.6844620559273698, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2655 			 1998 			 1149
2184 			 2123 			 1062
2132 			 1966 			 1037
1530 			 1683 			 791
1307 			 1657 			 654
461 			 842 			 299
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 97
275 			 248 			 69
237 			 251 			 77
228 			 169 			 55
148 			 237 			 45
67 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3891923456
Epoch [144/1000] took 97.45328140258789s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3848334840524976, train accuracy: 0.4890446976336547
Val mean loss: 1.7004763585765188, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2590 			 1998 			 1145
2190 			 2123 			 1074
2164 			 1966 			 1049
1508 			 1683 			 783
1345 			 1657 			 666
472 			 842 			 305
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
365 			 263 			 109
292 			 248 			 70
209 			 251 			 70
229 			 169 			 57
141 			 237 			 46
48 			 116 			 23
Max memory allocated: 8772412928; Memory allocated: 3821797888
Epoch [145/1000] took 97.08011245727539s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3818079248024295, train accuracy: 0.4910896874087058
Val mean loss: 1.690658176817545, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2676 			 1998 			 1168
2188 			 2123 			 1071
2082 			 1966 			 1028
1554 			 1683 			 802
1317 			 1657 			 674
452 			 842 			 300
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 100
272 			 248 			 70
210 			 251 			 71
244 			 169 			 65
145 			 237 			 48
73 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3845555712
Epoch [146/1000] took 97.23752069473267s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3827388661672764, train accuracy: 0.48865517577174017
Val mean loss: 1.6894313649433415, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2593 			 1998 			 1150
2227 			 2123 			 1072
2051 			 1966 			 1011
1547 			 1683 			 793
1378 			 1657 			 689
473 			 842 			 303
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
356 			 263 			 107
266 			 248 			 67
244 			 251 			 78
218 			 169 			 56
129 			 237 			 41
71 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3846997504
Epoch [147/1000] took 97.07280087471008s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.382669794225247, train accuracy: 0.48972636089200505
Val mean loss: 1.696571376265549, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2656 			 1998 			 1165
2164 			 2123 			 1049
2118 			 1966 			 1042
1539 			 1683 			 791
1301 			 1657 			 660
491 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 104
297 			 248 			 74
223 			 251 			 74
219 			 169 			 58
147 			 237 			 45
63 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846211072
Epoch [148/1000] took 96.89051151275635s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3815987084142145, train accuracy: 0.4905054046158341
Val mean loss: 1.6915320623211745, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2601 			 1998 			 1151
2261 			 2123 			 1091
2052 			 1966 			 1015
1563 			 1683 			 799
1304 			 1657 			 666
488 			 842 			 315
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 101
254 			 248 			 66
239 			 251 			 81
229 			 169 			 59
159 			 237 			 50
56 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3821797888
Epoch [149/1000] took 96.89964890480042s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3810750268330083, train accuracy: 0.49089492647774857
Val mean loss: 1.6937197650351175, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2711 			 1998 			 1179
2196 			 2123 			 1064
2045 			 1966 			 1022
1537 			 1683 			 801
1329 			 1657 			 677
451 			 842 			 298
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 87
264 			 248 			 66
296 			 251 			 92
209 			 169 			 52
162 			 237 			 47
72 			 116 			 31
Max memory allocated: 8772412928; Memory allocated: 3856729600
Epoch [150/1000] took 96.87211966514587s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3798109853750449, train accuracy: 0.4896289804265264
Val mean loss: 1.6983157454467401, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2585 			 1998 			 1137
2191 			 2123 			 1063
2154 			 1966 			 1046
1537 			 1683 			 802
1334 			 1657 			 673
468 			 842 			 307
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 98
277 			 248 			 70
241 			 251 			 79
209 			 169 			 53
164 			 237 			 48
65 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3845064192
Epoch [151/1000] took 96.83636736869812s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3790029101654004, train accuracy: 0.49216087252897067
Val mean loss: 1.6853219125328995, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2646 			 1998 			 1173
2107 			 2123 			 1035
2186 			 1966 			 1065
1561 			 1683 			 805
1299 			 1657 			 663
470 			 842 			 313
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 95
351 			 248 			 83
202 			 251 			 66
199 			 169 			 52
161 			 237 			 49
60 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3845719552
Epoch [152/1000] took 96.94376754760742s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3779856453060733, train accuracy: 0.49011588275391954
Val mean loss: 1.6978251934051514, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2608 			 1998 			 1146
2201 			 2123 			 1069
2068 			 1966 			 1019
1536 			 1683 			 798
1365 			 1657 			 690
491 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
364 			 263 			 106
280 			 248 			 68
225 			 251 			 72
222 			 169 			 57
143 			 237 			 45
50 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3892218368
Epoch [153/1000] took 97.17741417884827s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3829569441507168, train accuracy: 0.4917713506670562
Val mean loss: 1.6857784230534623, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2594 			 1998 			 1158
2273 			 2123 			 1103
2068 			 1966 			 1032
1578 			 1683 			 800
1301 			 1657 			 661
455 			 842 			 296
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 99
251 			 248 			 63
281 			 251 			 92
189 			 169 			 48
171 			 237 			 52
64 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3845391872
Epoch [154/1000] took 97.40457153320312s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3812671747534446, train accuracy: 0.49410848183854317
Val mean loss: 1.6932499583174543, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2645 			 1998 			 1170
2150 			 2123 			 1068
2149 			 1966 			 1060
1506 			 1683 			 794
1344 			 1657 			 670
475 			 842 			 312
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
354 			 263 			 103
289 			 248 			 74
215 			 251 			 70
218 			 169 			 56
157 			 237 			 45
51 			 116 			 22
Max memory allocated: 8772412928; Memory allocated: 3846211072
Epoch [155/1000] took 96.9465708732605s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3778698737747572, train accuracy: 0.4883630343753043
Val mean loss: 1.6987056034367258, val accuracy: 0.30062305295950154

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2610 			 1998 			 1144
2228 			 2123 			 1072
2062 			 1966 			 1026
1529 			 1683 			 788
1378 			 1657 			 688
462 			 842 			 297
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 107
273 			 248 			 68
236 			 251 			 80
228 			 169 			 59
133 			 237 			 44
63 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846538752
Epoch [156/1000] took 97.14175581932068s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.379896085953044, train accuracy: 0.4913818288051417
Val mean loss: 1.6850931586288824, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2675 			 1998 			 1166
2168 			 2123 			 1068
2132 			 1966 			 1053
1528 			 1683 			 793
1311 			 1657 			 662
455 			 842 			 304
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 91
292 			 248 			 70
235 			 251 			 79
236 			 169 			 58
160 			 237 			 49
62 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3802628608
Epoch [157/1000] took 97.3706305027008s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3780238115898917, train accuracy: 0.4938163404421073
Val mean loss: 1.6831271968236783, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2643 			 1998 			 1176
2168 			 2123 			 1062
2122 			 1966 			 1054
1548 			 1683 			 805
1312 			 1657 			 667
476 			 842 			 307
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
276 			 263 			 87
331 			 248 			 80
241 			 251 			 76
206 			 169 			 54
162 			 237 			 50
68 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846047232
Epoch [158/1000] took 97.02200484275818s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3808358168676262, train accuracy: 0.49001850228844096
Val mean loss: 1.688356754256458, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2576 			 1998 			 1146
2273 			 2123 			 1092
2057 			 1966 			 1017
1550 			 1683 			 800
1342 			 1657 			 675
471 			 842 			 302
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
369 			 263 			 105
262 			 248 			 64
219 			 251 			 72
228 			 169 			 61
144 			 237 			 45
62 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3891595776
Epoch [159/1000] took 96.7288088798523s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3773990375601985, train accuracy: 0.4938163404421073
Val mean loss: 1.6875712697098895, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2597 			 1998 			 1152
2152 			 2123 			 1062
2092 			 1966 			 1047
1649 			 1683 			 834
1305 			 1657 			 669
474 			 842 			 307
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 99
293 			 248 			 72
250 			 251 			 83
184 			 169 			 49
167 			 237 			 49
60 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846931968
Epoch [160/1000] took 96.47250366210938s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3770222537614103, train accuracy: 0.4924530139254066
Val mean loss: 1.6929089819512717, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2568 			 1998 			 1151
2223 			 2123 			 1082
2107 			 1966 			 1040
1547 			 1683 			 795
1347 			 1657 			 675
477 			 842 			 314
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
363 			 263 			 107
262 			 248 			 64
227 			 251 			 74
199 			 169 			 51
171 			 237 			 51
62 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3855877632
Epoch [161/1000] took 96.96594572067261s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3781678617186264, train accuracy: 0.4907001655467913
Val mean loss: 1.6990986742624423, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2589 			 1998 			 1141
2178 			 2123 			 1067
2090 			 1966 			 1035
1585 			 1683 			 815
1357 			 1657 			 680
470 			 842 			 301
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 101
274 			 248 			 69
252 			 251 			 81
200 			 169 			 50
151 			 237 			 46
65 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3846997504
Epoch [162/1000] took 96.48784112930298s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3765907031353388, train accuracy: 0.4915765897360989
Val mean loss: 1.6845100100447492, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2650 			 1998 			 1169
2166 			 2123 			 1055
2159 			 1966 			 1053
1522 			 1683 			 795
1286 			 1657 			 661
486 			 842 			 315
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
355 			 263 			 104
286 			 248 			 73
221 			 251 			 72
217 			 169 			 56
144 			 237 			 43
61 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3813278208
Epoch [163/1000] took 96.93124723434448s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.375873615065839, train accuracy: 0.49196611159801346
Val mean loss: 1.6852663697265997, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2605 			 1998 			 1146
2205 			 2123 			 1081
2032 			 1966 			 1019
1626 			 1683 			 830
1316 			 1657 			 661
485 			 842 			 315
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 98
313 			 248 			 76
242 			 251 			 75
171 			 169 			 45
159 			 237 			 49
60 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3891595776
Epoch [164/1000] took 97.39026665687561s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.377441072761084, train accuracy: 0.49323205764923556
Val mean loss: 1.7008607532919906, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2586 			 1998 			 1166
2190 			 2123 			 1064
2138 			 1966 			 1044
1508 			 1683 			 790
1380 			 1657 			 694
467 			 842 			 307
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
368 			 263 			 111
267 			 248 			 68
244 			 251 			 79
216 			 169 			 54
124 			 237 			 41
65 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3821797888
Epoch [165/1000] took 96.70373487472534s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.376927034134434, train accuracy: 0.4940111013730646
Val mean loss: 1.684337383363305, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2682 			 1998 			 1176
2157 			 2123 			 1070
2155 			 1966 			 1062
1510 			 1683 			 786
1282 			 1657 			 662
483 			 842 			 317
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
288 			 263 			 90
305 			 248 			 75
221 			 251 			 74
238 			 169 			 59
171 			 237 			 53
61 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3806888448
Epoch [166/1000] took 97.04766058921814s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3751790991453368, train accuracy: 0.49547180835524396
Val mean loss: 1.6864174342736966, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2646 			 1998 			 1173
2230 			 2123 			 1092
2063 			 1966 			 1029
1560 			 1683 			 811
1322 			 1657 			 679
448 			 842 			 304
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 94
278 			 248 			 69
231 			 251 			 78
210 			 169 			 53
187 			 237 			 53
79 			 116 			 31
Max memory allocated: 8772412928; Memory allocated: 3846997504
Epoch [167/1000] took 97.17764353752136s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3746439103396881, train accuracy: 0.4942058623040218
Val mean loss: 1.6953879769255475, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2568 			 1998 			 1146
2158 			 2123 			 1066
2117 			 1966 			 1043
1567 			 1683 			 803
1382 			 1657 			 702
477 			 842 			 315
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 104
284 			 248 			 69
251 			 251 			 79
181 			 169 			 49
145 			 237 			 44
70 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [168/1000] took 96.70727610588074s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3752263008248398, train accuracy: 0.49235563345992794
Val mean loss: 1.6798819070909081, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2677 			 1998 			 1169
2218 			 2123 			 1085
2098 			 1966 			 1042
1514 			 1683 			 799
1288 			 1657 			 656
474 			 842 			 305
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 98
257 			 248 			 63
257 			 251 			 79
225 			 169 			 58
157 			 237 			 48
61 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3892349440
Epoch [169/1000] took 96.24922561645508s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3743628397166172, train accuracy: 0.4961534716135943
Val mean loss: 1.6901141434181026, val accuracy: 0.3014018691588785

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2592 			 1998 			 1156
2162 			 2123 			 1072
2088 			 1966 			 1041
1573 			 1683 			 820
1376 			 1657 			 696
478 			 842 			 310
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 106
258 			 248 			 67
285 			 251 			 90
193 			 169 			 52
148 			 237 			 45
58 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3900608000
Epoch [170/1000] took 97.07170534133911s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.375814105120032, train accuracy: 0.4931346771837569
Val mean loss: 1.6901963745675437, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2633 			 1998 			 1169
2182 			 2123 			 1070
2100 			 1966 			 1040
1565 			 1683 			 792
1324 			 1657 			 682
465 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
354 			 263 			 102
281 			 248 			 72
246 			 251 			 79
204 			 169 			 52
135 			 237 			 40
64 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3849487872
Epoch [171/1000] took 97.19581127166748s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3748104044952867, train accuracy: 0.48953159996104784
Val mean loss: 1.6937754590336869, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2692 			 1998 			 1175
2138 			 2123 			 1046
2153 			 1966 			 1052
1528 			 1683 			 794
1304 			 1657 			 663
454 			 842 			 297
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 97
302 			 248 			 74
215 			 251 			 69
231 			 169 			 56
139 			 237 			 46
69 			 116 			 31
Max memory allocated: 8772412928; Memory allocated: 3846374912
Epoch [172/1000] took 96.88721585273743s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3740477353984322, train accuracy: 0.49479014509689356
Val mean loss: 1.706592545276735, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2622 			 1998 			 1161
2194 			 2123 			 1075
2162 			 1966 			 1068
1508 			 1683 			 794
1322 			 1657 			 676
461 			 842 			 307
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 99
308 			 248 			 77
202 			 251 			 68
227 			 169 			 60
144 			 237 			 43
73 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3892382208
Epoch [173/1000] took 97.14203929901123s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3746559036855015, train accuracy: 0.4922582529944493
Val mean loss: 1.685819875903246, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2579 			 1998 			 1148
2251 			 2123 			 1087
1989 			 1966 			 1010
1639 			 1683 			 824
1319 			 1657 			 672
492 			 842 			 314
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 100
225 			 248 			 57
274 			 251 			 86
202 			 169 			 51
185 			 237 			 53
60 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3891661312
Epoch [174/1000] took 97.20684552192688s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3735641270783088, train accuracy: 0.49625085207907293
Val mean loss: 1.7024512203728281, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2633 			 1998 			 1164
2195 			 2123 			 1078
2138 			 1966 			 1067
1492 			 1683 			 798
1358 			 1657 			 687
453 			 842 			 302
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 97
245 			 248 			 60
242 			 251 			 79
228 			 169 			 56
155 			 237 			 46
72 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3847521792
Epoch [175/1000] took 97.09551906585693s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3733568436631531, train accuracy: 0.49303729671827834
Val mean loss: 1.69114798452796, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2629 			 1998 			 1162
2140 			 2123 			 1061
2105 			 1966 			 1048
1529 			 1683 			 795
1377 			 1657 			 688
489 			 842 			 309
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 93
301 			 248 			 71
232 			 251 			 76
228 			 169 			 59
148 			 237 			 44
72 			 116 			 31
Max memory allocated: 8772412928; Memory allocated: 3846080000
Epoch [176/1000] took 96.81779074668884s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3717250144370248, train accuracy: 0.4979063199922096
Val mean loss: 1.677873663785981, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2580 			 1998 			 1159
2215 			 2123 			 1090
2087 			 1966 			 1041
1570 			 1683 			 821
1341 			 1657 			 688
476 			 842 			 314
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
288 			 263 			 94
286 			 248 			 74
261 			 251 			 87
215 			 169 			 52
170 			 237 			 50
64 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846080000
Epoch [177/1000] took 97.4563319683075s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3731130042180093, train accuracy: 0.4975167981302951
Val mean loss: 1.6961777471914523, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2576 			 1998 			 1165
2274 			 2123 			 1115
2065 			 1966 			 1029
1578 			 1683 			 819
1304 			 1657 			 672
472 			 842 			 309
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 97
240 			 248 			 66
286 			 251 			 90
188 			 169 			 50
183 			 237 			 53
60 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [178/1000] took 96.90973210334778s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3701116622794083, train accuracy: 0.49761417859577367
Val mean loss: 1.7037431757624557, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2559 			 1998 			 1157
2151 			 2123 			 1069
2156 			 1966 			 1068
1553 			 1683 			 811
1364 			 1657 			 693
486 			 842 			 312
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
360 			 263 			 105
293 			 248 			 76
237 			 251 			 75
194 			 169 			 47
141 			 237 			 42
59 			 116 			 25
Max memory allocated: 8772412928; Memory allocated: 3845883392
Epoch [179/1000] took 97.0726490020752s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3711215142520417, train accuracy: 0.4956665692862012
Val mean loss: 1.699671521419432, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2602 			 1998 			 1147
2198 			 2123 			 1091
2148 			 1966 			 1044
1530 			 1683 			 807
1309 			 1657 			 680
482 			 842 			 321
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
369 			 263 			 107
284 			 248 			 71
218 			 251 			 73
210 			 169 			 56
148 			 237 			 43
55 			 116 			 23
Max memory allocated: 8772412928; Memory allocated: 3845719552
Epoch [180/1000] took 97.09700012207031s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.372003098885961, train accuracy: 0.4944006232349791
Val mean loss: 1.6968802678875807, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2670 			 1998 			 1170
2196 			 2123 			 1077
2058 			 1966 			 1025
1559 			 1683 			 810
1346 			 1657 			 692
440 			 842 			 303
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 103
242 			 248 			 57
273 			 251 			 84
206 			 169 			 50
145 			 237 			 45
72 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3817538048
Epoch [181/1000] took 96.51037049293518s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3680549016994108, train accuracy: 0.4959587106826371
Val mean loss: 1.6761675869546286, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2637 			 1998 			 1174
2202 			 2123 			 1087
2087 			 1966 			 1031
1500 			 1683 			 792
1346 			 1657 			 688
497 			 842 			 321
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 95
244 			 248 			 60
263 			 251 			 86
254 			 169 			 61
145 			 237 			 46
69 			 116 			 29
Max memory allocated: 8772412928; Memory allocated: 3815408128
Epoch [182/1000] took 96.96936583518982s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3708855281366366, train accuracy: 0.4965429934755088
Val mean loss: 1.6844685920854894, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2586 			 1998 			 1160
2125 			 2123 			 1057
2101 			 1966 			 1043
1638 			 1683 			 846
1350 			 1657 			 684
469 			 842 			 309
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 103
272 			 248 			 67
259 			 251 			 82
189 			 169 			 51
155 			 237 			 48
63 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3813278208
Epoch [183/1000] took 96.98915982246399s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3700427266296196, train accuracy: 0.49829584185412407
Val mean loss: 1.685513353929287, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2616 			 1998 			 1177
2196 			 2123 			 1094
2160 			 1966 			 1061
1512 			 1683 			 800
1304 			 1657 			 671
481 			 842 			 314
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 99
267 			 248 			 67
245 			 251 			 79
217 			 169 			 55
154 			 237 			 48
62 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3798368768
Epoch [184/1000] took 97.4426748752594s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.372537378210145, train accuracy: 0.49479014509689356
Val mean loss: 1.7036333520237992, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2622 			 1998 			 1166
2134 			 2123 			 1053
2121 			 1966 			 1045
1572 			 1683 			 826
1342 			 1657 			 683
478 			 842 			 308
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 90
345 			 248 			 76
227 			 251 			 74
191 			 169 			 49
159 			 237 			 49
64 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3823927808
Epoch [185/1000] took 97.44938468933105s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3707824484952884, train accuracy: 0.49625085207907293
Val mean loss: 1.7123788100917166, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2640 			 1998 			 1167
2207 			 2123 			 1078
2136 			 1966 			 1056
1451 			 1683 			 778
1351 			 1657 			 699
484 			 842 			 318
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 96
285 			 248 			 73
215 			 251 			 74
243 			 169 			 62
152 			 237 			 49
55 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3846538752
Epoch [186/1000] took 97.42589163780212s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3691752818514624, train accuracy: 0.49849060278508134
Val mean loss: 1.7046611192749768, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2574 			 1998 			 1147
2215 			 2123 			 1095
2107 			 1966 			 1055
1544 			 1683 			 807
1347 			 1657 			 693
482 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 100
263 			 248 			 67
225 			 251 			 75
247 			 169 			 61
162 			 237 			 49
59 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846538752
Epoch [187/1000] took 97.063973903656s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.368811732140657, train accuracy: 0.49849060278508134
Val mean loss: 1.6906833270700967, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2593 			 1998 			 1161
2218 			 2123 			 1094
2088 			 1966 			 1045
1579 			 1683 			 823
1321 			 1657 			 685
470 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
355 			 263 			 104
246 			 248 			 66
274 			 251 			 89
188 			 169 			 48
161 			 237 			 49
60 			 116 			 27
Max memory allocated: 8772412928; Memory allocated: 3846047232
Epoch [188/1000] took 96.8701503276825s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3671458372817233, train accuracy: 0.4956665692862012
Val mean loss: 1.6875672747449177, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2593 			 1998 			 1153
2162 			 2123 			 1077
2081 			 1966 			 1032
1610 			 1683 			 830
1359 			 1657 			 683
464 			 842 			 315
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
343 			 263 			 100
258 			 248 			 64
298 			 251 			 95
171 			 169 			 46
149 			 237 			 46
65 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3809018368
Epoch [189/1000] took 96.94517421722412s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3673372810874773, train accuracy: 0.4996591683708248
Val mean loss: 1.6987437155188583, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2589 			 1998 			 1164
2175 			 2123 			 1078
2128 			 1966 			 1060
1519 			 1683 			 814
1367 			 1657 			 697
491 			 842 			 318
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 92
297 			 248 			 72
233 			 251 			 76
236 			 169 			 58
161 			 237 			 48
62 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3845391872
Epoch [190/1000] took 97.24733901023865s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.368125223667822, train accuracy: 0.4985879832505599
Val mean loss: 1.6842546986370552, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2647 			 1998 			 1172
2122 			 2123 			 1050
2163 			 1966 			 1074
1565 			 1683 			 840
1300 			 1657 			 673
472 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
277 			 263 			 89
333 			 248 			 80
225 			 251 			 76
213 			 169 			 54
173 			 237 			 52
63 			 116 			 26
Max memory allocated: 8772412928; Memory allocated: 3813278208
Epoch [191/1000] took 97.52523827552795s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.369430247868333, train accuracy: 0.4959587106826371
Val mean loss: 1.690094052291498, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2522 			 1998 			 1124
2300 			 2123 			 1111
2116 			 1966 			 1056
1504 			 1683 			 801
1333 			 1657 			 685
494 			 842 			 316
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
391 			 263 			 114
223 			 248 			 59
230 			 251 			 73
228 			 169 			 60
158 			 237 			 44
54 			 116 			 24
Max memory allocated: 8772412928; Memory allocated: 3891988992
Epoch [192/1000] took 97.83512616157532s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3689213415543982, train accuracy: 0.49849060278508134
Val mean loss: 1.7061096895031813, val accuracy: 0.2982866043613707

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2681 			 1998 			 1177
2134 			 2123 			 1076
2073 			 1966 			 1034
1586 			 1683 			 834
1347 			 1657 			 696
448 			 842 			 302
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 97
265 			 248 			 71
240 			 251 			 81
227 			 169 			 60
154 			 237 			 46
64 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3819667968
Epoch [193/1000] took 96.68551993370056s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3661235307235955, train accuracy: 0.4973220371993378
Val mean loss: 1.6943943442367926, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2582 			 1998 			 1164
2151 			 2123 			 1061
2140 			 1966 			 1054
1585 			 1683 			 825
1313 			 1657 			 677
498 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 95
256 			 248 			 65
244 			 251 			 79
225 			 169 			 57
167 			 237 			 49
63 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3846768128
Epoch [194/1000] took 96.77521514892578s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3656993079408306, train accuracy: 0.4970298958029019
Val mean loss: 1.697064236896794, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2599 			 1998 			 1155
2173 			 2123 			 1068
2141 			 1966 			 1060
1541 			 1683 			 810
1363 			 1657 			 710
452 			 842 			 301
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 93
295 			 248 			 73
235 			 251 			 76
224 			 169 			 55
144 			 237 			 47
78 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3891561984
Epoch [195/1000] took 96.83777713775635s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3644418742426458, train accuracy: 0.4999513097672607
Val mean loss: 1.6852315082782652, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2569 			 1998 			 1159
2189 			 2123 			 1089
2130 			 1966 			 1064
1570 			 1683 			 823
1310 			 1657 			 679
501 			 842 			 320
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 91
253 			 248 			 59
235 			 251 			 75
234 			 169 			 60
187 			 237 			 53
70 			 116 			 28
Max memory allocated: 8772412928; Memory allocated: 3892087296
Epoch [196/1000] took 97.15497517585754s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3666003374295814, train accuracy: 0.49907488557795304
Val mean loss: 1.6888752826830236, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2560 			 1998 			 1146
2173 			 2123 			 1086
2048 			 1966 			 1042
1674 			 1683 			 853
1345 			 1657 			 687
469 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 94
253 			 248 			 62
230 			 251 			 77
233 			 169 			 60
172 			 237 			 48
80 			 116 			 30
Max memory allocated: 8772412928; Memory allocated: 3815408128
Epoch [197/1000] took 96.85015344619751s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3661633774498914, train accuracy: 0.4996591683708248
Val mean loss: 1.7210333027490756, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2621 			 1998 			 1172
2123 			 2123 			 1068
2122 			 1966 			 1062
1549 			 1683 			 812
1357 			 1657 			 697
497 			 842 			 320
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 100
300 			 248 			 72
256 			 251 			 82
186 			 169 			 48
143 			 237 			 45
65 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [198/1000] took 96.7959303855896s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3639447354824743, train accuracy: 0.4980037004576882
Val mean loss: 1.7127855056669654, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2667 			 1998 			 1180
2190 			 2123 			 1080
2087 			 1966 			 1049
1547 			 1683 			 816
1311 			 1657 			 678
467 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 96
282 			 248 			 69
242 			 251 			 80
229 			 169 			 57
166 			 237 			 52
69 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [199/1000] took 96.68023586273193s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.363635563033392, train accuracy: 0.4992696465089103
Val mean loss: 1.7094466569947033, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2578 			 1998 			 1165
2154 			 2123 			 1077
2128 			 1966 			 1046
1551 			 1683 			 820
1371 			 1657 			 697
487 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 100
294 			 248 			 74
222 			 251 			 74
243 			 169 			 61
128 			 237 			 40
58 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3855877632
Epoch [200/1000] took 97.21533536911011s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3666488244154742, train accuracy: 0.5012172558184829
Val mean loss: 1.6897646770244692, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2589 			 1998 			 1172
2218 			 2123 			 1104
2128 			 1966 			 1062
1552 			 1683 			 822
1300 			 1657 			 670
482 			 842 			 317
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
281 			 248 			 69
204 			 251 			 70
267 			 169 			 67
157 			 237 			 49
58 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3892447744
Epoch [201/1000] took 97.25007724761963s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3658040446284403, train accuracy: 0.5012172558184829
Val mean loss: 1.701704318930463, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2583 			 1998 			 1163
2188 			 2123 			 1088
2009 			 1966 			 1027
1672 			 1683 			 852
1345 			 1657 			 700
472 			 842 			 317
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 100
283 			 248 			 70
274 			 251 			 85
178 			 169 			 47
145 			 237 			 46
63 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3906997760
Epoch [202/1000] took 96.93457579612732s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.364763455227528, train accuracy: 0.5000486902327393
Val mean loss: 1.694652324769555, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2568 			 1998 			 1158
2173 			 2123 			 1082
2155 			 1966 			 1068
1584 			 1683 			 817
1317 			 1657 			 691
472 			 842 			 319
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 105
274 			 248 			 69
239 			 251 			 80
208 			 169 			 54
152 			 237 			 44
65 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3846768128
Epoch [203/1000] took 97.23829555511475s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3639767255367148, train accuracy: 0.5001460706982179
Val mean loss: 1.685853452217288, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2638 			 1998 			 1185
2195 			 2123 			 1086
2093 			 1966 			 1043
1541 			 1683 			 819
1325 			 1657 			 693
477 			 842 			 310
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 97
241 			 248 			 62
282 			 251 			 90
214 			 169 			 53
159 			 237 			 48
66 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3892349440
Epoch [204/1000] took 97.30266332626343s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3651642914501678, train accuracy: 0.4994644074398676
Val mean loss: 1.6978289848420678, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2592 			 1998 			 1171
2179 			 2123 			 1089
2125 			 1966 			 1059
1540 			 1683 			 800
1357 			 1657 			 698
476 			 842 			 312
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 95
260 			 248 			 66
272 			 251 			 85
234 			 169 			 58
139 			 237 			 45
67 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3845719552
Epoch [205/1000] took 96.98046159744263s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3625182659826547, train accuracy: 0.5000486902327393
Val mean loss: 1.6883756591052543, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2527 			 1998 			 1147
2199 			 2123 			 1089
2102 			 1966 			 1050
1622 			 1683 			 835
1325 			 1657 			 688
494 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
403 			 263 			 117
248 			 248 			 66
247 			 251 			 78
187 			 169 			 50
141 			 237 			 42
58 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [206/1000] took 97.00299859046936s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.363257147812769, train accuracy: 0.49956178790534617
Val mean loss: 1.68526809971507, val accuracy: 0.30062305295950154

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2647 			 1998 			 1180
2183 			 2123 			 1087
2138 			 1966 			 1073
1539 			 1683 			 816
1294 			 1657 			 667
468 			 842 			 307
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 101
294 			 248 			 72
223 			 251 			 75
223 			 169 			 61
151 			 237 			 49
60 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3802628608
Epoch [207/1000] took 97.1302740573883s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.363981003330504, train accuracy: 0.5005355925601325
Val mean loss: 1.7000890127042445, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2639 			 1998 			 1175
2155 			 2123 			 1066
2087 			 1966 			 1058
1563 			 1683 			 821
1370 			 1657 			 706
455 			 842 			 314
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 99
306 			 248 			 75
225 			 251 			 74
215 			 169 			 54
144 			 237 			 47
71 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3811148288
Epoch [208/1000] took 96.65965437889099s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3623563849666036, train accuracy: 0.4981010809231668
Val mean loss: 1.6771292163104545, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2591 			 1998 			 1164
2195 			 2123 			 1083
2086 			 1966 			 1043
1542 			 1683 			 805
1354 			 1657 			 696
501 			 842 			 324
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
289 			 248 			 71
208 			 251 			 71
263 			 169 			 66
135 			 237 			 45
72 			 116 			 31
Max memory allocated: 8857609728; Memory allocated: 3815408128
Epoch [209/1000] took 96.47396874427795s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3612958437928528, train accuracy: 0.5023858214042263
Val mean loss: 1.6941211601582968, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2575 			 1998 			 1173
2208 			 2123 			 1108
2061 			 1966 			 1034
1620 			 1683 			 829
1313 			 1657 			 686
492 			 842 			 329
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 102
284 			 248 			 76
233 			 251 			 77
195 			 169 			 50
163 			 237 			 50
58 			 116 			 24
Max memory allocated: 8857609728; Memory allocated: 3823927808
Epoch [210/1000] took 97.14434432983398s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3609412448057132, train accuracy: 0.5025805823351835
Val mean loss: 1.7096877098083496, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2566 			 1998 			 1162
2173 			 2123 			 1092
2094 			 1966 			 1059
1602 			 1683 			 841
1366 			 1657 			 695
468 			 842 			 312
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 103
290 			 248 			 70
259 			 251 			 82
187 			 169 			 49
129 			 237 			 41
69 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3846735360
Epoch [211/1000] took 97.3500247001648s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3636061833283613, train accuracy: 0.502483201869705
Val mean loss: 1.699265323034147, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2602 			 1998 			 1173
2185 			 2123 			 1092
2126 			 1966 			 1061
1523 			 1683 			 805
1346 			 1657 			 707
487 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
371 			 263 			 105
277 			 248 			 69
238 			 251 			 79
197 			 169 			 52
142 			 237 			 44
59 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [212/1000] took 97.45322871208191s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3619795736865463, train accuracy: 0.501704158145876
Val mean loss: 1.6868280724781315, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2575 			 1998 			 1159
2243 			 2123 			 1106
2066 			 1966 			 1045
1575 			 1683 			 826
1339 			 1657 			 698
471 			 842 			 318
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 98
268 			 248 			 67
245 			 251 			 79
230 			 169 			 57
152 			 237 			 46
64 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3821797888
Epoch [213/1000] took 96.97370076179504s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3617909548809966, train accuracy: 0.5009251144220469
Val mean loss: 1.6838826522594545, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2586 			 1998 			 1160
2204 			 2123 			 1103
2110 			 1966 			 1055
1540 			 1683 			 818
1352 			 1657 			 697
477 			 842 			 311
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
352 			 263 			 106
222 			 248 			 57
259 			 251 			 83
240 			 169 			 58
149 			 237 			 45
62 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3846866432
Epoch [214/1000] took 96.69607734680176s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.358356398214061, train accuracy: 0.5039439088518843
Val mean loss: 1.6823493910998832, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2569 			 1998 			 1163
2130 			 2123 			 1075
2109 			 1966 			 1062
1604 			 1683 			 845
1370 			 1657 			 709
487 			 842 			 321
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
352 			 263 			 107
281 			 248 			 71
239 			 251 			 76
197 			 169 			 53
152 			 237 			 47
63 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3845555712
Epoch [215/1000] took 97.10502409934998s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.359527418175219, train accuracy: 0.5034570065244912
Val mean loss: 1.6801580568639243, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2621 			 1998 			 1180
2167 			 2123 			 1094
2074 			 1966 			 1053
1553 			 1683 			 822
1367 			 1657 			 705
487 			 842 			 316
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 105
253 			 248 			 63
252 			 251 			 83
210 			 169 			 51
160 			 237 			 48
67 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3819667968
Epoch [216/1000] took 96.84395289421082s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3575554577361015, train accuracy: 0.505307235368585
Val mean loss: 1.6912234207478964, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2605 			 1998 			 1177
2167 			 2123 			 1096
2099 			 1966 			 1060
1603 			 1683 			 848
1309 			 1657 			 689
486 			 842 			 319
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 96
260 			 248 			 66
271 			 251 			 88
183 			 169 			 48
183 			 237 			 52
59 			 116 			 24
Max memory allocated: 8857609728; Memory allocated: 3854140928
Epoch [217/1000] took 96.87927722930908s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3586866993027684, train accuracy: 0.5010224948875256
Val mean loss: 1.6815524508313435, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2513 			 1998 			 1149
2175 			 2123 			 1079
2198 			 1966 			 1078
1523 			 1683 			 808
1382 			 1657 			 709
478 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
396 			 263 			 112
239 			 248 			 62
226 			 251 			 70
232 			 169 			 60
137 			 237 			 40
54 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3891628544
Epoch [218/1000] took 97.05653047561646s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3604413853998867, train accuracy: 0.5047229525757133
Val mean loss: 1.696897169438804, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2718 			 1998 			 1213
2113 			 2123 			 1066
2117 			 1966 			 1067
1542 			 1683 			 825
1311 			 1657 			 693
468 			 842 			 319
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
278 			 263 			 86
285 			 248 			 71
253 			 251 			 81
217 			 169 			 55
182 			 237 			 55
69 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3791979008
Epoch [219/1000] took 96.1808032989502s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3569917452298221, train accuracy: 0.5021910604732691
Val mean loss: 1.7044661219527082, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2556 			 1998 			 1164
2186 			 2123 			 1092
2148 			 1966 			 1065
1529 			 1683 			 805
1361 			 1657 			 709
489 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 99
287 			 248 			 74
218 			 251 			 74
226 			 169 			 60
157 			 237 			 47
65 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3846833664
Epoch [220/1000] took 96.55923748016357s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3585987948925695, train accuracy: 0.504528191644756
Val mean loss: 1.6777129027901627, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2575 			 1998 			 1159
2246 			 2123 			 1122
2039 			 1966 			 1037
1583 			 1683 			 838
1332 			 1657 			 697
494 			 842 			 328
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 103
221 			 248 			 53
272 			 251 			 85
212 			 169 			 54
159 			 237 			 49
70 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3846080000
Epoch [221/1000] took 97.03485369682312s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3577779650316804, train accuracy: 0.502483201869705
Val mean loss: 1.6875817077915842, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2623 			 1998 			 1187
2086 			 2123 			 1059
2167 			 1966 			 1072
1540 			 1683 			 819
1374 			 1657 			 704
479 			 842 			 319
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 90
319 			 248 			 76
223 			 251 			 72
218 			 169 			 55
146 			 237 			 48
78 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3819667968
Epoch [222/1000] took 96.25043773651123s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3555997442976337, train accuracy: 0.5054046158340637
Val mean loss: 1.7061075844415805, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2544 			 1998 			 1165
2182 			 2123 			 1092
2084 			 1966 			 1048
1597 			 1683 			 853
1358 			 1657 			 708
504 			 842 			 324
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 102
315 			 248 			 75
243 			 251 			 81
188 			 169 			 50
140 			 237 			 43
58 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3846538752
Epoch [223/1000] took 97.13546705245972s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.358724727808872, train accuracy: 0.50141201674944
Val mean loss: 1.7172130113694726, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2618 			 1998 			 1172
2235 			 2123 			 1098
2085 			 1966 			 1051
1488 			 1683 			 798
1369 			 1657 			 711
474 			 842 			 319
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 95
271 			 248 			 68
233 			 251 			 73
249 			 169 			 63
145 			 237 			 43
71 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3819667968
Epoch [224/1000] took 96.86364984512329s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.357117883512907, train accuracy: 0.5020936800077904
Val mean loss: 1.7036745868078091, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2588 			 1998 			 1175
2154 			 2123 			 1083
2122 			 1966 			 1064
1605 			 1683 			 839
1308 			 1657 			 669
492 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 96
282 			 248 			 70
233 			 251 			 77
218 			 169 			 55
171 			 237 			 52
68 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3819667968
Epoch [225/1000] took 96.80893421173096s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3564373470169733, train accuracy: 0.5042360502483202
Val mean loss: 1.681724118023384, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2581 			 1998 			 1173
2146 			 2123 			 1074
2149 			 1966 			 1072
1569 			 1683 			 832
1352 			 1657 			 706
472 			 842 			 321
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 101
304 			 248 			 74
234 			 251 			 73
198 			 169 			 53
150 			 237 			 48
74 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3846800896
Epoch [226/1000] took 96.33353638648987s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.35572388981733, train accuracy: 0.5041386697828416
Val mean loss: 1.6943953240790017, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2589 			 1998 			 1165
2202 			 2123 			 1097
2104 			 1966 			 1058
1536 			 1683 			 822
1337 			 1657 			 705
501 			 842 			 330
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 90
305 			 248 			 73
217 			 251 			 72
235 			 169 			 59
178 			 237 			 56
68 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3892185600
Epoch [227/1000] took 96.99272036552429s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3550206092287818, train accuracy: 0.5047229525757133
Val mean loss: 1.7022299185031797, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2531 			 1998 			 1166
2223 			 2123 			 1117
2036 			 1966 			 1039
1625 			 1683 			 837
1362 			 1657 			 695
492 			 842 			 329
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
357 			 263 			 103
293 			 248 			 72
245 			 251 			 82
197 			 169 			 52
142 			 237 			 49
50 			 116 			 23
Max memory allocated: 8857609728; Memory allocated: 3891561984
Epoch [228/1000] took 97.14963269233704s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3569789575267803, train accuracy: 0.5060862790924141
Val mean loss: 1.695586117302499, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2589 			 1998 			 1162
2214 			 2123 			 1112
2107 			 1966 			 1078
1538 			 1683 			 826
1347 			 1657 			 704
474 			 842 			 315
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 100
254 			 248 			 61
261 			 251 			 80
202 			 169 			 52
163 			 237 			 50
60 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3815408128
Epoch [229/1000] took 96.83059239387512s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.354857830986427, train accuracy: 0.5079365079365079
Val mean loss: 1.6808318364910964, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2620 			 1998 			 1184
2157 			 2123 			 1096
2117 			 1966 			 1075
1559 			 1683 			 838
1329 			 1657 			 702
487 			 842 			 321
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 93
278 			 248 			 65
233 			 251 			 79
207 			 169 			 51
178 			 237 			 51
68 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [230/1000] took 97.09229922294617s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3555383095488742, train accuracy: 0.5037491479209271
Val mean loss: 1.7031947752324545, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2568 			 1998 			 1155
2219 			 2123 			 1103
2061 			 1966 			 1055
1568 			 1683 			 827
1371 			 1657 			 713
482 			 842 			 320
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
368 			 263 			 108
224 			 248 			 54
280 			 251 			 92
194 			 169 			 50
153 			 237 			 49
65 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3846374912
Epoch [231/1000] took 97.00048279762268s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3551791353017741, train accuracy: 0.5081312688674652
Val mean loss: 1.6819763125442877, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2620 			 1998 			 1193
2092 			 2123 			 1073
2148 			 1966 			 1076
1585 			 1683 			 849
1336 			 1657 			 702
488 			 842 			 325
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 107
291 			 248 			 68
234 			 251 			 73
202 			 169 			 51
147 			 237 			 46
66 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3819667968
Epoch [232/1000] took 97.07823514938354s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3556955681411649, train accuracy: 0.5060862790924141
Val mean loss: 1.7103208070848046, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2610 			 1998 			 1174
2171 			 2123 			 1097
2144 			 1966 			 1088
1541 			 1683 			 824
1316 			 1657 			 691
487 			 842 			 323
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 92
333 			 248 			 78
183 			 251 			 63
236 			 169 			 61
162 			 237 			 50
64 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3845555712
Epoch [233/1000] took 97.48658013343811s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3559355030178653, train accuracy: 0.5064758009543285
Val mean loss: 1.6923268829903952, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2550 			 1998 			 1157
2232 			 2123 			 1111
2085 			 1966 			 1062
1565 			 1683 			 837
1362 			 1657 			 715
475 			 842 			 319
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
375 			 263 			 107
276 			 248 			 68
208 			 251 			 71
217 			 169 			 54
151 			 237 			 44
57 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3892185600
Epoch [234/1000] took 96.8469831943512s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3537643215737982, train accuracy: 0.5048203330411919
Val mean loss: 1.6870724253538179, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2593 			 1998 			 1174
2211 			 2123 			 1102
2086 			 1966 			 1062
1563 			 1683 			 830
1346 			 1657 			 702
470 			 842 			 314
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 100
234 			 248 			 56
269 			 251 			 87
211 			 169 			 55
163 			 237 			 51
73 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3821797888
Epoch [235/1000] took 97.28111815452576s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3537541746721833, train accuracy: 0.5072548446781575
Val mean loss: 1.7038293350033644, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2582 			 1998 			 1175
2102 			 2123 			 1077
2098 			 1966 			 1072
1612 			 1683 			 849
1372 			 1657 			 706
503 			 842 			 330
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 100
334 			 248 			 81
232 			 251 			 74
190 			 169 			 52
137 			 237 			 47
61 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3846211072
Epoch [236/1000] took 96.97845482826233s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3547296160106719, train accuracy: 0.5072548446781575
Val mean loss: 1.7203273976721414, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2570 			 1998 			 1182
2265 			 2123 			 1119
2087 			 1966 			 1061
1518 			 1683 			 817
1336 			 1657 			 699
493 			 842 			 331
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 96
266 			 248 			 65
245 			 251 			 80
241 			 169 			 59
151 			 237 			 47
67 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846866432
Epoch [237/1000] took 97.02817606925964s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3524830189078025, train accuracy: 0.5076443665400721
Val mean loss: 1.6808416174679268, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2487 			 1998 			 1153
2190 			 2123 			 1098
2125 			 1966 			 1080
1568 			 1683 			 835
1420 			 1657 			 726
479 			 842 			 321
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
360 			 263 			 105
256 			 248 			 67
238 			 251 			 78
239 			 169 			 60
127 			 237 			 41
64 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3845391872
Epoch [238/1000] took 96.7406804561615s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3524837193087997, train accuracy: 0.5052098549031064
Val mean loss: 1.6809339087183883, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2684 			 1998 			 1215
2149 			 2123 			 1077
2101 			 1966 			 1065
1575 			 1683 			 825
1283 			 1657 			 686
477 			 842 			 320
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 86
284 			 248 			 69
240 			 251 			 81
219 			 169 			 59
183 			 237 			 55
75 			 116 			 32
Max memory allocated: 8857609728; Memory allocated: 3846997504
Epoch [239/1000] took 97.25946855545044s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.352489886254165, train accuracy: 0.5052098549031064
Val mean loss: 1.6885953792711583, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2483 			 1998 			 1137
2255 			 2123 			 1114
2090 			 1966 			 1058
1571 			 1683 			 843
1361 			 1657 			 705
509 			 842 			 331
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 95
225 			 248 			 60
271 			 251 			 86
214 			 169 			 57
172 			 237 			 50
68 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3855877632
Epoch [240/1000] took 97.44506573677063s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3512296238420909, train accuracy: 0.508423410263901
Val mean loss: 1.6958654653735277, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2507 			 1998 			 1147
2118 			 2123 			 1087
2179 			 1966 			 1091
1580 			 1683 			 838
1379 			 1657 			 726
506 			 842 			 332
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
371 			 263 			 109
281 			 248 			 68
228 			 251 			 77
198 			 169 			 55
141 			 237 			 45
65 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3846047232
Epoch [241/1000] took 97.96157836914062s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3525287533103492, train accuracy: 0.5064758009543285
Val mean loss: 1.7119590363851407, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2653 			 1998 			 1188
2137 			 2123 			 1081
2102 			 1966 			 1075
1559 			 1683 			 826
1344 			 1657 			 709
474 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 86
328 			 248 			 81
246 			 251 			 79
208 			 169 			 57
143 			 237 			 45
75 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3823927808
Epoch [242/1000] took 97.41030740737915s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3503410816192627, train accuracy: 0.50637842048885
Val mean loss: 1.6926785794700063, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2524 			 1998 			 1156
2193 			 2123 			 1111
2109 			 1966 			 1060
1589 			 1683 			 838
1356 			 1657 			 708
498 			 842 			 327
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
389 			 263 			 111
269 			 248 			 69
225 			 251 			 74
202 			 169 			 51
142 			 237 			 44
57 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3846047232
Epoch [243/1000] took 97.5106782913208s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3503490095940707, train accuracy: 0.5079365079365079
Val mean loss: 1.6812975842778275, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2601 			 1998 			 1194
2178 			 2123 			 1093
2105 			 1966 			 1071
1545 			 1683 			 825
1336 			 1657 			 699
504 			 842 			 334
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 95
288 			 248 			 67
227 			 251 			 77
223 			 169 			 57
175 			 237 			 51
64 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3846374912
Epoch [244/1000] took 97.8275420665741s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.350865083691487, train accuracy: 0.5047229525757133
Val mean loss: 1.7129096984863281, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2566 			 1998 			 1162
2162 			 2123 			 1086
2087 			 1966 			 1063
1588 			 1683 			 834
1364 			 1657 			 706
502 			 842 			 332
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 96
280 			 248 			 72
234 			 251 			 77
210 			 169 			 57
167 			 237 			 49
70 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3846931968
Epoch [245/1000] took 97.77210831642151s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.350733403476228, train accuracy: 0.5085207907293797
Val mean loss: 1.7019210297886918, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2567 			 1998 			 1178
2203 			 2123 			 1114
2120 			 1966 			 1073
1537 			 1683 			 827
1369 			 1657 			 712
473 			 842 			 318
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 91
247 			 248 			 64
257 			 251 			 85
237 			 169 			 60
175 			 237 			 51
68 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3811148288
Epoch [246/1000] took 97.45614337921143s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.351283466704538, train accuracy: 0.5075469860745935
Val mean loss: 1.7049455729926504, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2511 			 1998 			 1146
2182 			 2123 			 1107
2093 			 1966 			 1061
1611 			 1683 			 854
1376 			 1657 			 713
496 			 842 			 331
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 93
298 			 248 			 74
229 			 251 			 75
233 			 169 			 57
154 			 237 			 46
70 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3817538048
Epoch [247/1000] took 98.03317856788635s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3512296167860893, train accuracy: 0.5056967572304996
Val mean loss: 1.7095082794747702, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2555 			 1998 			 1155
2184 			 2123 			 1095
2135 			 1966 			 1074
1583 			 1683 			 847
1325 			 1657 			 697
487 			 842 			 325
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
271 			 248 			 70
192 			 251 			 67
254 			 169 			 69
182 			 237 			 52
67 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3846080000
Epoch [248/1000] took 98.19078922271729s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3498155751332315, train accuracy: 0.505307235368585
Val mean loss: 1.7002736446334095, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2548 			 1998 			 1174
2202 			 2123 			 1098
2070 			 1966 			 1042
1603 			 1683 			 839
1327 			 1657 			 699
519 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 95
237 			 248 			 56
248 			 251 			 78
201 			 169 			 50
223 			 237 			 63
58 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3846080000
Epoch [249/1000] took 97.79299306869507s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3509909510612488, train accuracy: 0.5082286493329438
Val mean loss: 1.6945764873085953, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2542 			 1998 			 1160
2126 			 2123 			 1089
2136 			 1966 			 1082
1512 			 1683 			 820
1472 			 1657 			 755
481 			 842 			 313
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
363 			 263 			 110
271 			 248 			 68
224 			 251 			 78
247 			 169 			 62
124 			 237 			 40
55 			 116 			 24
Max memory allocated: 8857609728; Memory allocated: 3851617792
Epoch [250/1000] took 97.5917637348175s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3496146592024332, train accuracy: 0.5064758009543285
Val mean loss: 1.7183539198666085, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2613 			 1998 			 1180
2172 			 2123 			 1085
2068 			 1966 			 1050
1611 			 1683 			 856
1328 			 1657 			 704
477 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 100
260 			 248 			 61
226 			 251 			 74
233 			 169 			 60
161 			 237 			 48
73 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3855877632
Epoch [251/1000] took 98.03120970726013s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3497415164549402, train accuracy: 0.5101762586425163
Val mean loss: 1.7040237391867288, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2582 			 1998 			 1174
2141 			 2123 			 1103
2126 			 1966 			 1074
1586 			 1683 			 851
1350 			 1657 			 709
484 			 842 			 328
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 102
280 			 248 			 70
217 			 251 			 70
215 			 169 			 56
154 			 237 			 47
68 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3823927808
Epoch [252/1000] took 97.95689344406128s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.348150717135159, train accuracy: 0.5077417470055506
Val mean loss: 1.710601649633268, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2588 			 1998 			 1178
2184 			 2123 			 1101
2116 			 1966 			 1070
1544 			 1683 			 829
1356 			 1657 			 712
481 			 842 			 324
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 93
239 			 248 			 56
241 			 251 			 81
245 			 169 			 60
176 			 237 			 52
76 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3846669824
Epoch [253/1000] took 97.8484194278717s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3488424890137907, train accuracy: 0.5108579219008667
Val mean loss: 1.7128089055782412, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2546 			 1998 			 1173
2135 			 2123 			 1098
2081 			 1966 			 1061
1608 			 1683 			 848
1371 			 1657 			 718
528 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
307 			 248 			 78
245 			 251 			 83
198 			 169 			 49
158 			 237 			 47
59 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3891595776
Epoch [254/1000] took 97.74486255645752s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3486676353531835, train accuracy: 0.5093972149186873
Val mean loss: 1.6953542610494101, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2503 			 1998 			 1162
2240 			 2123 			 1118
2082 			 1966 			 1067
1581 			 1683 			 847
1390 			 1657 			 713
473 			 842 			 324
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 95
235 			 248 			 57
300 			 251 			 92
197 			 169 			 51
170 			 237 			 50
74 			 116 			 32
Max memory allocated: 8857609728; Memory allocated: 3846768128
Epoch [255/1000] took 98.09940481185913s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3503058105242958, train accuracy: 0.5098841172460804
Val mean loss: 1.6966368512409489, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2574 			 1998 			 1178
2119 			 2123 			 1088
2177 			 1966 			 1087
1566 			 1683 			 843
1347 			 1657 			 715
486 			 842 			 325
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 85
308 			 248 			 74
212 			 251 			 74
213 			 169 			 56
191 			 237 			 54
77 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3849487872
Epoch [256/1000] took 97.63341474533081s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3478729736025088, train accuracy: 0.509494595384166
Val mean loss: 1.69641958213434, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2530 			 1998 			 1157
2182 			 2123 			 1101
2102 			 1966 			 1068
1556 			 1683 			 839
1389 			 1657 			 725
510 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 92
308 			 248 			 75
226 			 251 			 76
226 			 169 			 59
151 			 237 			 45
70 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3846899200
Epoch [257/1000] took 98.15853691101074s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3488883106879357, train accuracy: 0.5090076930567728
Val mean loss: 1.6912498328743912, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2535 			 1998 			 1170
2238 			 2123 			 1124
2072 			 1966 			 1054
1607 			 1683 			 846
1318 			 1657 			 701
499 			 842 			 332
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 101
269 			 248 			 69
234 			 251 			 77
220 			 169 			 54
175 			 237 			 53
60 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3891694080
Epoch [258/1000] took 97.86600732803345s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.346879300670089, train accuracy: 0.5127081507449606
Val mean loss: 1.6949751260803967, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2516 			 1998 			 1176
2146 			 2123 			 1097
2154 			 1966 			 1088
1553 			 1683 			 838
1407 			 1657 			 730
493 			 842 			 336
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 107
304 			 248 			 73
214 			 251 			 71
225 			 169 			 58
131 			 237 			 43
59 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3932885504
Epoch [259/1000] took 98.14791679382324s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3464230610945513, train accuracy: 0.5118317265556529
Val mean loss: 1.7005135635050332, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2580 			 1998 			 1177
2198 			 2123 			 1122
2073 			 1966 			 1067
1582 			 1683 			 844
1330 			 1657 			 709
506 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 102
287 			 248 			 71
235 			 251 			 75
211 			 169 			 52
149 			 237 			 47
61 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [260/1000] took 97.64913487434387s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3471668888103925, train accuracy: 0.5112474437627812
Val mean loss: 1.7065934756907022, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2537 			 1998 			 1169
2261 			 2123 			 1131
2047 			 1966 			 1055
1609 			 1683 			 858
1342 			 1657 			 712
473 			 842 			 325
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
348 			 263 			 101
252 			 248 			 64
246 			 251 			 81
221 			 169 			 56
148 			 237 			 45
69 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3849487872
Epoch [261/1000] took 97.40249752998352s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3460740014400066, train accuracy: 0.5105657805044308
Val mean loss: 1.6877583323455438, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2583 			 1998 			 1175
2083 			 2123 			 1079
2195 			 1966 			 1114
1567 			 1683 			 841
1337 			 1657 			 702
504 			 842 			 332
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 90
325 			 248 			 78
216 			 251 			 72
198 			 169 			 50
185 			 237 			 55
71 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846538752
Epoch [262/1000] took 97.85946822166443s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3454318306527777, train accuracy: 0.5112474437627812
Val mean loss: 1.6908510952461056, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2529 			 1998 			 1166
2251 			 2123 			 1126
2071 			 1966 			 1057
1549 			 1683 			 839
1354 			 1657 			 722
515 			 842 			 340
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 97
296 			 248 			 69
257 			 251 			 82
201 			 169 			 51
146 			 237 			 46
68 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3817538048
Epoch [263/1000] took 98.3934736251831s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3486019774018048, train accuracy: 0.5099814977115591
Val mean loss: 1.7042576452580893, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2501 			 1998 			 1160
2138 			 2123 			 1089
2148 			 1966 			 1082
1567 			 1683 			 841
1404 			 1657 			 731
511 			 842 			 334
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 102
338 			 248 			 81
198 			 251 			 68
210 			 169 			 58
130 			 237 			 43
62 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3845555712
Epoch [264/1000] took 97.94348454475403s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3465455670223059, train accuracy: 0.5075469860745935
Val mean loss: 1.7212851890703527, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2573 			 1998 			 1179
2267 			 2123 			 1124
2083 			 1966 			 1063
1555 			 1683 			 835
1312 			 1657 			 689
479 			 842 			 322
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 95
275 			 248 			 68
266 			 251 			 85
210 			 169 			 54
156 			 237 			 47
71 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846014464
Epoch [265/1000] took 98.07337284088135s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.344104336429608, train accuracy: 0.5119291070211316
Val mean loss: 1.6930183957262737, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2604 			 1998 			 1184
2187 			 2123 			 1113
2073 			 1966 			 1063
1573 			 1683 			 854
1321 			 1657 			 704
511 			 842 			 339
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 93
251 			 248 			 61
267 			 251 			 84
226 			 169 			 55
191 			 237 			 57
58 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3846047232
Epoch [266/1000] took 97.74906086921692s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3456840130770318, train accuracy: 0.5110526828318239
Val mean loss: 1.6990325799802455, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2558 			 1998 			 1160
2154 			 2123 			 1094
2146 			 1966 			 1095
1543 			 1683 			 844
1379 			 1657 			 725
489 			 842 			 330
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 101
291 			 248 			 71
225 			 251 			 77
231 			 169 			 61
142 			 237 			 44
58 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3846047232
Epoch [267/1000] took 97.59515309333801s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3432151651085351, train accuracy: 0.5110526828318239
Val mean loss: 1.7180042470373758, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2539 			 1998 			 1170
2238 			 2123 			 1115
2116 			 1966 			 1084
1581 			 1683 			 852
1316 			 1657 			 701
479 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
275 			 263 			 85
266 			 248 			 63
238 			 251 			 79
243 			 169 			 58
174 			 237 			 50
88 			 116 			 32
Max memory allocated: 8857609728; Memory allocated: 3846047232
Epoch [268/1000] took 97.83285593986511s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3444639604039652, train accuracy: 0.5067679423507644
Val mean loss: 1.6940280024598284, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2504 			 1998 			 1151
2154 			 2123 			 1086
2099 			 1966 			 1065
1580 			 1683 			 840
1399 			 1657 			 718
533 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 98
303 			 248 			 71
242 			 251 			 79
222 			 169 			 58
139 			 237 			 45
58 			 116 			 24
Max memory allocated: 8857609728; Memory allocated: 3845719552
Epoch [269/1000] took 97.82607436180115s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3425355865576556, train accuracy: 0.5124160093485247
Val mean loss: 1.6941751532438325, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2524 			 1998 			 1169
2146 			 2123 			 1100
2142 			 1966 			 1085
1571 			 1683 			 849
1409 			 1657 			 736
477 			 842 			 323
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 103
317 			 248 			 74
232 			 251 			 77
210 			 169 			 54
121 			 237 			 39
64 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3849487872
Epoch [270/1000] took 97.9052140712738s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.347058051843138, train accuracy: 0.5124160093485247
Val mean loss: 1.7034603328239628, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2522 			 1998 			 1173
2298 			 2123 			 1149
2076 			 1966 			 1068
1555 			 1683 			 835
1311 			 1657 			 696
507 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 100
265 			 248 			 68
262 			 251 			 82
202 			 169 			 50
168 			 237 			 50
62 			 116 			 28
Max memory allocated: 8857609728; Memory allocated: 3846374912
Epoch [271/1000] took 97.82526540756226s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3432086880704694, train accuracy: 0.5139740967961827
Val mean loss: 1.7200590023180333, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2570 			 1998 			 1180
2135 			 2123 			 1099
2109 			 1966 			 1079
1565 			 1683 			 853
1388 			 1657 			 729
502 			 842 			 338
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 89
329 			 248 			 78
232 			 251 			 72
207 			 169 			 55
160 			 237 			 49
66 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3821797888
Epoch [272/1000] took 97.89357542991638s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3432139886131167, train accuracy: 0.5142662381926185
Val mean loss: 1.7151876455400048, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2502 			 1998 			 1156
2163 			 2123 			 1111
2121 			 1966 			 1085
1592 			 1683 			 863
1377 			 1657 			 731
514 			 842 			 335
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
394 			 263 			 115
322 			 248 			 76
177 			 251 			 60
203 			 169 			 53
129 			 237 			 43
59 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3846538752
Epoch [273/1000] took 98.16743040084839s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3434231028749934, train accuracy: 0.511539585159217
Val mean loss: 1.7019329681629087, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2620 			 1998 			 1188
2249 			 2123 			 1131
2038 			 1966 			 1059
1568 			 1683 			 847
1322 			 1657 			 705
472 			 842 			 323
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 93
265 			 248 			 64
239 			 251 			 80
211 			 169 			 55
196 			 237 			 55
68 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846899200
Epoch [274/1000] took 98.0986602306366s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3439471439409107, train accuracy: 0.5127081507449606
Val mean loss: 1.7092104423336867, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2537 			 1998 			 1183
2228 			 2123 			 1128
2058 			 1966 			 1063
1590 			 1683 			 844
1379 			 1657 			 721
477 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 95
246 			 248 			 65
248 			 251 			 82
207 			 169 			 51
178 			 237 			 49
74 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846735360
Epoch [275/1000] took 97.69752383232117s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3417155092750384, train accuracy: 0.5129029116759178
Val mean loss: 1.698983980388176, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2543 			 1998 			 1180
2122 			 2123 			 1097
2154 			 1966 			 1095
1571 			 1683 			 839
1372 			 1657 			 715
507 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 98
273 			 248 			 66
237 			 251 			 78
214 			 169 			 56
165 			 237 			 49
76 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846735360
Epoch [276/1000] took 97.83726572990417s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3419244950434128, train accuracy: 0.5113448242282598
Val mean loss: 1.7144832204027873, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2555 			 1998 			 1179
2203 			 2123 			 1117
2114 			 1966 			 1074
1536 			 1683 			 835
1385 			 1657 			 720
476 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
357 			 263 			 107
256 			 248 			 64
218 			 251 			 74
224 			 169 			 59
156 			 237 			 48
73 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3851617792
Epoch [277/1000] took 97.92824578285217s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.341987934439353, train accuracy: 0.5145583795890545
Val mean loss: 1.700922207134526, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2543 			 1998 			 1174
2176 			 2123 			 1116
2066 			 1966 			 1071
1615 			 1683 			 863
1365 			 1657 			 716
504 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 108
233 			 248 			 55
269 			 251 			 84
195 			 169 			 49
174 			 237 			 53
60 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3817538048
Epoch [278/1000] took 97.56558847427368s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3406691146416827, train accuracy: 0.5108579219008667
Val mean loss: 1.684060762568218, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2519 			 1998 			 1157
2136 			 2123 			 1103
2111 			 1966 			 1070
1600 			 1683 			 856
1405 			 1657 			 726
498 			 842 			 334
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
406 			 263 			 120
259 			 248 			 64
248 			 251 			 80
175 			 169 			 47
135 			 237 			 42
61 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3891561984
Epoch [279/1000] took 97.71905493736267s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3419712726200852, train accuracy: 0.511539585159217
Val mean loss: 1.700383773664149, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2627 			 1998 			 1201
2128 			 2123 			 1078
2109 			 1966 			 1077
1539 			 1683 			 838
1360 			 1657 			 724
506 			 842 			 335
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
282 			 248 			 62
239 			 251 			 80
227 			 169 			 57
153 			 237 			 50
65 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3823927808
Epoch [280/1000] took 97.6019561290741s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3408323823477248, train accuracy: 0.5110526828318239
Val mean loss: 1.7007602104326573, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2502 			 1998 			 1169
2237 			 2123 			 1120
2104 			 1966 			 1070
1579 			 1683 			 844
1364 			 1657 			 719
483 			 842 			 326
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 97
238 			 248 			 57
230 			 251 			 76
222 			 169 			 59
171 			 237 			 51
82 			 116 			 31
Max memory allocated: 8857609728; Memory allocated: 3851617792
Epoch [281/1000] took 97.7691159248352s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3416444011195054, train accuracy: 0.5142662381926185
Val mean loss: 1.7212512871114218, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2551 			 1998 			 1174
2097 			 2123 			 1090
2104 			 1966 			 1081
1602 			 1683 			 859
1404 			 1657 			 735
511 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 102
282 			 248 			 70
257 			 251 			 82
196 			 169 			 52
137 			 237 			 43
73 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3845719552
Epoch [282/1000] took 98.07278251647949s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3420921879765402, train accuracy: 0.5117343460901743
Val mean loss: 1.7040214538574219, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2655 			 1998 			 1204
2130 			 2123 			 1098
2099 			 1966 			 1074
1567 			 1683 			 849
1337 			 1657 			 706
481 			 842 			 324
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 93
304 			 248 			 72
221 			 251 			 72
227 			 169 			 57
165 			 237 			 50
78 			 116 			 32
Max memory allocated: 8857609728; Memory allocated: 3823927808
Epoch [283/1000] took 97.67996668815613s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.33942512856837, train accuracy: 0.5158243256402766
Val mean loss: 1.7102535497851488, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2513 			 1998 			 1159
2199 			 2123 			 1126
2080 			 1966 			 1081
1599 			 1683 			 861
1381 			 1657 			 730
497 			 842 			 340
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 109
283 			 248 			 68
245 			 251 			 79
193 			 169 			 52
143 			 237 			 44
70 			 116 			 30
Max memory allocated: 8857609728; Memory allocated: 3846538752
Epoch [284/1000] took 97.84067416191101s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.340112060772667, train accuracy: 0.515434803778362
Val mean loss: 1.7050046455569383, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2596 			 1998 			 1197
2081 			 2123 			 1083
2184 			 1966 			 1116
1531 			 1683 			 832
1387 			 1657 			 728
490 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 97
298 			 248 			 71
218 			 251 			 69
232 			 169 			 59
138 			 237 			 43
71 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3845883392
Epoch [285/1000] took 97.87796092033386s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3399931225821236, train accuracy: 0.5127081507449606
Val mean loss: 1.712901737631821, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2543 			 1998 			 1166
2210 			 2123 			 1119
2087 			 1966 			 1076
1609 			 1683 			 868
1322 			 1657 			 704
498 			 842 			 332
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 96
272 			 248 			 67
250 			 251 			 82
206 			 169 			 52
165 			 237 			 50
84 			 116 			 31
Max memory allocated: 8857609728; Memory allocated: 3804758528
Epoch [286/1000] took 97.94467520713806s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.340977490876694, train accuracy: 0.5124160093485247
Val mean loss: 1.7091268126557513, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2544 			 1998 			 1179
2205 			 2123 			 1116
2081 			 1966 			 1069
1568 			 1683 			 848
1352 			 1657 			 713
519 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 93
309 			 248 			 76
244 			 251 			 80
220 			 169 			 55
154 			 237 			 49
63 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3813278208
Epoch [287/1000] took 97.88267278671265s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.339772631446149, train accuracy: 0.5149479014509689
Val mean loss: 1.7106355864827225, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2518 			 1998 			 1174
2221 			 2123 			 1121
2120 			 1966 			 1090
1584 			 1683 			 851
1319 			 1657 			 711
507 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 99
254 			 248 			 62
227 			 251 			 75
222 			 169 			 54
187 			 237 			 53
59 			 116 			 24
Max memory allocated: 8857609728; Memory allocated: 3891561984
Epoch [288/1000] took 97.96534872055054s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.338774387710191, train accuracy: 0.513389814003311
Val mean loss: 1.6914691314464663, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2509 			 1998 			 1161
2202 			 2123 			 1118
2080 			 1966 			 1063
1557 			 1683 			 847
1424 			 1657 			 746
497 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 100
237 			 248 			 59
271 			 251 			 87
233 			 169 			 56
149 			 237 			 47
70 			 116 			 29
Max memory allocated: 8857609728; Memory allocated: 3846374912
Epoch [289/1000] took 98.36230635643005s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3393067050945722, train accuracy: 0.513389814003311
Val mean loss: 1.7160483220728433, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2578 			 1998 			 1188
2107 			 2123 			 1087
2168 			 1966 			 1104
1583 			 1683 			 849
1350 			 1657 			 715
483 			 842 			 329
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
343 			 263 			 98
255 			 248 			 61
234 			 251 			 77
216 			 169 			 55
176 			 237 			 52
60 			 116 			 27
Max memory allocated: 8857609728; Memory allocated: 3819667968
Epoch [290/1000] took 97.51408743858337s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3372355016221138, train accuracy: 0.5147531405200116
Val mean loss: 1.728923021293268, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2529 			 1998 			 1173
2223 			 2123 			 1120
2072 			 1966 			 1071
1602 			 1683 			 872
1355 			 1657 			 714
488 			 842 			 336
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 93
234 			 248 			 56
249 			 251 			 79
216 			 169 			 55
194 			 237 			 54
66 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3847259648
Epoch [291/1000] took 98.16580080986023s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3370528302831441, train accuracy: 0.5140714772616614
Val mean loss: 1.680739824364825, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2580 			 1998 			 1182
2129 			 2123 			 1097
2067 			 1966 			 1063
1604 			 1683 			 864
1377 			 1657 			 732
512 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 94
292 			 248 			 69
250 			 251 			 82
183 			 169 			 51
193 			 237 			 56
54 			 116 			 26
Max memory allocated: 8857609728; Memory allocated: 3809018368
Epoch [292/1000] took 98.07664036750793s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.337926457047091, train accuracy: 0.5146557600545331
Val mean loss: 1.706503879733202, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2511 			 1998 			 1168
2224 			 2123 			 1119
2153 			 1966 			 1106
1522 			 1683 			 837
1369 			 1657 			 720
490 			 842 			 335
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
376 			 263 			 109
234 			 248 			 62
251 			 251 			 82
193 			 169 			 48
171 			 237 			 49
59 			 116 			 25
Max memory allocated: 8857609728; Memory allocated: 3853747712
Epoch [293/1000] took 97.24864912033081s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3363731483061365, train accuracy: 0.5167981302950628
Val mean loss: 1.7139903394187368, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2606 			 1998 			 1200
2074 			 2123 			 1078
2115 			 1966 			 1082
1614 			 1683 			 878
1381 			 1657 			 738
479 			 842 			 331
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
359 			 263 			 101
282 			 248 			 67
226 			 251 			 72
177 			 169 			 45
177 			 237 			 50
63 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [294/1000] took 97.87182140350342s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.338215542730884, train accuracy: 0.5111500632973026
Val mean loss: 1.6967556767347383, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2544 			 1998 			 1173
2292 			 2123 			 1134
2027 			 1966 			 1052
1514 			 1683 			 828
1394 			 1657 			 729
498 			 842 			 333
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 104
224 			 248 			 56
282 			 251 			 87
233 			 169 			 60
146 			 237 			 45
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892546048
Epoch [295/1000] took 97.97583222389221s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3379538705415814, train accuracy: 0.5146557600545331
Val mean loss: 1.7019849608584148, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2519 			 1998 			 1166
2107 			 2123 			 1094
2166 			 1966 			 1101
1631 			 1683 			 869
1350 			 1657 			 717
496 			 842 			 338
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 98
266 			 248 			 67
259 			 251 			 84
207 			 169 			 55
157 			 237 			 48
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892316672
Epoch [296/1000] took 97.44305229187012s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3363008142631745, train accuracy: 0.5144609991235758
Val mean loss: 1.7180925927511075, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2596 			 1998 			 1191
2149 			 2123 			 1101
2117 			 1966 			 1083
1549 			 1683 			 854
1359 			 1657 			 719
499 			 842 			 335
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 95
288 			 248 			 73
221 			 251 			 73
223 			 169 			 58
176 			 237 			 52
64 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [297/1000] took 97.82359576225281s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.335229064445258, train accuracy: 0.5135845749342682
Val mean loss: 1.7228332321818283, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2514 			 1998 			 1171
2219 			 2123 			 1113
2075 			 1966 			 1063
1574 			 1683 			 854
1371 			 1657 			 726
516 			 842 			 347
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 99
279 			 248 			 69
231 			 251 			 80
223 			 169 			 57
158 			 237 			 48
65 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3802628608
Epoch [298/1000] took 98.00694942474365s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3352339813642413, train accuracy: 0.5157269451747979
Val mean loss: 1.6841019682767915, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2542 			 1998 			 1181
2138 			 2123 			 1103
2117 			 1966 			 1085
1572 			 1683 			 859
1406 			 1657 			 736
494 			 842 			 332
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 98
271 			 248 			 62
255 			 251 			 82
223 			 169 			 55
161 			 237 			 51
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3893365248
Epoch [299/1000] took 97.91571164131165s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3346417247320632, train accuracy: 0.5150452819164476
Val mean loss: 1.6966976043654651, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2603 			 1998 			 1192
2191 			 2123 			 1113
2067 			 1966 			 1076
1589 			 1683 			 862
1318 			 1657 			 709
501 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
268 			 263 			 87
285 			 248 			 68
243 			 251 			 79
229 			 169 			 54
183 			 237 			 55
76 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [300/1000] took 97.6400191783905s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3350253123731999, train accuracy: 0.5159217061057552
Val mean loss: 1.683978135992841, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2412 			 1998 			 1140
2229 			 2123 			 1123
2089 			 1966 			 1079
1596 			 1683 			 865
1425 			 1657 			 749
518 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
360 			 263 			 109
240 			 248 			 56
286 			 251 			 91
195 			 169 			 49
141 			 237 			 44
62 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [301/1000] took 97.93630266189575s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3368261606893808, train accuracy: 0.5155321842438407
Val mean loss: 1.6974194776721117, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2582 			 1998 			 1190
2116 			 2123 			 1097
2133 			 1966 			 1094
1563 			 1683 			 854
1382 			 1657 			 730
493 			 842 			 329
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 93
279 			 248 			 67
245 			 251 			 81
225 			 169 			 56
152 			 237 			 47
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [302/1000] took 97.510094165802s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3337723544082167, train accuracy: 0.5168955107605414
Val mean loss: 1.7018576569673491, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2520 			 1998 			 1166
2186 			 2123 			 1115
2051 			 1966 			 1069
1621 			 1683 			 877
1372 			 1657 			 733
519 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 92
235 			 248 			 59
296 			 251 			 93
209 			 169 			 53
157 			 237 			 46
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892087296
Epoch [303/1000] took 98.05363845825195s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.334889875022793, train accuracy: 0.5188431200701139
Val mean loss: 1.706568898224249, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2504 			 1998 			 1173
2196 			 2123 			 1140
2174 			 1966 			 1104
1566 			 1683 			 857
1311 			 1657 			 712
518 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
332 			 263 			 96
245 			 248 			 58
235 			 251 			 80
226 			 169 			 58
185 			 237 			 51
61 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3892610560
Epoch [304/1000] took 98.0762631893158s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.334978470178408, train accuracy: 0.5169928912260201
Val mean loss: 1.7023935230766856, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2555 			 1998 			 1189
2133 			 2123 			 1100
2106 			 1966 			 1080
1598 			 1683 			 867
1393 			 1657 			 742
484 			 842 			 331
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
352 			 263 			 102
279 			 248 			 66
245 			 251 			 81
191 			 169 			 49
154 			 237 			 45
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [305/1000] took 97.38027906417847s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3338703459297014, train accuracy: 0.5179666958808063
Val mean loss: 1.7105678988666069, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2512 			 1998 			 1168
2176 			 2123 			 1128
2136 			 1966 			 1095
1555 			 1683 			 856
1371 			 1657 			 724
519 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
359 			 263 			 107
263 			 248 			 65
230 			 251 			 73
187 			 169 			 50
186 			 237 			 51
59 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [306/1000] took 97.89335346221924s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3348847408532354, train accuracy: 0.5189405005355926
Val mean loss: 1.7188140851695364, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2551 			 1998 			 1186
2140 			 2123 			 1112
2097 			 1966 			 1090
1591 			 1683 			 858
1403 			 1657 			 746
487 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
355 			 263 			 102
280 			 248 			 69
231 			 251 			 74
214 			 169 			 56
141 			 237 			 43
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845359104
Epoch [307/1000] took 98.0093126296997s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3334749632535323, train accuracy: 0.5174797935534132
Val mean loss: 1.7019014707425746, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2607 			 1998 			 1201
2159 			 2123 			 1120
2092 			 1966 			 1076
1592 			 1683 			 856
1317 			 1657 			 716
502 			 842 			 345
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 87
286 			 248 			 66
255 			 251 			 80
203 			 169 			 53
189 			 237 			 54
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846800896
Epoch [308/1000] took 98.01216149330139s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3343023236295515, train accuracy: 0.5168955107605414
Val mean loss: 1.7105522359289773, val accuracy: 0.29906542056074764

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2479 			 1998 			 1159
2190 			 2123 			 1130
2090 			 1966 			 1074
1569 			 1683 			 856
1431 			 1657 			 748
510 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 101
248 			 248 			 66
231 			 251 			 76
259 			 169 			 68
143 			 237 			 46
64 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [309/1000] took 97.43642854690552s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3319263465679323, train accuracy: 0.5156295647093193
Val mean loss: 1.7167286960090078, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2577 			 1998 			 1196
2144 			 2123 			 1104
2091 			 1966 			 1077
1601 			 1683 			 866
1355 			 1657 			 713
501 			 842 			 339
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 98
273 			 248 			 65
228 			 251 			 79
229 			 169 			 57
162 			 237 			 49
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [310/1000] took 97.78265953063965s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3323862946293437, train accuracy: 0.5168955107605414
Val mean loss: 1.6885913436005755, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2531 			 1998 			 1186
2189 			 2123 			 1123
2075 			 1966 			 1072
1613 			 1683 			 863
1375 			 1657 			 726
486 			 842 			 338
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 93
279 			 248 			 62
240 			 251 			 77
251 			 169 			 59
147 			 237 			 48
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891726848
Epoch [311/1000] took 98.2238998413086s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3338184705775846, train accuracy: 0.5203038270522933
Val mean loss: 1.703166374346105, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2524 			 1998 			 1180
2242 			 2123 			 1136
2078 			 1966 			 1099
1585 			 1683 			 864
1326 			 1657 			 720
514 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 92
220 			 248 			 54
301 			 251 			 93
207 			 169 			 51
180 			 237 			 54
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [312/1000] took 97.81828927993774s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3307275786949466, train accuracy: 0.5169928912260201
Val mean loss: 1.727348144461469, val accuracy: 0.29595015576323985

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2578 			 1998 			 1194
2098 			 2123 			 1095
2130 			 1966 			 1104
1598 			 1683 			 854
1352 			 1657 			 717
513 			 842 			 345
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 94
320 			 248 			 78
261 			 251 			 82
167 			 169 			 47
164 			 237 			 51
62 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [313/1000] took 98.05420255661011s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3317823755406888, train accuracy: 0.5197195442594216
Val mean loss: 1.7053741769092838, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2511 			 1998 			 1175
2173 			 2123 			 1129
2140 			 1966 			 1094
1549 			 1683 			 851
1400 			 1657 			 743
496 			 842 			 345
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 93
335 			 248 			 81
200 			 251 			 67
219 			 169 			 56
155 			 237 			 45
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [314/1000] took 97.76533651351929s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3285147001810163, train accuracy: 0.5187457396046353
Val mean loss: 1.7258006625059175, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2569 			 1998 			 1189
2170 			 2123 			 1122
2153 			 1966 			 1109
1547 			 1683 			 856
1314 			 1657 			 706
516 			 842 			 345
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 91
266 			 248 			 67
202 			 251 			 69
243 			 169 			 59
206 			 237 			 55
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [315/1000] took 97.9921464920044s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3332941981490898, train accuracy: 0.5183562177427208
Val mean loss: 1.7143181649650014, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2536 			 1998 			 1182
2195 			 2123 			 1130
2067 			 1966 			 1074
1565 			 1683 			 860
1386 			 1657 			 733
520 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 92
280 			 248 			 65
256 			 251 			 83
220 			 169 			 54
169 			 237 			 52
67 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [316/1000] took 97.74300813674927s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3303679729176459, train accuracy: 0.519330022397507
Val mean loss: 1.6985770202264554, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2526 			 1998 			 1183
2171 			 2123 			 1119
2069 			 1966 			 1078
1587 			 1683 			 866
1397 			 1657 			 739
519 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 95
250 			 248 			 59
260 			 251 			 80
227 			 169 			 56
157 			 237 			 46
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [317/1000] took 97.24916863441467s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3294899485935674, train accuracy: 0.5182588372772422
Val mean loss: 1.6962346420055483, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2463 			 1998 			 1162
2189 			 2123 			 1135
2096 			 1966 			 1081
1626 			 1683 			 877
1388 			 1657 			 728
507 			 842 			 339
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
371 			 263 			 109
255 			 248 			 64
245 			 251 			 81
200 			 169 			 50
148 			 237 			 44
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [318/1000] took 97.69004845619202s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3287792937406497, train accuracy: 0.5163112279676697
Val mean loss: 1.7024878321624384, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2546 			 1998 			 1190
2153 			 2123 			 1106
2143 			 1966 			 1087
1551 			 1683 			 844
1379 			 1657 			 731
497 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 98
276 			 248 			 70
215 			 251 			 73
239 			 169 			 58
153 			 237 			 47
72 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [319/1000] took 97.87138414382935s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.330468274351221, train accuracy: 0.5163112279676697
Val mean loss: 1.706972886876362, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2518 			 1998 			 1176
2241 			 2123 			 1127
2122 			 1966 			 1102
1562 			 1683 			 855
1318 			 1657 			 703
508 			 842 			 339
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 102
250 			 248 			 62
202 			 251 			 68
238 			 169 			 61
196 			 237 			 55
62 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3892120064
Epoch [320/1000] took 97.94560647010803s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3287795659537627, train accuracy: 0.5200116856558574
Val mean loss: 1.696141786691619, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2493 			 1998 			 1172
2087 			 2123 			 1099
2132 			 1966 			 1096
1615 			 1683 			 874
1438 			 1657 			 758
504 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 102
327 			 248 			 78
224 			 251 			 74
200 			 169 			 51
147 			 237 			 48
57 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [321/1000] took 97.93469476699829s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.330172950976363, train accuracy: 0.5168955107605414
Val mean loss: 1.6945285419138467, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2484 			 1998 			 1162
2186 			 2123 			 1122
2137 			 1966 			 1098
1526 			 1683 			 843
1436 			 1657 			 746
500 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 101
297 			 248 			 73
230 			 251 			 74
217 			 169 			 55
134 			 237 			 41
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [322/1000] took 98.06586718559265s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3285120562229573, train accuracy: 0.5215697731035155
Val mean loss: 1.713166917242655, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2607 			 1998 			 1211
2157 			 2123 			 1122
2029 			 1966 			 1060
1631 			 1683 			 888
1327 			 1657 			 726
518 			 842 			 349
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
267 			 263 			 82
301 			 248 			 69
289 			 251 			 87
182 			 169 			 46
171 			 237 			 53
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [323/1000] took 97.69062328338623s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3269915974400126, train accuracy: 0.5196221637939429
Val mean loss: 1.700618275781957, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2485 			 1998 			 1170
2162 			 2123 			 1120
2126 			 1966 			 1099
1553 			 1683 			 849
1437 			 1657 			 754
506 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 106
336 			 248 			 85
246 			 251 			 80
172 			 169 			 46
120 			 237 			 39
59 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [324/1000] took 97.8944764137268s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3294084722007917, train accuracy: 0.5195247833284643
Val mean loss: 1.7336575228993485, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2564 			 1998 			 1202
2266 			 2123 			 1144
2151 			 1966 			 1104
1480 			 1683 			 827
1301 			 1657 			 707
507 			 842 			 351
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 93
253 			 248 			 60
249 			 251 			 78
248 			 169 			 62
147 			 237 			 46
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [325/1000] took 97.63056635856628s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.326382600258444, train accuracy: 0.5209854903106437
Val mean loss: 1.6972107974494375, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2493 			 1998 			 1180
2115 			 2123 			 1100
2118 			 1966 			 1085
1637 			 1683 			 885
1396 			 1657 			 753
510 			 842 			 347
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 101
310 			 248 			 67
201 			 251 			 71
225 			 169 			 58
139 			 237 			 41
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [326/1000] took 97.94148516654968s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3280398945942102, train accuracy: 0.5208881098451651
Val mean loss: 1.6942002511605985, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2516 			 1998 			 1176
2226 			 2123 			 1144
2076 			 1966 			 1088
1578 			 1683 			 861
1370 			 1657 			 735
503 			 842 			 345
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 104
239 			 248 			 56
288 			 251 			 88
210 			 169 			 55
137 			 237 			 42
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [327/1000] took 97.69210577011108s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.32768505013249, train accuracy: 0.5215697731035155
Val mean loss: 1.7025441919885032, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2568 			 1998 			 1202
2178 			 2123 			 1126
2083 			 1966 			 1084
1618 			 1683 			 873
1331 			 1657 			 724
491 			 842 			 347
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 93
269 			 248 			 65
252 			 251 			 79
203 			 169 			 52
177 			 237 			 52
75 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [328/1000] took 97.50171828269958s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3265953505893362, train accuracy: 0.5218619144999513
Val mean loss: 1.7096302974514845, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2508 			 1998 			 1186
2128 			 2123 			 1109
2108 			 1966 			 1093
1599 			 1683 			 871
1402 			 1657 			 748
524 			 842 			 352
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 100
293 			 248 			 73
225 			 251 			 70
214 			 169 			 55
152 			 237 			 48
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891792384
Epoch [329/1000] took 97.96753287315369s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3263563630736876, train accuracy: 0.5180640763462849
Val mean loss: 1.701126307975955, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2547 			 1998 			 1192
2186 			 2123 			 1116
2093 			 1966 			 1076
1590 			 1683 			 859
1367 			 1657 			 741
486 			 842 			 336
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 99
281 			 248 			 68
224 			 251 			 77
207 			 169 			 55
158 			 237 			 46
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [330/1000] took 97.74235486984253s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3267222509205898, train accuracy: 0.5194274028629857
Val mean loss: 1.7116720269366008, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2515 			 1998 			 1179
2186 			 2123 			 1131
2100 			 1966 			 1076
1579 			 1683 			 862
1362 			 1657 			 734
527 			 842 			 352
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 98
283 			 248 			 65
242 			 251 			 76
217 			 169 			 56
152 			 237 			 48
64 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [331/1000] took 97.89444780349731s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.326014855940394, train accuracy: 0.5223488168273445
Val mean loss: 1.7143152952194214, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2490 			 1998 			 1177
2209 			 2123 			 1141
2080 			 1966 			 1087
1611 			 1683 			 871
1357 			 1657 			 735
522 			 842 			 353
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 103
223 			 248 			 57
291 			 251 			 90
204 			 169 			 52
165 			 237 			 51
59 			 116 			 23
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [332/1000] took 97.92015671730042s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3274786130661533, train accuracy: 0.5182588372772422
Val mean loss: 1.7065217582190908, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2577 			 1998 			 1193
2130 			 2123 			 1107
2092 			 1966 			 1079
1600 			 1683 			 873
1391 			 1657 			 732
479 			 842 			 338
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 98
234 			 248 			 58
276 			 251 			 89
217 			 169 			 53
151 			 237 			 44
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [333/1000] took 97.80583596229553s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3247981101181647, train accuracy: 0.5190378810010712
Val mean loss: 1.6978159997521378, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2504 			 1998 			 1176
2162 			 2123 			 1116
2161 			 1966 			 1099
1564 			 1683 			 857
1353 			 1657 			 725
525 			 842 			 357
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 96
262 			 248 			 60
240 			 251 			 81
224 			 169 			 56
170 			 237 			 50
65 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846571520
Epoch [334/1000] took 97.74223780632019s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3251896600856958, train accuracy: 0.5209854903106437
Val mean loss: 1.7117076908669822, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2527 			 1998 			 1182
2222 			 2123 			 1143
2053 			 1966 			 1077
1611 			 1683 			 872
1356 			 1657 			 734
500 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 96
257 			 248 			 60
296 			 251 			 94
186 			 169 			 46
172 			 237 			 49
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [335/1000] took 97.72540020942688s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3253302095092345, train accuracy: 0.518550978673678
Val mean loss: 1.731742105832914, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2617 			 1998 			 1202
2088 			 2123 			 1097
2150 			 1966 			 1106
1525 			 1683 			 837
1406 			 1657 			 745
483 			 842 			 338
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 93
319 			 248 			 71
238 			 251 			 75
215 			 169 			 55
138 			 237 			 44
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892382208
Epoch [336/1000] took 98.15773415565491s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3258706505796247, train accuracy: 0.5186483591391566
Val mean loss: 1.6913156015116995, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2488 			 1998 			 1171
2177 			 2123 			 1122
2130 			 1966 			 1092
1589 			 1683 			 863
1368 			 1657 			 732
517 			 842 			 346
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 95
288 			 248 			 67
238 			 251 			 77
210 			 169 			 54
163 			 237 			 49
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [337/1000] took 98.2574896812439s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3256994810802543, train accuracy: 0.5218619144999513
Val mean loss: 1.7073720984342622, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2471 			 1998 			 1174
2230 			 2123 			 1147
2083 			 1966 			 1078
1575 			 1683 			 873
1388 			 1657 			 734
522 			 842 			 353
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 93
241 			 248 			 57
273 			 251 			 86
215 			 169 			 54
167 			 237 			 47
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [338/1000] took 98.26989889144897s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3268151558076853, train accuracy: 0.5227383386892589
Val mean loss: 1.713058006472704, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2581 			 1998 			 1197
2202 			 2123 			 1151
2118 			 1966 			 1100
1518 			 1683 			 848
1357 			 1657 			 727
493 			 842 			 345
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 97
220 			 248 			 54
255 			 251 			 80
240 			 169 			 59
176 			 237 			 50
81 			 116 			 34
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [339/1000] took 98.0258002281189s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3256123827253918, train accuracy: 0.5215697731035155
Val mean loss: 1.7329252376789, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2471 			 1998 			 1171
2115 			 2123 			 1114
2142 			 1966 			 1107
1614 			 1683 			 878
1418 			 1657 			 740
509 			 842 			 346
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 94
294 			 248 			 71
220 			 251 			 71
228 			 169 			 58
148 			 237 			 46
89 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [340/1000] took 97.60454368591309s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3235633833757443, train accuracy: 0.5222514363618658
Val mean loss: 1.712187772843896, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2507 			 1998 			 1177
2189 			 2123 			 1127
2106 			 1966 			 1108
1576 			 1683 			 863
1366 			 1657 			 734
525 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 96
271 			 248 			 62
205 			 251 			 66
237 			 169 			 60
165 			 237 			 49
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [341/1000] took 98.16582489013672s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3246248503711735, train accuracy: 0.5226409582237803
Val mean loss: 1.7094351285841407, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2528 			 1998 			 1187
2148 			 2123 			 1114
2077 			 1966 			 1081
1570 			 1683 			 872
1432 			 1657 			 764
514 			 842 			 349
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 97
282 			 248 			 70
242 			 251 			 79
229 			 169 			 60
138 			 237 			 43
70 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3891956224
Epoch [342/1000] took 97.76415634155273s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3227708503093303, train accuracy: 0.5220566754309086
Val mean loss: 1.7232675290689237, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2514 			 1998 			 1183
2220 			 2123 			 1135
2076 			 1966 			 1092
1611 			 1683 			 872
1315 			 1657 			 721
533 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 90
298 			 248 			 74
211 			 251 			 70
211 			 169 			 58
196 			 237 			 56
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [343/1000] took 97.49736618995667s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3221486899711632, train accuracy: 0.5217645340344726
Val mean loss: 1.6967725288577196, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2516 			 1998 			 1184
2135 			 2123 			 1109
2167 			 1966 			 1124
1556 			 1683 			 861
1383 			 1657 			 734
512 			 842 			 346
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 103
280 			 248 			 63
199 			 251 			 66
231 			 169 			 57
170 			 237 			 49
57 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [344/1000] took 97.51018595695496s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3220806222095667, train accuracy: 0.5220566754309086
Val mean loss: 1.6979606587712357, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2558 			 1998 			 1194
2176 			 2123 			 1127
2001 			 1966 			 1063
1631 			 1683 			 879
1410 			 1657 			 754
493 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
343 			 263 			 102
259 			 248 			 62
253 			 251 			 82
212 			 169 			 54
143 			 237 			 45
74 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [345/1000] took 97.56608557701111s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3215263242662139, train accuracy: 0.5236147628785666
Val mean loss: 1.7237461805343628, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2548 			 1998 			 1185
2166 			 2123 			 1137
2169 			 1966 			 1121
1556 			 1683 			 859
1318 			 1657 			 727
512 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 89
323 			 248 			 70
190 			 251 			 63
236 			 169 			 62
181 			 237 			 55
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [346/1000] took 98.15471172332764s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.324948779519102, train accuracy: 0.5241990456714383
Val mean loss: 1.7161861861624368, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2541 			 1998 			 1199
2166 			 2123 			 1120
2047 			 1966 			 1086
1601 			 1683 			 888
1409 			 1657 			 753
505 			 842 			 337
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 95
310 			 248 			 72
264 			 251 			 81
197 			 169 			 53
131 			 237 			 42
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [347/1000] took 97.41653203964233s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3211392639582031, train accuracy: 0.522446197292823
Val mean loss: 1.695359544056218, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2532 			 1998 			 1182
2208 			 2123 			 1137
2133 			 1966 			 1097
1560 			 1683 			 871
1322 			 1657 			 727
514 			 842 			 351
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 96
282 			 248 			 69
246 			 251 			 79
224 			 169 			 56
156 			 237 			 50
67 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3926691328
Epoch [348/1000] took 97.7303159236908s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3222862998273142, train accuracy: 0.5243938066023955
Val mean loss: 1.7372099946184856, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2523 			 1998 			 1176
2199 			 2123 			 1140
2070 			 1966 			 1098
1595 			 1683 			 875
1381 			 1657 			 749
501 			 842 			 347
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 89
267 			 248 			 62
241 			 251 			 81
245 			 169 			 58
165 			 237 			 48
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [349/1000] took 97.66001749038696s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3222211005160371, train accuracy: 0.5215697731035155
Val mean loss: 1.6990255350019874, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2521 			 1998 			 1190
2117 			 2123 			 1111
2078 			 1966 			 1085
1622 			 1683 			 881
1402 			 1657 			 734
529 			 842 			 355
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 100
311 			 248 			 72
243 			 251 			 74
199 			 169 			 51
147 			 237 			 47
62 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [350/1000] took 97.96739387512207s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.321434107524955, train accuracy: 0.5233226214821307
Val mean loss: 1.7103293174650611, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2524 			 1998 			 1196
2166 			 2123 			 1120
2099 			 1966 			 1088
1591 			 1683 			 879
1392 			 1657 			 749
497 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 95
294 			 248 			 68
247 			 251 			 74
218 			 169 			 56
132 			 237 			 42
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [351/1000] took 97.618816614151s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.320787857626086, train accuracy: 0.5237121433440451
Val mean loss: 1.7161616057884403, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2565 			 1998 			 1190
2112 			 2123 			 1115
2151 			 1966 			 1108
1557 			 1683 			 873
1360 			 1657 			 738
524 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 93
322 			 248 			 71
214 			 251 			 68
228 			 169 			 60
155 			 237 			 49
62 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3891628544
Epoch [352/1000] took 98.01206064224243s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3231695938704542, train accuracy: 0.5237121433440451
Val mean loss: 1.7122960759372245, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2476 			 1998 			 1178
2250 			 2123 			 1147
2072 			 1966 			 1091
1614 			 1683 			 880
1363 			 1657 			 741
494 			 842 			 341
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 99
264 			 248 			 63
246 			 251 			 78
207 			 169 			 54
162 			 237 			 51
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [353/1000] took 97.34209728240967s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3201851744518103, train accuracy: 0.5221540558963872
Val mean loss: 1.718074249058235, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2515 			 1998 			 1184
2176 			 2123 			 1121
2098 			 1966 			 1091
1585 			 1683 			 873
1371 			 1657 			 735
524 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
343 			 263 			 101
255 			 248 			 63
249 			 251 			 80
193 			 169 			 51
179 			 237 			 51
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [354/1000] took 97.95221447944641s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3214340618466291, train accuracy: 0.5266335573084039
Val mean loss: 1.7178643156842488, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2546 			 1998 			 1190
2170 			 2123 			 1136
2120 			 1966 			 1114
1527 			 1683 			 868
1386 			 1657 			 746
520 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
271 			 263 			 87
258 			 248 			 59
253 			 251 			 80
251 			 169 			 62
185 			 237 			 54
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [355/1000] took 97.93974614143372s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3218585661267195, train accuracy: 0.524296426136917
Val mean loss: 1.7007685172848586, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2462 			 1998 			 1177
2157 			 2123 			 1132
2130 			 1966 			 1099
1602 			 1683 			 883
1425 			 1657 			 749
493 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
279 			 248 			 62
243 			 251 			 78
221 			 169 			 56
155 			 237 			 47
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [356/1000] took 97.45557022094727s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.319454313067261, train accuracy: 0.522446197292823
Val mean loss: 1.7112194910282041, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2554 			 1998 			 1196
2170 			 2123 			 1136
2086 			 1966 			 1089
1579 			 1683 			 862
1358 			 1657 			 733
522 			 842 			 349
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
274 			 263 			 82
308 			 248 			 70
237 			 251 			 76
233 			 169 			 58
169 			 237 			 50
63 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [357/1000] took 97.54010009765625s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3197382588252844, train accuracy: 0.5266335573084039
Val mean loss: 1.70045382511325, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2497 			 1998 			 1188
2186 			 2123 			 1146
2131 			 1966 			 1101
1565 			 1683 			 868
1379 			 1657 			 750
511 			 842 			 355
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 110
275 			 248 			 65
228 			 251 			 73
213 			 169 			 54
138 			 237 			 44
64 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3928297984
Epoch [358/1000] took 96.53642320632935s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3175176898267038, train accuracy: 0.5200116856558574
Val mean loss: 1.7044243376429489, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2536 			 1998 			 1179
2217 			 2123 			 1134
2083 			 1966 			 1081
1581 			 1683 			 868
1333 			 1657 			 720
519 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 89
248 			 248 			 59
249 			 251 			 81
217 			 169 			 54
204 			 237 			 59
69 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3802628608
Epoch [359/1000] took 96.6458625793457s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3179292288896078, train accuracy: 0.5272178401012757
Val mean loss: 1.7258845654929555, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2479 			 1998 			 1181
2108 			 2123 			 1111
2119 			 1966 			 1110
1605 			 1683 			 896
1454 			 1657 			 768
504 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
358 			 263 			 108
316 			 248 			 75
200 			 251 			 65
202 			 169 			 53
141 			 237 			 43
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [360/1000] took 96.28022384643555s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3173945520154413, train accuracy: 0.5215697731035155
Val mean loss: 1.708373479726838, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2528 			 1998 			 1190
2234 			 2123 			 1134
2085 			 1966 			 1088
1578 			 1683 			 875
1337 			 1657 			 725
507 			 842 			 344
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 101
251 			 248 			 63
227 			 251 			 76
221 			 169 			 56
177 			 237 			 52
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [361/1000] took 97.33705997467041s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.321152548366618, train accuracy: 0.5244911870678742
Val mean loss: 1.7260670516549088, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2533 			 1998 			 1192
2100 			 2123 			 1114
2126 			 1966 			 1107
1578 			 1683 			 873
1404 			 1657 			 746
528 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 90
315 			 248 			 76
221 			 251 			 72
223 			 169 			 58
149 			 237 			 47
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [362/1000] took 96.63449215888977s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3179628574216848, train accuracy: 0.5216671535689941
Val mean loss: 1.6908368177530242, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2510 			 1998 			 1180
2231 			 2123 			 1139
2071 			 1966 			 1081
1586 			 1683 			 867
1358 			 1657 			 740
513 			 842 			 350
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 104
254 			 248 			 63
266 			 251 			 84
203 			 169 			 52
161 			 237 			 50
59 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [363/1000] took 96.7501163482666s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3166995965803152, train accuracy: 0.5219592949654299
Val mean loss: 1.7038351181076794, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2608 			 1998 			 1199
2079 			 2123 			 1104
2090 			 1966 			 1092
1612 			 1683 			 887
1381 			 1657 			 738
499 			 842 			 340
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 90
340 			 248 			 75
225 			 251 			 71
202 			 169 			 51
157 			 237 			 49
60 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3796238848
Epoch [364/1000] took 96.78564143180847s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.318588898375027, train accuracy: 0.5249780893952674
Val mean loss: 1.704413544840929, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2463 			 1998 			 1174
2231 			 2123 			 1148
2129 			 1966 			 1109
1539 			 1683 			 859
1377 			 1657 			 742
530 			 842 			 359
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 98
267 			 248 			 64
238 			 251 			 74
217 			 169 			 55
187 			 237 			 53
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [365/1000] took 96.41534662246704s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3175731696815134, train accuracy: 0.5257571331190963
Val mean loss: 1.716646249701337, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2541 			 1998 			 1204
2135 			 2123 			 1120
2142 			 1966 			 1112
1562 			 1683 			 863
1399 			 1657 			 757
490 			 842 			 343
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
278 			 263 			 85
273 			 248 			 64
251 			 251 			 76
229 			 169 			 56
176 			 237 			 55
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891857920
Epoch [366/1000] took 96.89941430091858s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3175410986689393, train accuracy: 0.5251728503262245
Val mean loss: 1.7171315216436618, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2521 			 1998 			 1186
2138 			 2123 			 1124
2094 			 1966 			 1101
1633 			 1683 			 884
1371 			 1657 			 741
512 			 842 			 357
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 99
263 			 248 			 58
230 			 251 			 75
212 			 169 			 56
183 			 237 			 55
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3849881088
Epoch [367/1000] took 96.98005652427673s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.31835245491931, train accuracy: 0.5261466549810108
Val mean loss: 1.7159811897975643, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2507 			 1998 			 1195
2115 			 2123 			 1119
2122 			 1966 			 1110
1619 			 1683 			 886
1375 			 1657 			 732
531 			 842 			 361
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
348 			 263 			 99
377 			 248 			 87
175 			 251 			 61
171 			 169 			 48
153 			 237 			 47
60 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [368/1000] took 97.11305022239685s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3162343932832141, train accuracy: 0.5254649917226605
Val mean loss: 1.7031473531955625, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2490 			 1998 			 1182
2294 			 2123 			 1166
2054 			 1966 			 1084
1549 			 1683 			 864
1365 			 1657 			 741
517 			 842 			 359
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 96
276 			 248 			 64
250 			 251 			 78
201 			 169 			 53
161 			 237 			 49
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [369/1000] took 97.20249629020691s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.31746336975573, train accuracy: 0.5244911870678742
Val mean loss: 1.717946750361745, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2536 			 1998 			 1197
2144 			 2123 			 1120
2082 			 1966 			 1088
1591 			 1683 			 877
1404 			 1657 			 757
512 			 842 			 347
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 98
282 			 248 			 68
255 			 251 			 81
204 			 169 			 53
151 			 237 			 46
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [370/1000] took 97.09415555000305s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.316364713175646, train accuracy: 0.5249780893952674
Val mean loss: 1.7154569160647508, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2502 			 1998 			 1183
2166 			 2123 			 1130
2140 			 1966 			 1113
1552 			 1683 			 861
1388 			 1657 			 751
521 			 842 			 353
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 98
268 			 248 			 63
234 			 251 			 75
226 			 169 			 57
166 			 237 			 51
67 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [371/1000] took 97.13995385169983s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3156656450200304, train accuracy: 0.5308209173239848
Val mean loss: 1.726005304150465, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1188
2210 			 2123 			 1156
2081 			 1966 			 1094
1590 			 1683 			 882
1422 			 1657 			 769
521 			 842 			 362
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
392 			 263 			 111
236 			 248 			 55
260 			 251 			 83
197 			 169 			 51
139 			 237 			 44
60 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [372/1000] took 96.29090809822083s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3142234187259851, train accuracy: 0.5259518940500536
Val mean loss: 1.7043267662932233, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2533 			 1998 			 1198
2199 			 2123 			 1142
2138 			 1966 			 1112
1539 			 1683 			 862
1337 			 1657 			 732
523 			 842 			 355
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 94
250 			 248 			 58
226 			 251 			 76
252 			 169 			 65
171 			 237 			 52
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846669824
Epoch [373/1000] took 97.08763837814331s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3184574315109727, train accuracy: 0.5244911870678742
Val mean loss: 1.7012735227259195, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2528 			 1998 			 1199
2132 			 2123 			 1126
2095 			 1966 			 1096
1643 			 1683 			 889
1367 			 1657 			 733
504 			 842 			 343
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 95
260 			 248 			 63
234 			 251 			 77
211 			 169 			 56
178 			 237 			 52
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [374/1000] took 96.36948871612549s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.313862005498179, train accuracy: 0.5283864056870192
Val mean loss: 1.7061830555520408, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2475 			 1998 			 1182
2157 			 2123 			 1139
2076 			 1966 			 1094
1600 			 1683 			 885
1415 			 1657 			 756
546 			 842 			 370
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 105
284 			 248 			 65
227 			 251 			 73
215 			 169 			 54
151 			 237 			 47
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [375/1000] took 97.03239583969116s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3156452141818227, train accuracy: 0.5248807089297887
Val mean loss: 1.7075128090090868, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2506 			 1998 			 1182
2208 			 2123 			 1146
2084 			 1966 			 1103
1578 			 1683 			 876
1360 			 1657 			 725
533 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 104
239 			 248 			 58
268 			 251 			 84
221 			 169 			 56
157 			 237 			 49
58 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [376/1000] took 96.83413004875183s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3139384557896314, train accuracy: 0.5253676112571818
Val mean loss: 1.7089797752659495, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2512 			 1998 			 1192
2115 			 2123 			 1116
2152 			 1966 			 1120
1616 			 1683 			 881
1374 			 1657 			 743
500 			 842 			 343
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
360 			 263 			 104
280 			 248 			 66
210 			 251 			 71
194 			 169 			 52
175 			 237 			 51
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [377/1000] took 96.70066666603088s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3145503771267948, train accuracy: 0.5260492745155322
Val mean loss: 1.715353736063329, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2496 			 1998 			 1183
2162 			 2123 			 1126
2086 			 1966 			 1102
1603 			 1683 			 880
1409 			 1657 			 757
513 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 91
305 			 248 			 72
244 			 251 			 76
214 			 169 			 56
145 			 237 			 46
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847259648
Epoch [378/1000] took 97.01205849647522s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3159023879844451, train accuracy: 0.5264387963774467
Val mean loss: 1.711759404438298, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2461 			 1998 			 1172
2179 			 2123 			 1131
2141 			 1966 			 1108
1566 			 1683 			 883
1393 			 1657 			 754
529 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
357 			 263 			 105
315 			 248 			 74
180 			 251 			 62
206 			 169 			 54
163 			 237 			 49
63 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3891988992
Epoch [379/1000] took 96.59851050376892s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3136879391387988, train accuracy: 0.5293602103418055
Val mean loss: 1.7263756641527501, val accuracy: 0.29750778816199375

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2533 			 1998 			 1215
2167 			 2123 			 1134
2056 			 1966 			 1092
1614 			 1683 			 892
1392 			 1657 			 755
507 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 102
307 			 248 			 77
234 			 251 			 78
205 			 169 			 53
136 			 237 			 44
65 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3906997760
Epoch [380/1000] took 96.82340359687805s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3145555380348848, train accuracy: 0.5262440354464895
Val mean loss: 1.6917051076889038, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2475 			 1998 			 1191
2174 			 2123 			 1143
2071 			 1966 			 1083
1611 			 1683 			 871
1409 			 1657 			 756
529 			 842 			 360
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 91
249 			 248 			 58
290 			 251 			 90
206 			 169 			 50
163 			 237 			 50
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [381/1000] took 96.47762441635132s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3108574018300136, train accuracy: 0.527120459635797
Val mean loss: 1.7227292962190581, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2537 			 1998 			 1203
2164 			 2123 			 1132
2108 			 1966 			 1097
1559 			 1683 			 876
1381 			 1657 			 745
520 			 842 			 360
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 95
270 			 248 			 67
223 			 251 			 72
227 			 169 			 58
157 			 237 			 49
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [382/1000] took 96.64575934410095s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3119855049614595, train accuracy: 0.526341415911968
Val mean loss: 1.7069687261814024, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2489 			 1998 			 1182
2146 			 2123 			 1136
2087 			 1966 			 1089
1608 			 1683 			 886
1409 			 1657 			 753
530 			 842 			 359
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 94
270 			 248 			 66
257 			 251 			 81
210 			 169 			 55
162 			 237 			 49
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [383/1000] took 97.06577181816101s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3127369876962585, train accuracy: 0.5258545135845749
Val mean loss: 1.7067036192591598, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2475 			 1998 			 1181
2180 			 2123 			 1130
2144 			 1966 			 1115
1591 			 1683 			 882
1361 			 1657 			 735
518 			 842 			 357
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 98
222 			 248 			 53
284 			 251 			 87
187 			 169 			 48
193 			 237 			 54
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [384/1000] took 97.2704689502716s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3123077852332332, train accuracy: 0.5268283182393612
Val mean loss: 1.7082113172949813, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2546 			 1998 			 1201
2141 			 2123 			 1127
2132 			 1966 			 1119
1535 			 1683 			 853
1389 			 1657 			 747
526 			 842 			 363
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 93
250 			 248 			 59
253 			 251 			 81
245 			 169 			 61
171 			 237 			 53
59 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [385/1000] took 97.72118473052979s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3142202520667579, train accuracy: 0.527120459635797
Val mean loss: 1.6993237908293561, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2498 			 1998 			 1192
2137 			 2123 			 1120
2124 			 1966 			 1114
1629 			 1683 			 889
1373 			 1657 			 750
508 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 93
308 			 248 			 75
222 			 251 			 74
216 			 169 			 59
149 			 237 			 46
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [386/1000] took 97.1288628578186s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.312765567844902, train accuracy: 0.5275099814977116
Val mean loss: 1.723026205853718, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2486 			 1998 			 1188
2155 			 2123 			 1123
2090 			 1966 			 1102
1624 			 1683 			 903
1387 			 1657 			 747
527 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 91
304 			 248 			 71
253 			 251 			 77
201 			 169 			 53
152 			 237 			 45
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3926167040
Epoch [387/1000] took 96.95976567268372s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.312393123115706, train accuracy: 0.5241990456714383
Val mean loss: 1.7167818720747785, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2500 			 1998 			 1176
2202 			 2123 			 1143
2113 			 1966 			 1099
1566 			 1683 			 871
1366 			 1657 			 738
522 			 842 			 356
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 93
278 			 248 			 67
255 			 251 			 80
210 			 169 			 56
160 			 237 			 49
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846735360
Epoch [388/1000] took 97.17642951011658s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3130908614007113, train accuracy: 0.5298471126691986
Val mean loss: 1.7006162201485984, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2476 			 1998 			 1191
2095 			 2123 			 1118
2160 			 1966 			 1126
1622 			 1683 			 897
1393 			 1657 			 751
523 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 100
330 			 248 			 76
227 			 251 			 74
181 			 169 			 49
137 			 237 			 45
68 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [389/1000] took 97.05021142959595s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3131656041397854, train accuracy: 0.5261466549810108
Val mean loss: 1.7050738043901397, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2470 			 1998 			 1184
2243 			 2123 			 1149
2090 			 1966 			 1098
1548 			 1683 			 865
1381 			 1657 			 750
537 			 842 			 357
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 96
258 			 248 			 61
246 			 251 			 80
223 			 169 			 57
168 			 237 			 52
66 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3900608000
Epoch [390/1000] took 96.79171705245972s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3131740223210178, train accuracy: 0.5283864056870192
Val mean loss: 1.7143492466065942, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2492 			 1998 			 1187
2212 			 2123 			 1151
2073 			 1966 			 1104
1614 			 1683 			 894
1373 			 1657 			 748
505 			 842 			 342
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 103
253 			 248 			 64
234 			 251 			 77
213 			 169 			 54
156 			 237 			 48
75 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [391/1000] took 96.88527274131775s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3125403462169327, train accuracy: 0.5278995033596261
Val mean loss: 1.7007083660218774, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2533 			 1998 			 1204
2131 			 2123 			 1121
2108 			 1966 			 1102
1588 			 1683 			 877
1383 			 1657 			 756
526 			 842 			 361
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
299 			 248 			 71
227 			 251 			 74
213 			 169 			 58
161 			 237 			 50
66 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [392/1000] took 96.8199691772461s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3093943807566277, train accuracy: 0.5278995033596261
Val mean loss: 1.7209713749769258, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2514 			 1998 			 1195
2184 			 2123 			 1137
2091 			 1966 			 1104
1596 			 1683 			 876
1378 			 1657 			 749
506 			 842 			 360
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 88
276 			 248 			 66
229 			 251 			 76
227 			 169 			 58
187 			 237 			 55
80 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3846669824
Epoch [393/1000] took 96.86353373527527s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.309599748281675, train accuracy: 0.5293602103418055
Val mean loss: 1.7099027691817865, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2480 			 1998 			 1192
2131 			 2123 			 1131
2145 			 1966 			 1123
1566 			 1683 			 875
1425 			 1657 			 763
522 			 842 			 352
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 92
297 			 248 			 69
194 			 251 			 62
225 			 169 			 56
171 			 237 			 50
82 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [394/1000] took 96.73127698898315s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3107503544875765, train accuracy: 0.5311130587204207
Val mean loss: 1.7242150277626225, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2471 			 1998 			 1201
2230 			 2123 			 1161
2051 			 1966 			 1093
1622 			 1683 			 894
1359 			 1657 			 737
536 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 96
265 			 248 			 63
252 			 251 			 82
219 			 169 			 56
166 			 237 			 52
71 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [395/1000] took 97.50666928291321s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3122821712048254, train accuracy: 0.5278021228941474
Val mean loss: 1.7099341095947638, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2508 			 1998 			 1205
2169 			 2123 			 1139
2086 			 1966 			 1092
1574 			 1683 			 872
1393 			 1657 			 751
539 			 842 			 361
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 99
242 			 248 			 59
260 			 251 			 82
223 			 169 			 57
158 			 237 			 51
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [396/1000] took 96.9172010421753s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3094100402523052, train accuracy: 0.527412601032233
Val mean loss: 1.7264081530454682, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2524 			 1998 			 1205
2126 			 2123 			 1120
2135 			 1966 			 1115
1584 			 1683 			 878
1399 			 1657 			 750
501 			 842 			 348
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 89
279 			 248 			 67
249 			 251 			 79
227 			 169 			 55
164 			 237 			 51
82 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [397/1000] took 96.91349363327026s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.310575847313783, train accuracy: 0.530236634531113
Val mean loss: 1.7194376544254582, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2454 			 1998 			 1179
2163 			 2123 			 1142
2140 			 1966 			 1115
1576 			 1683 			 882
1378 			 1657 			 750
558 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 102
305 			 248 			 76
212 			 251 			 72
202 			 169 			 55
154 			 237 			 48
60 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [398/1000] took 97.00047564506531s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3094759150828899, train accuracy: 0.5266335573084039
Val mean loss: 1.7161655193421899, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2528 			 1998 			 1196
2186 			 2123 			 1141
2096 			 1966 			 1101
1573 			 1683 			 875
1380 			 1657 			 748
506 			 842 			 347
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 92
275 			 248 			 65
230 			 251 			 76
216 			 169 			 56
179 			 237 			 51
59 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [399/1000] took 96.84394955635071s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3086632170781167, train accuracy: 0.5276073619631901
Val mean loss: 1.721732802507354, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2472 			 1998 			 1178
2167 			 2123 			 1149
2084 			 1966 			 1090
1623 			 1683 			 881
1411 			 1657 			 762
512 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 94
255 			 248 			 62
261 			 251 			 83
229 			 169 			 55
153 			 237 			 47
73 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [400/1000] took 96.77060985565186s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3082681340963298, train accuracy: 0.5264387963774467
Val mean loss: 1.7301228976831204, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2532 			 1998 			 1200
2124 			 2123 			 1115
2118 			 1966 			 1113
1591 			 1683 			 882
1367 			 1657 			 737
537 			 842 			 359
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 86
304 			 248 			 69
249 			 251 			 81
207 			 169 			 53
170 			 237 			 53
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [401/1000] took 97.04548692703247s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.308968857812733, train accuracy: 0.5297497322037199
Val mean loss: 1.7116907079045365, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2485 			 1998 			 1191
2196 			 2123 			 1152
2093 			 1966 			 1100
1577 			 1683 			 884
1409 			 1657 			 760
509 			 842 			 353
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 95
291 			 248 			 70
250 			 251 			 79
198 			 169 			 53
150 			 237 			 48
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892610560
Epoch [402/1000] took 96.71563267707825s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.30791113829687, train accuracy: 0.5293602103418055
Val mean loss: 1.7148636957494223, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1181
2196 			 2123 			 1149
2126 			 1966 			 1106
1604 			 1683 			 886
1372 			 1657 			 760
526 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
382 			 263 			 109
225 			 248 			 60
251 			 251 			 79
198 			 169 			 50
160 			 237 			 49
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [403/1000] took 96.6756386756897s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3073539759882513, train accuracy: 0.5293602103418055
Val mean loss: 1.7145089928696795, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2517 			 1998 			 1194
2128 			 2123 			 1128
2116 			 1966 			 1105
1538 			 1683 			 873
1427 			 1657 			 768
543 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 100
278 			 248 			 61
217 			 251 			 72
236 			 169 			 60
159 			 237 			 48
67 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [404/1000] took 96.8447093963623s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3068796262562832, train accuracy: 0.5280942642905833
Val mean loss: 1.704067363971617, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2542 			 1998 			 1213
2154 			 2123 			 1131
2111 			 1966 			 1105
1605 			 1683 			 880
1355 			 1657 			 744
502 			 842 			 350
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 92
254 			 248 			 63
225 			 251 			 73
243 			 169 			 58
178 			 237 			 54
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [405/1000] took 97.32192468643188s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.30878184517596, train accuracy: 0.5315999610478138
Val mean loss: 1.721898483067024, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2572 			 1998 			 1225
2144 			 2123 			 1140
2074 			 1966 			 1107
1605 			 1683 			 889
1363 			 1657 			 744
511 			 842 			 354
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
252 			 263 			 78
286 			 248 			 64
256 			 251 			 82
191 			 169 			 52
207 			 237 			 62
92 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3892087296
Epoch [406/1000] took 96.91381549835205s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3069560505519404, train accuracy: 0.5277047424286688
Val mean loss: 1.7176106964669577, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1181
2168 			 2123 			 1132
2132 			 1966 			 1108
1564 			 1683 			 871
1425 			 1657 			 760
546 			 842 			 367
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 88
277 			 248 			 68
235 			 251 			 75
233 			 169 			 56
169 			 237 			 47
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [407/1000] took 96.44223594665527s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3057850086800407, train accuracy: 0.5331580484954718
Val mean loss: 1.7141589507824038, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2407 			 1998 			 1182
2192 			 2123 			 1156
2088 			 1966 			 1106
1647 			 1683 			 901
1407 			 1657 			 767
528 			 842 			 363
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 100
289 			 248 			 67
232 			 251 			 76
200 			 169 			 55
151 			 237 			 48
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [408/1000] took 96.67718005180359s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3056236345819967, train accuracy: 0.5311130587204207
Val mean loss: 1.739507009343403, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2454 			 1998 			 1180
2180 			 2123 			 1149
2089 			 1966 			 1113
1603 			 1683 			 889
1412 			 1657 			 755
531 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 103
263 			 248 			 60
253 			 251 			 79
200 			 169 			 53
144 			 237 			 45
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [409/1000] took 96.75215530395508s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3050679417785453, train accuracy: 0.5288733080144123
Val mean loss: 1.697280046416492, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2582 			 1998 			 1220
2125 			 2123 			 1120
2154 			 1966 			 1120
1528 			 1683 			 864
1338 			 1657 			 738
542 			 842 			 369
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
275 			 263 			 85
295 			 248 			 67
227 			 251 			 76
240 			 169 			 61
188 			 237 			 55
59 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3847259648
Epoch [410/1000] took 96.4990246295929s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3056560407546451, train accuracy: 0.5314052001168565
Val mean loss: 1.7238704285970547, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2430 			 1998 			 1177
2179 			 2123 			 1144
2042 			 1966 			 1088
1632 			 1683 			 906
1458 			 1657 			 785
528 			 842 			 357
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 103
281 			 248 			 66
236 			 251 			 77
209 			 169 			 56
136 			 237 			 41
73 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [411/1000] took 96.58502626419067s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3056650351141101, train accuracy: 0.5276073619631901
Val mean loss: 1.7296785406950044, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2505 			 1998 			 1184
2201 			 2123 			 1144
2093 			 1966 			 1102
1592 			 1683 			 885
1340 			 1657 			 733
538 			 842 			 370
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 100
253 			 248 			 59
249 			 251 			 78
240 			 169 			 61
147 			 237 			 45
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [412/1000] took 96.5766851902008s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.306316855540528, train accuracy: 0.5337423312883436
Val mean loss: 1.7227067947387695, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2470 			 1998 			 1196
2193 			 2123 			 1149
2099 			 1966 			 1115
1590 			 1683 			 898
1383 			 1657 			 756
534 			 842 			 367
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 93
260 			 248 			 59
267 			 251 			 83
226 			 169 			 58
154 			 237 			 48
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [413/1000] took 96.96023178100586s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3071829771327081, train accuracy: 0.5315025805823352
Val mean loss: 1.7390318440227974, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2528 			 1998 			 1211
2164 			 2123 			 1137
2112 			 1966 			 1117
1568 			 1683 			 876
1374 			 1657 			 757
523 			 842 			 360
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
275 			 263 			 86
255 			 248 			 59
292 			 251 			 89
219 			 169 			 54
166 			 237 			 50
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [414/1000] took 97.11313438415527s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3060544920487567, train accuracy: 0.5329632875645146
Val mean loss: 1.7274942950504582, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2424 			 1998 			 1175
2108 			 2123 			 1131
2178 			 1966 			 1139
1598 			 1683 			 894
1428 			 1657 			 766
533 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 98
305 			 248 			 70
220 			 251 			 73
225 			 169 			 59
147 			 237 			 45
68 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [415/1000] took 96.92181372642517s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.305860730717858, train accuracy: 0.5311130587204207
Val mean loss: 1.707050300225979, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2470 			 1998 			 1191
2185 			 2123 			 1152
2108 			 1966 			 1112
1619 			 1683 			 895
1343 			 1657 			 737
544 			 842 			 367
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 103
268 			 248 			 63
263 			 251 			 79
204 			 169 			 55
143 			 237 			 46
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [416/1000] took 96.26854681968689s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3036234512507359, train accuracy: 0.5323790047716428
Val mean loss: 1.7451727739194545, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2572 			 1998 			 1222
2155 			 2123 			 1140
2070 			 1966 			 1097
1570 			 1683 			 884
1373 			 1657 			 756
529 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 90
303 			 248 			 67
252 			 251 			 79
208 			 169 			 53
151 			 237 			 48
73 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [417/1000] took 96.92117285728455s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3061579269040782, train accuracy: 0.5329632875645146
Val mean loss: 1.7106958307871005, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2451 			 1998 			 1190
2133 			 2123 			 1135
2143 			 1966 			 1121
1610 			 1683 			 893
1393 			 1657 			 768
539 			 842 			 366
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 100
325 			 248 			 78
202 			 251 			 70
207 			 169 			 53
154 			 237 			 49
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [418/1000] took 96.42074584960938s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.303240771976958, train accuracy: 0.5298471126691986
Val mean loss: 1.7122879609829043, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2505 			 1998 			 1201
2175 			 2123 			 1136
2080 			 1966 			 1110
1600 			 1683 			 888
1401 			 1657 			 757
508 			 842 			 349
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 97
279 			 248 			 67
213 			 251 			 72
222 			 169 			 56
161 			 237 			 49
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [419/1000] took 96.74858117103577s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3036975045441839, train accuracy: 0.5321842438406855
Val mean loss: 1.7109144955146602, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2511 			 1998 			 1211
2151 			 2123 			 1142
2082 			 1966 			 1103
1588 			 1683 			 891
1396 			 1657 			 753
541 			 842 			 365
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 91
269 			 248 			 61
243 			 251 			 78
232 			 169 			 59
163 			 237 			 50
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [420/1000] took 96.84299063682556s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3042724667308487, train accuracy: 0.530236634531113
Val mean loss: 1.7145199339564254, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2477 			 1998 			 1188
2183 			 2123 			 1151
2121 			 1966 			 1112
1580 			 1683 			 883
1386 			 1657 			 750
522 			 842 			 361
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 89
256 			 248 			 63
260 			 251 			 82
242 			 169 			 59
154 			 237 			 48
67 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [421/1000] took 96.70431232452393s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3024312172351968, train accuracy: 0.5277047424286688
Val mean loss: 1.7310471999935988, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2513 			 1998 			 1200
2133 			 2123 			 1128
2098 			 1966 			 1098
1595 			 1683 			 876
1391 			 1657 			 751
539 			 842 			 366
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 87
286 			 248 			 66
278 			 251 			 84
207 			 169 			 52
168 			 237 			 51
61 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [422/1000] took 96.57278299331665s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3029735242466318, train accuracy: 0.5307235368585062
Val mean loss: 1.7146158014855735, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2411 			 1998 			 1167
2205 			 2123 			 1152
2162 			 1966 			 1131
1580 			 1683 			 895
1404 			 1657 			 759
507 			 842 			 346
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 99
232 			 248 			 54
242 			 251 			 80
228 			 169 			 59
177 			 237 			 51
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [423/1000] took 96.79635000228882s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3026420572465083, train accuracy: 0.5319894829097284
Val mean loss: 1.7286507676287395, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2492 			 1998 			 1199
2167 			 2123 			 1138
2104 			 1966 			 1120
1572 			 1683 			 879
1395 			 1657 			 761
539 			 842 			 366
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
244 			 248 			 57
272 			 251 			 83
214 			 169 			 54
167 			 237 			 52
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [424/1000] took 96.6771948337555s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3035286750377524, train accuracy: 0.5342292336157367
Val mean loss: 1.7246603675004912, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2489 			 1998 			 1190
2120 			 2123 			 1137
2069 			 1966 			 1106
1654 			 1683 			 917
1400 			 1657 			 768
537 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 104
287 			 248 			 69
237 			 251 			 77
194 			 169 			 51
151 			 237 			 47
64 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [425/1000] took 96.66412997245789s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3018702621400542, train accuracy: 0.5301392540656344
Val mean loss: 1.7149944857853214, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2487 			 1998 			 1194
2170 			 2123 			 1144
2096 			 1966 			 1106
1611 			 1683 			 895
1393 			 1657 			 748
512 			 842 			 357
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 102
249 			 248 			 58
244 			 251 			 77
212 			 169 			 53
173 			 237 			 50
62 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [426/1000] took 97.11036372184753s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3041575482329848, train accuracy: 0.5325737657026001
Val mean loss: 1.7067439905027064, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2509 			 1998 			 1211
2164 			 2123 			 1140
2109 			 1966 			 1110
1561 			 1683 			 881
1404 			 1657 			 768
522 			 842 			 359
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 88
270 			 248 			 65
229 			 251 			 74
250 			 169 			 63
166 			 237 			 50
76 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [427/1000] took 96.73271059989929s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3024899599336761, train accuracy: 0.5316973415132924
Val mean loss: 1.7206674029187459, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2509 			 1998 			 1202
2171 			 2123 			 1144
2078 			 1966 			 1104
1587 			 1683 			 888
1390 			 1657 			 757
534 			 842 			 365
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 89
256 			 248 			 60
238 			 251 			 77
253 			 169 			 61
176 			 237 			 54
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [428/1000] took 96.8414454460144s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3014529765209304, train accuracy: 0.535203038270523
Val mean loss: 1.712685302990239, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2439 			 1998 			 1186
2169 			 2123 			 1150
2074 			 1966 			 1101
1629 			 1683 			 904
1410 			 1657 			 781
548 			 842 			 374
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 99
275 			 248 			 63
250 			 251 			 79
235 			 169 			 59
139 			 237 			 44
62 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [429/1000] took 96.76587200164795s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3033088794749845, train accuracy: 0.5328659070990359
Val mean loss: 1.7368766970750762, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2496 			 1998 			 1216
2130 			 2123 			 1128
2147 			 1966 			 1124
1629 			 1683 			 899
1348 			 1657 			 749
519 			 842 			 356
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 90
296 			 248 			 65
228 			 251 			 73
224 			 169 			 56
171 			 237 			 51
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [430/1000] took 96.40733122825623s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2994655181314343, train accuracy: 0.5323790047716428
Val mean loss: 1.7129719664410847, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2455 			 1998 			 1177
2118 			 2123 			 1134
2112 			 1966 			 1111
1638 			 1683 			 905
1414 			 1657 			 768
532 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 100
310 			 248 			 73
221 			 251 			 73
206 			 169 			 57
145 			 237 			 44
76 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [431/1000] took 96.44261741638184s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3004277647470017, train accuracy: 0.5346187554776511
Val mean loss: 1.7126568933812583, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2488 			 1998 			 1207
2203 			 2123 			 1159
2070 			 1966 			 1109
1592 			 1683 			 889
1368 			 1657 			 749
548 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 93
242 			 248 			 62
236 			 251 			 76
239 			 169 			 59
181 			 237 			 52
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [432/1000] took 96.99323630332947s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.3017416512854745, train accuracy: 0.5315999610478138
Val mean loss: 1.7108653493043853, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2548 			 1998 			 1207
2118 			 2123 			 1136
2088 			 1966 			 1111
1601 			 1683 			 900
1414 			 1657 			 759
500 			 842 			 346
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
262 			 263 			 81
293 			 248 			 69
256 			 251 			 81
237 			 169 			 59
159 			 237 			 51
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [433/1000] took 96.47819781303406s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2983336329831512, train accuracy: 0.538319213165839
Val mean loss: 1.7157512147252152, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1187
2166 			 2123 			 1162
2120 			 1966 			 1121
1609 			 1683 			 904
1398 			 1657 			 767
559 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 95
313 			 248 			 71
216 			 251 			 72
213 			 169 			 55
157 			 237 			 50
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [434/1000] took 96.89773964881897s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.301180194471484, train accuracy: 0.5335475703573863
Val mean loss: 1.7309929888422897, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2508 			 1998 			 1204
2150 			 2123 			 1144
2122 			 1966 			 1126
1615 			 1683 			 900
1361 			 1657 			 747
513 			 842 			 358
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 91
290 			 248 			 75
232 			 251 			 73
200 			 169 			 53
171 			 237 			 50
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891988992
Epoch [435/1000] took 96.69758462905884s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2991828593509591, train accuracy: 0.5325737657026001
Val mean loss: 1.7207597290597312, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2490 			 1998 			 1204
2162 			 2123 			 1146
2059 			 1966 			 1089
1564 			 1683 			 877
1461 			 1657 			 789
533 			 842 			 364
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
282 			 263 			 87
278 			 248 			 61
271 			 251 			 83
222 			 169 			 55
156 			 237 			 48
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [436/1000] took 96.9457368850708s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.300189487844984, train accuracy: 0.5351056578050443
Val mean loss: 1.7169255047309688, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2465 			 1998 			 1200
2165 			 2123 			 1156
2115 			 1966 			 1114
1617 			 1683 			 900
1378 			 1657 			 761
529 			 842 			 364
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 95
263 			 248 			 62
275 			 251 			 85
211 			 169 			 52
153 			 237 			 46
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [437/1000] took 96.42361426353455s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2998532443031716, train accuracy: 0.5362742233907878
Val mean loss: 1.709249420863826, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2453 			 1998 			 1193
2179 			 2123 			 1166
2118 			 1966 			 1119
1588 			 1683 			 891
1390 			 1657 			 770
541 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 91
273 			 248 			 61
263 			 251 			 81
223 			 169 			 56
164 			 237 			 52
76 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3926167040
Epoch [438/1000] took 96.62072014808655s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2993280044597257, train accuracy: 0.5310156782549421
Val mean loss: 1.7137906144304973, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2465 			 1998 			 1186
2162 			 2123 			 1132
2112 			 1966 			 1114
1612 			 1683 			 900
1368 			 1657 			 746
550 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 90
292 			 248 			 69
241 			 251 			 76
219 			 169 			 54
170 			 237 			 56
58 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [439/1000] took 96.95735836029053s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2996252359259537, train accuracy: 0.5326711461680786
Val mean loss: 1.7160153679731416, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2495 			 1998 			 1202
2170 			 2123 			 1148
2101 			 1966 			 1110
1587 			 1683 			 879
1404 			 1657 			 770
512 			 842 			 361
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 91
280 			 248 			 60
259 			 251 			 81
228 			 169 			 57
154 			 237 			 48
64 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [440/1000] took 96.51622605323792s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2975571536200812, train accuracy: 0.5355925601324374
Val mean loss: 1.705667553878412, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2450 			 1998 			 1188
2182 			 2123 			 1160
2158 			 1966 			 1127
1569 			 1683 			 894
1396 			 1657 			 763
514 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 98
255 			 248 			 59
221 			 251 			 74
232 			 169 			 61
164 			 237 			 49
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846669824
Epoch [441/1000] took 96.71340727806091s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2981853516673745, train accuracy: 0.5332554289609505
Val mean loss: 1.7121834028057936, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2514 			 1998 			 1210
2110 			 2123 			 1129
2069 			 1966 			 1097
1626 			 1683 			 900
1411 			 1657 			 777
539 			 842 			 363
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 95
262 			 248 			 64
241 			 251 			 77
249 			 169 			 65
152 			 237 			 48
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892447744
Epoch [442/1000] took 96.86065983772278s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2987334899070477, train accuracy: 0.5345213750121726
Val mean loss: 1.7132605663159999, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2479 			 1998 			 1198
2124 			 2123 			 1133
2120 			 1966 			 1122
1614 			 1683 			 905
1401 			 1657 			 760
531 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 98
310 			 248 			 75
228 			 251 			 72
217 			 169 			 58
141 			 237 			 43
63 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [443/1000] took 96.71458077430725s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.298715975054328, train accuracy: 0.5334501898919077
Val mean loss: 1.709138835348734, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2415 			 1998 			 1180
2237 			 2123 			 1164
2075 			 1966 			 1099
1598 			 1683 			 890
1404 			 1657 			 770
540 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
373 			 263 			 107
247 			 248 			 56
260 			 251 			 80
201 			 169 			 54
138 			 237 			 46
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [444/1000] took 96.85601449012756s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2985613171556658, train accuracy: 0.5311130587204207
Val mean loss: 1.7294206561111822, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2522 			 1998 			 1205
2096 			 2123 			 1123
2091 			 1966 			 1098
1623 			 1683 			 900
1432 			 1657 			 772
505 			 842 			 356
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 101
281 			 248 			 66
261 			 251 			 83
195 			 169 			 54
138 			 237 			 41
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892218368
Epoch [445/1000] took 97.18198132514954s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2998869645632687, train accuracy: 0.5368585061836596
Val mean loss: 1.7336139969709443, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2491 			 1998 			 1214
2153 			 2123 			 1154
2159 			 1966 			 1138
1567 			 1683 			 881
1337 			 1657 			 735
562 			 842 			 391
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 93
276 			 248 			 65
221 			 251 			 74
235 			 169 			 57
174 			 237 			 50
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3804758528
Epoch [446/1000] took 97.06348466873169s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.299537640865718, train accuracy: 0.5327685266335573
Val mean loss: 1.7372622780683564, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2496 			 1998 			 1208
2139 			 2123 			 1128
2081 			 1966 			 1104
1610 			 1683 			 901
1406 			 1657 			 758
537 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 91
290 			 248 			 65
229 			 251 			 77
240 			 169 			 63
158 			 237 			 48
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [447/1000] took 96.52263975143433s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2958096470045524, train accuracy: 0.5365663647872236
Val mean loss: 1.7090583341877634, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2463 			 1998 			 1201
2152 			 2123 			 1148
2109 			 1966 			 1117
1614 			 1683 			 908
1400 			 1657 			 766
531 			 842 			 370
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 95
268 			 248 			 57
256 			 251 			 82
211 			 169 			 56
156 			 237 			 46
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [448/1000] took 96.68267726898193s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.296971856062286, train accuracy: 0.5337423312883436
Val mean loss: 1.7131977401128629, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2488 			 1998 			 1194
2223 			 2123 			 1168
2084 			 1966 			 1112
1560 			 1683 			 883
1379 			 1657 			 752
535 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 88
221 			 248 			 55
241 			 251 			 72
256 			 169 			 65
177 			 237 			 49
78 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892546048
Epoch [449/1000] took 96.67328405380249s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.29497920575543, train accuracy: 0.5357873210633947
Val mean loss: 1.734466023561431, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2456 			 1998 			 1198
2115 			 2123 			 1126
2136 			 1966 			 1130
1613 			 1683 			 900
1415 			 1657 			 775
534 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 97
283 			 248 			 66
205 			 251 			 67
240 			 169 			 61
152 			 237 			 45
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3804758528
Epoch [450/1000] took 96.76222515106201s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2962010983737458, train accuracy: 0.5367611257181809
Val mean loss: 1.7257920678068952, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2522 			 1998 			 1219
2194 			 2123 			 1158
2021 			 1966 			 1107
1590 			 1683 			 894
1405 			 1657 			 761
537 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 90
249 			 248 			 59
259 			 251 			 81
229 			 169 			 54
167 			 237 			 50
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [451/1000] took 96.85502314567566s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.297482159278846, train accuracy: 0.5378323108384458
Val mean loss: 1.7084537599144913, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2460 			 1998 			 1201
2143 			 2123 			 1153
2081 			 1966 			 1106
1635 			 1683 			 905
1422 			 1657 			 790
528 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
287 			 263 			 88
299 			 248 			 74
272 			 251 			 84
207 			 169 			 55
146 			 237 			 44
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [452/1000] took 96.78851199150085s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2962084626111658, train accuracy: 0.5336449508228649
Val mean loss: 1.7509267417395986, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1189
2166 			 2123 			 1143
2108 			 1966 			 1115
1584 			 1683 			 885
1422 			 1657 			 773
544 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 93
288 			 248 			 64
243 			 251 			 79
229 			 169 			 57
163 			 237 			 51
70 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3892087296
Epoch [453/1000] took 97.82980632781982s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2980234270155244, train accuracy: 0.5369558866491382
Val mean loss: 1.734835354293265, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2447 			 1998 			 1189
2152 			 2123 			 1155
2163 			 1966 			 1140
1597 			 1683 			 895
1373 			 1657 			 762
537 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 97
272 			 248 			 65
222 			 251 			 74
237 			 169 			 59
158 			 237 			 47
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3850143232
Epoch [454/1000] took 97.90980839729309s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2956350326909454, train accuracy: 0.5376375499074886
Val mean loss: 1.7366099241303234, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1203
2199 			 2123 			 1156
2065 			 1966 			 1106
1599 			 1683 			 896
1429 			 1657 			 782
543 			 842 			 378
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
351 			 263 			 107
259 			 248 			 63
258 			 251 			 80
213 			 169 			 55
136 			 237 			 43
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [455/1000] took 97.54901266098022s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2954512570877312, train accuracy: 0.5325737657026001
Val mean loss: 1.7437867972908951, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2546 			 1998 			 1211
2129 			 2123 			 1138
2125 			 1966 			 1127
1560 			 1683 			 886
1381 			 1657 			 746
528 			 842 			 361
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
282 			 263 			 87
278 			 248 			 64
259 			 251 			 80
231 			 169 			 58
160 			 237 			 47
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [456/1000] took 97.70295095443726s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2947148890510154, train accuracy: 0.5405589638718473
Val mean loss: 1.7094786545125449, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2407 			 1998 			 1188
2126 			 2123 			 1147
2104 			 1966 			 1121
1649 			 1683 			 930
1425 			 1657 			 782
558 			 842 			 383
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 92
279 			 248 			 64
274 			 251 			 85
197 			 169 			 51
164 			 237 			 49
57 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3892578816
Epoch [457/1000] took 97.94702792167664s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2952921386819762, train accuracy: 0.538027071769403
Val mean loss: 1.7185459398641818, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2479 			 1998 			 1204
2177 			 2123 			 1165
2162 			 1966 			 1144
1531 			 1683 			 876
1392 			 1657 			 769
528 			 842 			 367
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 98
276 			 248 			 62
239 			 251 			 73
213 			 169 			 55
153 			 237 			 47
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [458/1000] took 97.83285641670227s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2932609941729132, train accuracy: 0.5351056578050443
Val mean loss: 1.7116852329998482, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2543 			 1998 			 1209
2162 			 2123 			 1146
2080 			 1966 			 1107
1559 			 1683 			 888
1384 			 1657 			 768
541 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 92
298 			 248 			 67
264 			 251 			 81
210 			 169 			 54
144 			 237 			 46
65 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3800498688
Epoch [459/1000] took 97.90918731689453s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.294238230149694, train accuracy: 0.5343266140812153
Val mean loss: 1.723450582201888, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2410 			 1998 			 1178
2214 			 2123 			 1168
2093 			 1966 			 1101
1607 			 1683 			 896
1420 			 1657 			 776
525 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 105
268 			 248 			 60
245 			 251 			 74
204 			 169 			 54
142 			 237 			 45
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892316672
Epoch [460/1000] took 98.0045530796051s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.29447309083285, train accuracy: 0.5370532671146168
Val mean loss: 1.7131769424531518, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2551 			 1998 			 1219
2133 			 2123 			 1142
2097 			 1966 			 1124
1586 			 1683 			 906
1369 			 1657 			 754
533 			 842 			 370
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
275 			 263 			 85
296 			 248 			 69
270 			 251 			 81
216 			 169 			 55
152 			 237 			 46
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892610560
Epoch [461/1000] took 97.95825338363647s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2939298242052024, train accuracy: 0.5358847015288734
Val mean loss: 1.7155499342011242, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2460 			 1998 			 1195
2179 			 2123 			 1166
2081 			 1966 			 1104
1619 			 1683 			 901
1402 			 1657 			 766
528 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 87
277 			 248 			 62
284 			 251 			 88
202 			 169 			 52
161 			 237 			 51
76 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [462/1000] took 97.55559158325195s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2925524173124556, train accuracy: 0.5347161359431298
Val mean loss: 1.7175103106149814, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2431 			 1998 			 1191
2212 			 2123 			 1155
2140 			 1966 			 1121
1582 			 1683 			 899
1374 			 1657 			 760
530 			 842 			 365
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 97
230 			 248 			 54
268 			 251 			 83
213 			 169 			 53
180 			 237 			 55
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846964736
Epoch [463/1000] took 98.34058380126953s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2943122242098657, train accuracy: 0.5356899405979161
Val mean loss: 1.7282199685166522, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2557 			 1998 			 1229
2149 			 2123 			 1143
2069 			 1966 			 1106
1579 			 1683 			 901
1376 			 1657 			 751
539 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
280 			 263 			 87
260 			 248 			 59
264 			 251 			 82
223 			 169 			 57
184 			 237 			 56
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [464/1000] took 97.90724015235901s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2915565702031335, train accuracy: 0.5384165936313176
Val mean loss: 1.7208052611932523, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2416 			 1998 			 1177
2183 			 2123 			 1159
2113 			 1966 			 1131
1581 			 1683 			 906
1444 			 1657 			 787
532 			 842 			 369
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 96
243 			 248 			 58
249 			 251 			 81
230 			 169 			 58
158 			 237 			 48
66 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [465/1000] took 97.47826027870178s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2926573296573673, train accuracy: 0.5366637452527023
Val mean loss: 1.7352537440090645, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2527 			 1998 			 1214
2100 			 2123 			 1139
2107 			 1966 			 1124
1617 			 1683 			 901
1387 			 1657 			 766
531 			 842 			 367
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
272 			 263 			 82
306 			 248 			 69
241 			 251 			 74
234 			 169 			 57
161 			 237 			 50
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [466/1000] took 98.04332613945007s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2924256374902814, train accuracy: 0.5363716038562665
Val mean loss: 1.7258968847553904, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2379 			 1998 			 1173
2186 			 2123 			 1165
2142 			 1966 			 1134
1625 			 1683 			 905
1395 			 1657 			 760
542 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 86
281 			 248 			 65
271 			 251 			 81
188 			 169 			 49
175 			 237 			 54
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [467/1000] took 98.18938207626343s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2934286512689799, train accuracy: 0.5365663647872236
Val mean loss: 1.7164373339676275, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2487 			 1998 			 1207
2170 			 2123 			 1149
2091 			 1966 			 1117
1580 			 1683 			 893
1397 			 1657 			 774
544 			 842 			 370
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 93
279 			 248 			 61
258 			 251 			 78
222 			 169 			 56
159 			 237 			 48
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3804758528
Epoch [468/1000] took 98.00899815559387s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2926626838627635, train accuracy: 0.5394877787515824
Val mean loss: 1.7337083089642409, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2458 			 1998 			 1205
2168 			 2123 			 1156
2152 			 1966 			 1143
1561 			 1683 			 894
1394 			 1657 			 769
536 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 96
272 			 248 			 62
241 			 251 			 78
226 			 169 			 62
160 			 237 			 50
67 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [469/1000] took 98.13435769081116s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2940967807145876, train accuracy: 0.5384165936313176
Val mean loss: 1.7142277868782603, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2490 			 1998 			 1221
2145 			 2123 			 1142
2068 			 1966 			 1108
1630 			 1683 			 909
1420 			 1657 			 784
516 			 842 			 365
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 101
272 			 248 			 64
262 			 251 			 83
195 			 169 			 53
151 			 237 			 45
79 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [470/1000] took 97.51401114463806s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2910091342212997, train accuracy: 0.538319213165839
Val mean loss: 1.7442713103643277, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2497 			 1998 			 1205
2140 			 2123 			 1150
2083 			 1966 			 1123
1619 			 1683 			 905
1387 			 1657 			 773
543 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 97
274 			 248 			 65
303 			 251 			 90
174 			 169 			 48
151 			 237 			 45
69 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [471/1000] took 98.00041604042053s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2917814150777562, train accuracy: 0.5361768429253092
Val mean loss: 1.7223661440174753, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2466 			 1998 			 1206
2158 			 2123 			 1144
2158 			 1966 			 1134
1554 			 1683 			 887
1403 			 1657 			 764
530 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 97
304 			 248 			 70
219 			 251 			 70
230 			 169 			 58
140 			 237 			 43
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [472/1000] took 98.05170035362244s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2912516423103595, train accuracy: 0.5376375499074886
Val mean loss: 1.7219919780405557, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2423 			 1998 			 1193
2145 			 2123 			 1151
2129 			 1966 			 1119
1616 			 1683 			 905
1418 			 1657 			 778
538 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 97
317 			 248 			 71
230 			 251 			 73
215 			 169 			 57
134 			 237 			 39
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891726848
Epoch [473/1000] took 97.62495398521423s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.292154371181381, train accuracy: 0.5388061154932321
Val mean loss: 1.7119524130007115, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2516 			 1998 			 1220
2148 			 2123 			 1144
2075 			 1966 			 1118
1621 			 1683 			 909
1392 			 1657 			 771
517 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 87
285 			 248 			 67
244 			 251 			 77
220 			 169 			 56
173 			 237 			 51
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [474/1000] took 97.56490635871887s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2880954946684318, train accuracy: 0.5389034959587107
Val mean loss: 1.7041128757523327, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2472 			 1998 			 1201
2194 			 2123 			 1175
2087 			 1966 			 1113
1571 			 1683 			 895
1397 			 1657 			 777
548 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 86
243 			 248 			 58
267 			 251 			 80
217 			 169 			 54
192 			 237 			 56
63 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [475/1000] took 97.63601851463318s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.289956757955462, train accuracy: 0.5377349303729672
Val mean loss: 1.7231442026975679, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2471 			 1998 			 1202
2128 			 2123 			 1151
2101 			 1966 			 1128
1603 			 1683 			 896
1426 			 1657 			 776
540 			 842 			 369
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 91
295 			 248 			 68
238 			 251 			 76
218 			 169 			 55
154 			 237 			 48
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [476/1000] took 97.84056305885315s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2892417829727458, train accuracy: 0.5384165936313176
Val mean loss: 1.714271589023311, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2377 			 1998 			 1187
2186 			 2123 			 1168
2098 			 1966 			 1120
1641 			 1683 			 907
1424 			 1657 			 774
543 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 100
246 			 248 			 57
244 			 251 			 79
216 			 169 			 55
167 			 237 			 53
70 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [477/1000] took 97.94119501113892s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2897847274382166, train accuracy: 0.5405589638718473
Val mean loss: 1.7287541249903238, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2481 			 1998 			 1213
2149 			 2123 			 1153
2109 			 1966 			 1125
1606 			 1683 			 910
1379 			 1657 			 778
545 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 107
270 			 248 			 65
233 			 251 			 76
180 			 169 			 47
174 			 237 			 51
61 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [478/1000] took 97.74618124961853s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2896341987859423, train accuracy: 0.5378323108384458
Val mean loss: 1.709630878960214, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2479 			 1998 			 1209
2215 			 2123 			 1173
2067 			 1966 			 1121
1592 			 1683 			 890
1390 			 1657 			 765
526 			 842 			 365
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 93
284 			 248 			 65
237 			 251 			 77
205 			 169 			 54
176 			 237 			 53
73 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [479/1000] took 97.90366077423096s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2888921365559658, train accuracy: 0.5377349303729672
Val mean loss: 1.7208249249109409, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2476 			 1998 			 1197
2155 			 2123 			 1150
2094 			 1966 			 1119
1611 			 1683 			 908
1405 			 1657 			 773
528 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 103
281 			 248 			 66
202 			 251 			 66
230 			 169 			 59
149 			 237 			 45
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [480/1000] took 97.84826040267944s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2882655064264934, train accuracy: 0.54036420294089
Val mean loss: 1.7166738015849416, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2433 			 1998 			 1189
2173 			 2123 			 1166
2092 			 1966 			 1118
1583 			 1683 			 901
1440 			 1657 			 788
548 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
362 			 263 			 107
283 			 248 			 65
230 			 251 			 72
215 			 169 			 56
129 			 237 			 40
65 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [481/1000] took 97.77044486999512s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.289396603902181, train accuracy: 0.5392930178206252
Val mean loss: 1.7212643768729232, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2442 			 1998 			 1203
2157 			 2123 			 1144
2121 			 1966 			 1134
1590 			 1683 			 900
1424 			 1657 			 782
535 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 102
257 			 248 			 62
236 			 251 			 76
229 			 169 			 58
153 			 237 			 49
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845490176
Epoch [482/1000] took 97.96772909164429s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2883096775533254, train accuracy: 0.5370532671146168
Val mean loss: 1.73415488440816, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2525 			 1998 			 1214
2100 			 2123 			 1147
2106 			 1966 			 1116
1582 			 1683 			 900
1419 			 1657 			 772
537 			 842 			 366
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 91
366 			 248 			 84
207 			 251 			 70
224 			 169 			 59
119 			 237 			 38
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [483/1000] took 97.99917101860046s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2900646887835683, train accuracy: 0.5364689843217451
Val mean loss: 1.7485131752200243, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2496 			 1998 			 1194
2182 			 2123 			 1150
2072 			 1966 			 1115
1601 			 1683 			 906
1380 			 1657 			 764
538 			 842 			 380
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
255 			 263 			 80
298 			 248 			 71
240 			 251 			 77
233 			 169 			 58
183 			 237 			 57
75 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [484/1000] took 97.60443329811096s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2868485870391038, train accuracy: 0.5390008764241893
Val mean loss: 1.7176789743144338, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1177
2200 			 2123 			 1166
2099 			 1966 			 1118
1625 			 1683 			 909
1436 			 1657 			 783
553 			 842 			 382
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 103
240 			 248 			 56
250 			 251 			 78
216 			 169 			 59
162 			 237 			 50
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [485/1000] took 97.6970751285553s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2881962105492566, train accuracy: 0.5402668224754115
Val mean loss: 1.7251843592015708, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2440 			 1998 			 1194
2180 			 2123 			 1171
2118 			 1966 			 1129
1622 			 1683 			 917
1370 			 1657 			 766
539 			 842 			 371
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 95
256 			 248 			 58
247 			 251 			 79
207 			 169 			 55
190 			 237 			 58
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [486/1000] took 97.58602046966553s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2873370355906144, train accuracy: 0.5378323108384458
Val mean loss: 1.7300331069201957, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2457 			 1998 			 1200
2151 			 2123 			 1159
2107 			 1966 			 1113
1594 			 1683 			 891
1423 			 1657 			 787
537 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 100
283 			 248 			 69
239 			 251 			 77
195 			 169 			 53
143 			 237 			 44
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [487/1000] took 97.81057953834534s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.287538524914382, train accuracy: 0.539098256889668
Val mean loss: 1.7277252354272983, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2478 			 1998 			 1210
2197 			 2123 			 1167
2144 			 1966 			 1132
1538 			 1683 			 884
1368 			 1657 			 764
544 			 842 			 379
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 92
298 			 248 			 70
191 			 251 			 66
253 			 169 			 60
169 			 237 			 50
71 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3891726848
Epoch [488/1000] took 97.86555814743042s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2892950693394907, train accuracy: 0.5437725192326419
Val mean loss: 1.7287899168526255, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2427 			 1998 			 1207
2148 			 2123 			 1161
2112 			 1966 			 1130
1641 			 1683 			 923
1393 			 1657 			 778
548 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 90
293 			 248 			 68
219 			 251 			 69
207 			 169 			 54
179 			 237 			 54
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [489/1000] took 97.42974066734314s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2865312066033623, train accuracy: 0.5388061154932321
Val mean loss: 1.724135102295294, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2501 			 1998 			 1206
2078 			 2123 			 1135
2111 			 1966 			 1125
1607 			 1683 			 908
1431 			 1657 			 783
541 			 842 			 376
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 94
328 			 248 			 73
211 			 251 			 69
206 			 169 			 57
141 			 237 			 46
80 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [490/1000] took 97.4487624168396s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.285487726841389, train accuracy: 0.5394877787515824
Val mean loss: 1.745861094172408, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2501 			 1998 			 1208
2152 			 2123 			 1172
2084 			 1966 			 1108
1583 			 1683 			 899
1398 			 1657 			 768
551 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
280 			 263 			 83
349 			 248 			 80
206 			 251 			 68
228 			 169 			 56
147 			 237 			 45
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846800896
Epoch [491/1000] took 97.95085167884827s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2878840712372017, train accuracy: 0.5391956373551465
Val mean loss: 1.7426036945203456, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2495 			 1998 			 1210
2165 			 2123 			 1157
2102 			 1966 			 1119
1602 			 1683 			 918
1369 			 1657 			 759
536 			 842 			 374
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 98
303 			 248 			 71
220 			 251 			 73
223 			 169 			 56
161 			 237 			 50
61 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [492/1000] took 97.89053750038147s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2887411477781159, train accuracy: 0.5397799201480183
Val mean loss: 1.7197863212445887, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2461 			 1998 			 1213
2169 			 2123 			 1155
2063 			 1966 			 1109
1636 			 1683 			 910
1403 			 1657 			 783
537 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 84
288 			 248 			 69
239 			 251 			 75
215 			 169 			 55
170 			 237 			 51
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [493/1000] took 97.59861016273499s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2875560462660507, train accuracy: 0.5399746810789755
Val mean loss: 1.7250788909632986, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2440 			 1998 			 1203
2138 			 2123 			 1149
2139 			 1966 			 1133
1598 			 1683 			 908
1407 			 1657 			 774
547 			 842 			 378
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 91
313 			 248 			 71
220 			 251 			 72
200 			 169 			 53
169 			 237 			 51
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [494/1000] took 97.28860569000244s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2848393876233204, train accuracy: 0.5410458661992404
Val mean loss: 1.7189638236673868, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2477 			 1998 			 1218
2194 			 2123 			 1168
2075 			 1966 			 1116
1564 			 1683 			 896
1430 			 1657 			 790
529 			 842 			 368
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 91
263 			 248 			 64
254 			 251 			 77
239 			 169 			 61
147 			 237 			 49
77 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [495/1000] took 97.67249155044556s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.286546622109933, train accuracy: 0.5392930178206252
Val mean loss: 1.7361400563542435, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2479 			 1998 			 1199
2110 			 2123 			 1133
2108 			 1966 			 1130
1640 			 1683 			 926
1402 			 1657 			 775
530 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 88
284 			 248 			 65
249 			 251 			 81
207 			 169 			 54
169 			 237 			 50
84 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [496/1000] took 97.0799732208252s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2847639922041016, train accuracy: 0.5408511052682832
Val mean loss: 1.748230896344999, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2447 			 1998 			 1208
2133 			 2123 			 1147
2118 			 1966 			 1132
1569 			 1683 			 905
1436 			 1657 			 781
566 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
348 			 263 			 102
294 			 248 			 67
214 			 251 			 70
227 			 169 			 58
135 			 237 			 42
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [497/1000] took 97.74580979347229s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.287750763685161, train accuracy: 0.5390008764241893
Val mean loss: 1.7076684847110655, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2514 			 1998 			 1222
2147 			 2123 			 1147
2069 			 1966 			 1110
1638 			 1683 			 922
1376 			 1657 			 769
525 			 842 			 365
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 85
299 			 248 			 66
265 			 251 			 78
190 			 169 			 50
173 			 237 			 54
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [498/1000] took 97.41327810287476s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2828078986699707, train accuracy: 0.5412406271301977
Val mean loss: 1.703952670097351, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2420 			 1998 			 1193
2207 			 2123 			 1173
2103 			 1966 			 1125
1572 			 1683 			 897
1412 			 1657 			 781
555 			 842 			 389
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 89
310 			 248 			 69
207 			 251 			 70
239 			 169 			 60
151 			 237 			 46
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846931968
Epoch [499/1000] took 96.77718043327332s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2808153293214484, train accuracy: 0.5396825396825397
Val mean loss: 1.73385889937238, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2475 			 1998 			 1194
2180 			 2123 			 1165
2065 			 1966 			 1120
1608 			 1683 			 909
1389 			 1657 			 778
552 			 842 			 376
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 94
239 			 248 			 59
232 			 251 			 75
251 			 169 			 64
172 			 237 			 56
67 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891628544
Epoch [500/1000] took 96.86213874816895s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2822367872033165, train accuracy: 0.5379296913039244
Val mean loss: 1.754860430229001, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2458 			 1998 			 1196
2120 			 2123 			 1140
2054 			 1966 			 1110
1693 			 1683 			 935
1391 			 1657 			 767
553 			 842 			 376
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
267 			 263 			 81
317 			 248 			 70
261 			 251 			 83
191 			 169 			 52
171 			 237 			 52
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [501/1000] took 97.60677671432495s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2845574342573172, train accuracy: 0.5409484857337618
Val mean loss: 1.7173524833307034, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2395 			 1998 			 1189
2142 			 2123 			 1168
2133 			 1966 			 1132
1595 			 1683 			 901
1482 			 1657 			 793
522 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 100
293 			 248 			 71
238 			 251 			 73
187 			 169 			 49
140 			 237 			 44
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [502/1000] took 97.5739176273346s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2841000809476384, train accuracy: 0.5433829973707275
Val mean loss: 1.7248074397808169, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2463 			 1998 			 1219
2153 			 2123 			 1156
2146 			 1966 			 1143
1609 			 1683 			 913
1343 			 1657 			 761
555 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 92
292 			 248 			 71
202 			 251 			 69
190 			 169 			 51
209 			 237 			 60
71 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [503/1000] took 97.84537243843079s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2840856614513931, train accuracy: 0.5421170513195053
Val mean loss: 1.7112406521308712, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2494 			 1998 			 1211
2177 			 2123 			 1171
2073 			 1966 			 1127
1560 			 1683 			 898
1413 			 1657 			 779
552 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 93
250 			 248 			 58
256 			 251 			 79
217 			 169 			 56
183 			 237 			 59
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [504/1000] took 97.67301034927368s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2857806229517097, train accuracy: 0.5398773006134969
Val mean loss: 1.7158287967123635, val accuracy: 0.2772585669781931

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2494 			 1998 			 1208
2152 			 2123 			 1154
2111 			 1966 			 1127
1599 			 1683 			 906
1395 			 1657 			 774
518 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
265 			 263 			 79
267 			 248 			 59
232 			 251 			 75
250 			 169 			 59
189 			 237 			 55
81 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [505/1000] took 97.2549569606781s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2828917718750665, train accuracy: 0.5415327685266336
Val mean loss: 1.7397439305375262, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1189
2193 			 2123 			 1173
2048 			 1966 			 1104
1638 			 1683 			 921
1433 			 1657 			 786
556 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
356 			 263 			 104
252 			 248 			 57
242 			 251 			 80
204 			 169 			 54
162 			 237 			 53
68 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846931968
Epoch [506/1000] took 97.78873610496521s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2832338141503734, train accuracy: 0.5419222903885481
Val mean loss: 1.7595111945780313, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2444 			 1998 			 1210
2128 			 2123 			 1155
2105 			 1966 			 1134
1598 			 1683 			 903
1458 			 1657 			 786
536 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 93
270 			 248 			 58
267 			 251 			 80
210 			 169 			 53
156 			 237 			 46
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [507/1000] took 97.48435759544373s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2830168521292855, train accuracy: 0.5404615834063686
Val mean loss: 1.7164699885903336, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1187
2233 			 2123 			 1186
2153 			 1966 			 1137
1555 			 1683 			 892
1396 			 1657 			 775
544 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 103
247 			 248 			 60
247 			 251 			 79
229 			 169 			 58
151 			 237 			 48
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [508/1000] took 97.53446054458618s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2813512918733734, train accuracy: 0.5418249099230694
Val mean loss: 1.7283442689151298, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2470 			 1998 			 1216
2153 			 2123 			 1159
2095 			 1966 			 1126
1614 			 1683 			 907
1403 			 1657 			 781
534 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 98
270 			 248 			 64
230 			 251 			 75
222 			 169 			 56
174 			 237 			 53
67 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [509/1000] took 97.17101669311523s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2815951536006274, train accuracy: 0.539098256889668
Val mean loss: 1.711886382684475, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2432 			 1998 			 1193
2202 			 2123 			 1160
2127 			 1966 			 1141
1590 			 1683 			 898
1361 			 1657 			 759
557 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 94
239 			 248 			 59
232 			 251 			 76
235 			 169 			 60
196 			 237 			 58
66 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [510/1000] took 97.5245509147644s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2839135453337078, train accuracy: 0.5426039536468984
Val mean loss: 1.7370496261410597, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2472 			 1998 			 1211
2107 			 2123 			 1148
2068 			 1966 			 1117
1621 			 1683 			 915
1468 			 1657 			 804
533 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 92
329 			 248 			 73
244 			 251 			 75
199 			 169 			 55
134 			 237 			 42
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [511/1000] took 97.34682774543762s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2818535946982672, train accuracy: 0.541143246664719
Val mean loss: 1.7299915086932298, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2567 			 1998 			 1245
2129 			 2123 			 1161
2073 			 1966 			 1105
1600 			 1683 			 902
1355 			 1657 			 766
545 			 842 			 378
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
288 			 263 			 90
342 			 248 			 74
239 			 251 			 75
183 			 169 			 50
149 			 237 			 47
83 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [512/1000] took 97.41472816467285s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2809425443875084, train accuracy: 0.5428960950433344
Val mean loss: 1.7523507984673106, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2416 			 1998 			 1194
2254 			 2123 			 1197
2102 			 1966 			 1127
1570 			 1683 			 901
1381 			 1657 			 771
546 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 98
274 			 248 			 65
245 			 251 			 80
217 			 169 			 57
158 			 237 			 48
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847685632
Epoch [513/1000] took 97.60694766044617s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2827304839345153, train accuracy: 0.5413380075956763
Val mean loss: 1.7271501930748545, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2495 			 1998 			 1224
2164 			 2123 			 1162
2093 			 1966 			 1127
1608 			 1683 			 906
1363 			 1657 			 759
546 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 87
271 			 248 			 60
232 			 251 			 73
226 			 169 			 57
196 			 237 			 59
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [514/1000] took 97.08181095123291s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.281396330517029, train accuracy: 0.5455253676112571
Val mean loss: 1.735566374732227, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2403 			 1998 			 1199
2175 			 2123 			 1180
2086 			 1966 			 1140
1597 			 1683 			 909
1446 			 1657 			 789
562 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 90
242 			 248 			 55
252 			 251 			 80
226 			 169 			 57
191 			 237 			 59
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [515/1000] took 97.20150995254517s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.280811575342933, train accuracy: 0.5451358457493427
Val mean loss: 1.728048513575298, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2427 			 1998 			 1212
2177 			 2123 			 1178
2105 			 1966 			 1132
1569 			 1683 			 899
1436 			 1657 			 799
555 			 842 			 378
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 88
240 			 248 			 58
259 			 251 			 80
248 			 169 			 59
181 			 237 			 56
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [516/1000] took 97.71409344673157s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2813086160618197, train accuracy: 0.5435777583016846
Val mean loss: 1.7409236838177937, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1217
2167 			 2123 			 1161
2118 			 1966 			 1139
1602 			 1683 			 908
1417 			 1657 			 783
519 			 842 			 374
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 88
277 			 248 			 66
222 			 251 			 75
240 			 169 			 60
168 			 237 			 50
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [517/1000] took 97.5942268371582s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2817181096270076, train accuracy: 0.541435388061155
Val mean loss: 1.7621470282717449, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2461 			 1998 			 1208
2125 			 2123 			 1148
2053 			 1966 			 1108
1663 			 1683 			 925
1398 			 1657 			 778
569 			 842 			 393
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 95
286 			 248 			 67
239 			 251 			 76
217 			 169 			 55
163 			 237 			 49
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [518/1000] took 97.60088634490967s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2814991882656965, train accuracy: 0.5447463238874282
Val mean loss: 1.738431750274286, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2460 			 1998 			 1214
2192 			 2123 			 1169
2085 			 1966 			 1126
1590 			 1683 			 913
1416 			 1657 			 797
526 			 842 			 375
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 91
263 			 248 			 59
270 			 251 			 84
223 			 169 			 57
158 			 237 			 47
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [519/1000] took 97.2632942199707s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2790260285231927, train accuracy: 0.5454279871457786
Val mean loss: 1.7574586461230022, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2442 			 1998 			 1212
2105 			 2123 			 1152
2171 			 1966 			 1159
1583 			 1683 			 903
1424 			 1657 			 795
544 			 842 			 380
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 96
298 			 248 			 68
226 			 251 			 74
226 			 169 			 57
144 			 237 			 43
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [520/1000] took 97.22613668441772s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2797729274565557, train accuracy: 0.5423118122504625
Val mean loss: 1.74021648779148, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2441 			 1998 			 1214
2188 			 2123 			 1172
2078 			 1966 			 1118
1603 			 1683 			 900
1404 			 1657 			 780
555 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 97
269 			 248 			 61
284 			 251 			 88
209 			 169 			 54
146 			 237 			 47
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [521/1000] took 97.44745707511902s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2798567971336507, train accuracy: 0.5457201285422144
Val mean loss: 1.7375407625989217, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2408 			 1998 			 1202
2141 			 2123 			 1170
2151 			 1966 			 1146
1596 			 1683 			 914
1414 			 1657 			 787
559 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 102
254 			 248 			 56
229 			 251 			 79
221 			 169 			 55
187 			 237 			 58
55 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3906997760
Epoch [522/1000] took 97.55167293548584s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2813083635312374, train accuracy: 0.5436751387671633
Val mean loss: 1.7198254422443668, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2486 			 1998 			 1222
2137 			 2123 			 1159
2078 			 1966 			 1123
1626 			 1683 			 917
1410 			 1657 			 779
532 			 842 			 383
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 87
278 			 248 			 65
218 			 251 			 74
218 			 169 			 58
191 			 237 			 56
84 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [523/1000] took 97.18003058433533s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2799025395206203, train accuracy: 0.5419222903885481
Val mean loss: 1.7347955005924875, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2403 			 1998 			 1183
2181 			 2123 			 1164
2114 			 1966 			 1134
1596 			 1683 			 908
1419 			 1657 			 789
556 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 103
292 			 248 			 67
214 			 251 			 70
224 			 169 			 57
152 			 237 			 49
66 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [524/1000] took 97.29973316192627s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.278539568092964, train accuracy: 0.542214431784984
Val mean loss: 1.7467292111094406, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2470 			 1998 			 1213
2197 			 2123 			 1162
2059 			 1966 			 1111
1631 			 1683 			 928
1369 			 1657 			 772
543 			 842 			 382
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 109
240 			 248 			 57
241 			 251 			 78
184 			 169 			 50
187 			 237 			 56
66 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [525/1000] took 97.09136199951172s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.279242641829256, train accuracy: 0.5413380075956763
Val mean loss: 1.7292755667756243, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2524 			 1998 			 1222
2172 			 2123 			 1165
2061 			 1966 			 1111
1545 			 1683 			 889
1417 			 1657 			 784
550 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 89
252 			 248 			 60
275 			 251 			 87
233 			 169 			 58
159 			 237 			 48
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [526/1000] took 97.11749076843262s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2794819290392867, train accuracy: 0.5435777583016846
Val mean loss: 1.7298744887840458, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1201
2137 			 2123 			 1146
2141 			 1966 			 1142
1611 			 1683 			 919
1416 			 1657 			 788
563 			 842 			 386
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 100
270 			 248 			 66
235 			 251 			 75
225 			 169 			 56
143 			 237 			 44
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [527/1000] took 97.13568830490112s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.279001827737624, train accuracy: 0.5413380075956763
Val mean loss: 1.7170395298701961, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2499 			 1998 			 1221
2220 			 2123 			 1184
2046 			 1966 			 1110
1635 			 1683 			 916
1330 			 1657 			 745
539 			 842 			 383
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
282 			 263 			 85
256 			 248 			 60
285 			 251 			 89
194 			 169 			 51
196 			 237 			 59
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892480512
Epoch [528/1000] took 97.64055347442627s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2782054391234092, train accuracy: 0.5437725192326419
Val mean loss: 1.7286416786472971, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2414 			 1998 			 1200
2148 			 2123 			 1165
2124 			 1966 			 1128
1596 			 1683 			 923
1448 			 1657 			 786
539 			 842 			 382
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 90
291 			 248 			 68
283 			 251 			 86
188 			 169 			 48
149 			 237 			 47
80 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [529/1000] took 97.2463104724884s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.278121586343581, train accuracy: 0.5446489434219496
Val mean loss: 1.7246198276194131, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2448 			 1998 			 1202
2243 			 2123 			 1190
2104 			 1966 			 1134
1558 			 1683 			 915
1361 			 1657 			 768
555 			 842 			 384
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 91
205 			 248 			 50
302 			 251 			 93
204 			 169 			 55
191 			 237 			 56
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [530/1000] took 97.27177309989929s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.278744716139226, train accuracy: 0.5445515629564709
Val mean loss: 1.7337298800305623, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2383 			 1998 			 1196
2125 			 2123 			 1151
2191 			 1966 			 1161
1568 			 1683 			 901
1457 			 1657 			 796
545 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
380 			 263 			 110
262 			 248 			 61
225 			 251 			 72
205 			 169 			 52
143 			 237 			 45
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [531/1000] took 97.37415385246277s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2776447939352826, train accuracy: 0.5423118122504625
Val mean loss: 1.726787276384307, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2485 			 1998 			 1216
2145 			 2123 			 1158
2088 			 1966 			 1122
1612 			 1683 			 914
1389 			 1657 			 780
550 			 842 			 379
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 87
274 			 248 			 64
266 			 251 			 82
205 			 169 			 52
174 			 237 			 56
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [532/1000] took 97.2129898071289s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2783602444553672, train accuracy: 0.5427013341123771
Val mean loss: 1.7204084774342978, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2466 			 1998 			 1211
2142 			 2123 			 1152
2133 			 1966 			 1146
1559 			 1683 			 895
1435 			 1657 			 797
534 			 842 			 372
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 93
298 			 248 			 70
233 			 251 			 71
209 			 169 			 54
165 			 237 			 50
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [533/1000] took 97.80664682388306s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2795322143771566, train accuracy: 0.5446489434219496
Val mean loss: 1.7247971674291098, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2428 			 1998 			 1208
2179 			 2123 			 1170
2121 			 1966 			 1141
1580 			 1683 			 906
1414 			 1657 			 789
547 			 842 			 379
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
355 			 263 			 107
270 			 248 			 62
218 			 251 			 74
208 			 169 			 53
160 			 237 			 51
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [534/1000] took 97.6257803440094s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2785515584678293, train accuracy: 0.5408511052682832
Val mean loss: 1.7532548468287399, val accuracy: 0.2780373831775701

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2519 			 1998 			 1229
2139 			 2123 			 1160
2152 			 1966 			 1136
1543 			 1683 			 885
1383 			 1657 			 766
533 			 842 			 378
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 87
295 			 248 			 64
203 			 251 			 67
231 			 169 			 59
177 			 237 			 51
81 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845752320
Epoch [535/1000] took 97.35667157173157s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2777400883930123, train accuracy: 0.5446489434219496
Val mean loss: 1.7189427061778744, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2465 			 1998 			 1218
2176 			 2123 			 1176
2073 			 1966 			 1126
1609 			 1683 			 918
1384 			 1657 			 771
562 			 842 			 384
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 89
314 			 248 			 71
210 			 251 			 71
220 			 169 			 58
164 			 237 			 48
78 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [536/1000] took 97.36011910438538s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2770596653501565, train accuracy: 0.5436751387671633
Val mean loss: 1.7215764231798125, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2457 			 1998 			 1210
2127 			 2123 			 1152
2106 			 1966 			 1137
1605 			 1683 			 919
1433 			 1657 			 787
541 			 842 			 378
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 93
313 			 248 			 72
212 			 251 			 71
224 			 169 			 59
162 			 237 			 50
63 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [537/1000] took 97.659916639328s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2767826987204152, train accuracy: 0.5464991722660434
Val mean loss: 1.7122418168114453, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1208
2201 			 2123 			 1184
2071 			 1966 			 1135
1588 			 1683 			 915
1434 			 1657 			 789
541 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 93
231 			 248 			 53
271 			 251 			 84
231 			 169 			 60
165 			 237 			 53
72 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [538/1000] took 96.81865382194519s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2738559899671797, train accuracy: 0.5478624987827442
Val mean loss: 1.7170096025234316, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1216
2121 			 2123 			 1156
2126 			 1966 			 1144
1613 			 1683 			 926
1423 			 1657 			 795
552 			 842 			 389
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
285 			 248 			 62
228 			 251 			 72
219 			 169 			 55
162 			 237 			 48
73 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [539/1000] took 97.2907931804657s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2757826033410997, train accuracy: 0.5462070308696075
Val mean loss: 1.7410535928679676, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2447 			 1998 			 1213
2165 			 2123 			 1173
2080 			 1966 			 1135
1637 			 1683 			 918
1392 			 1657 			 785
548 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 91
314 			 248 			 72
244 			 251 			 78
197 			 169 			 52
155 			 237 			 48
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [540/1000] took 96.96303629875183s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2744524569897637, train accuracy: 0.5458175090076931
Val mean loss: 1.7193217335677728, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2451 			 1998 			 1216
2148 			 2123 			 1173
2121 			 1966 			 1136
1586 			 1683 			 908
1411 			 1657 			 788
552 			 842 			 384
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 97
311 			 248 			 70
225 			 251 			 71
213 			 169 			 55
142 			 237 			 44
78 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [541/1000] took 97.45653414726257s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.274392004325011, train accuracy: 0.5455253676112571
Val mean loss: 1.7518331539340135, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2457 			 1998 			 1209
2203 			 2123 			 1179
2067 			 1966 			 1123
1595 			 1683 			 917
1403 			 1657 			 792
544 			 842 			 382
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
257 			 248 			 60
268 			 251 			 84
211 			 169 			 54
153 			 237 			 44
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [542/1000] took 96.88030791282654s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2743728547081399, train accuracy: 0.5453306066803
Val mean loss: 1.7442748517524906, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2394 			 1998 			 1198
2190 			 2123 			 1179
2084 			 1966 			 1126
1606 			 1683 			 913
1427 			 1657 			 787
568 			 842 			 397
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 98
272 			 248 			 60
256 			 251 			 81
216 			 169 			 55
146 			 237 			 45
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [543/1000] took 97.35597538948059s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2740562719719433, train accuracy: 0.5465965527315221
Val mean loss: 1.71778777750527, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2436 			 1998 			 1219
2159 			 2123 			 1174
2106 			 1966 			 1133
1633 			 1683 			 928
1396 			 1657 			 778
539 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 91
280 			 248 			 65
252 			 251 			 81
188 			 169 			 52
175 			 237 			 55
82 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892546048
Epoch [544/1000] took 97.3718192577362s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2746916786532536, train accuracy: 0.5463044113350862
Val mean loss: 1.721777930492308, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2490 			 1998 			 1230
2172 			 2123 			 1179
2100 			 1966 			 1129
1547 			 1683 			 893
1416 			 1657 			 791
544 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 102
243 			 248 			 58
249 			 251 			 80
226 			 169 			 55
163 			 237 			 51
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [545/1000] took 97.02711486816406s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2738297143457835, train accuracy: 0.5505891518161456
Val mean loss: 1.7199928208095272, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2468 			 1998 			 1229
2144 			 2123 			 1177
2052 			 1966 			 1127
1654 			 1683 			 947
1400 			 1657 			 787
551 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
275 			 263 			 84
249 			 248 			 57
301 			 251 			 91
201 			 169 			 49
183 			 237 			 55
75 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [546/1000] took 97.16341376304626s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2749462153681341, train accuracy: 0.5484467815756159
Val mean loss: 1.7129783717597402, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1200
2164 			 2123 			 1179
2089 			 1966 			 1131
1634 			 1683 			 931
1410 			 1657 			 792
571 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 89
240 			 248 			 57
283 			 251 			 92
200 			 169 			 52
198 			 237 			 58
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [547/1000] took 97.34086179733276s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2716560742565404, train accuracy: 0.5476677378517869
Val mean loss: 1.7341795810839025, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1219
2104 			 2123 			 1148
2126 			 1966 			 1141
1606 			 1683 			 922
1442 			 1657 			 800
557 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 98
291 			 248 			 68
263 			 251 			 84
179 			 169 			 49
163 			 237 			 50
60 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [548/1000] took 97.18236231803894s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2730942804865377, train accuracy: 0.5471808355243938
Val mean loss: 1.7172062862210158, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2473 			 1998 			 1233
2202 			 2123 			 1185
2052 			 1966 			 1122
1588 			 1683 			 913
1440 			 1657 			 800
514 			 842 			 366
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
274 			 263 			 86
272 			 248 			 65
296 			 251 			 91
216 			 169 			 55
147 			 237 			 45
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [549/1000] took 97.75511050224304s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2745233107578717, train accuracy: 0.5482520206446587
Val mean loss: 1.7253743729940274, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2426 			 1998 			 1210
2129 			 2123 			 1161
2157 			 1966 			 1155
1604 			 1683 			 922
1383 			 1657 			 789
570 			 842 			 393
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 85
294 			 248 			 63
221 			 251 			 74
228 			 169 			 59
174 			 237 			 53
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [550/1000] took 97.20419907569885s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2731845280463079, train accuracy: 0.5453306066803
Val mean loss: 1.7266564601805152, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1212
2203 			 2123 			 1181
2065 			 1966 			 1114
1610 			 1683 			 919
1415 			 1657 			 794
542 			 842 			 380
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 83
266 			 248 			 63
268 			 251 			 85
222 			 169 			 58
156 			 237 			 48
93 			 116 			 34
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [551/1000] took 97.3491096496582s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2740375698541184, train accuracy: 0.5444541824909923
Val mean loss: 1.7300058254381505, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2399 			 1998 			 1198
2136 			 2123 			 1152
2110 			 1966 			 1141
1581 			 1683 			 910
1468 			 1657 			 795
575 			 842 			 395
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 97
273 			 248 			 61
254 			 251 			 78
219 			 169 			 57
144 			 237 			 43
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [552/1000] took 97.55672955513s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2722833347840472, train accuracy: 0.5476677378517869
Val mean loss: 1.7074760314894886, val accuracy: 0.3014018691588785

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2403 			 1998 			 1198
2230 			 2123 			 1195
2099 			 1966 			 1145
1603 			 1683 			 923
1394 			 1657 			 782
540 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
342 			 263 			 107
235 			 248 			 58
263 			 251 			 83
216 			 169 			 58
157 			 237 			 52
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [553/1000] took 96.93883776664734s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2713613175900182, train accuracy: 0.5453306066803
Val mean loss: 1.7305883081947886, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2518 			 1998 			 1225
2134 			 2123 			 1166
2104 			 1966 			 1131
1583 			 1683 			 915
1397 			 1657 			 787
533 			 842 			 376
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 89
252 			 248 			 58
266 			 251 			 80
203 			 169 			 53
185 			 237 			 54
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846964736
Epoch [554/1000] took 97.59046483039856s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2746013727886283, train accuracy: 0.5460122699386503
Val mean loss: 1.7450800232770967, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2491 			 1998 			 1228
2125 			 2123 			 1149
2093 			 1966 			 1133
1593 			 1683 			 919
1396 			 1657 			 781
571 			 842 			 397
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 99
286 			 248 			 66
243 			 251 			 76
204 			 169 			 54
160 			 237 			 49
62 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [555/1000] took 96.89808344841003s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2716055031506073, train accuracy: 0.5485441620410946
Val mean loss: 1.7459996444423025, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2428 			 1998 			 1208
2195 			 2123 			 1185
2100 			 1966 			 1144
1560 			 1683 			 907
1441 			 1657 			 798
545 			 842 			 391
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 95
256 			 248 			 58
243 			 251 			 76
240 			 169 			 60
157 			 237 			 48
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [556/1000] took 97.34527969360352s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2716950493809591, train accuracy: 0.5452332262148213
Val mean loss: 1.7299856238248872, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2461 			 1998 			 1222
2172 			 2123 			 1166
2105 			 1966 			 1134
1626 			 1683 			 919
1349 			 1657 			 772
556 			 842 			 386
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 91
263 			 248 			 58
272 			 251 			 85
199 			 169 			 52
183 			 237 			 56
63 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [557/1000] took 97.26001334190369s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2719730354172418, train accuracy: 0.5510760541435388
Val mean loss: 1.7505417713304845, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2428 			 1998 			 1209
2109 			 2123 			 1174
2128 			 1966 			 1142
1571 			 1683 			 909
1475 			 1657 			 825
558 			 842 			 400
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 95
267 			 248 			 61
242 			 251 			 78
228 			 169 			 59
158 			 237 			 50
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846669824
Epoch [558/1000] took 97.51512956619263s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2739126455746708, train accuracy: 0.5465965527315221
Val mean loss: 1.7169460145438589, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2447 			 1998 			 1207
2171 			 2123 			 1176
2106 			 1966 			 1138
1605 			 1683 			 920
1392 			 1657 			 785
548 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 94
277 			 248 			 61
230 			 251 			 74
234 			 169 			 59
159 			 237 			 50
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [559/1000] took 97.19741201400757s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.270033316077473, train accuracy: 0.5485441620410946
Val mean loss: 1.734643462227612, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1211
2152 			 2123 			 1184
2082 			 1966 			 1136
1648 			 1683 			 936
1406 			 1657 			 790
536 			 842 			 376
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 97
264 			 248 			 60
221 			 251 			 73
225 			 169 			 56
176 			 237 			 53
67 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3891726848
Epoch [560/1000] took 97.05133128166199s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2707268749813425, train accuracy: 0.5467913136624794
Val mean loss: 1.7169447991906144, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2490 			 1998 			 1220
2214 			 2123 			 1193
2054 			 1966 			 1128
1571 			 1683 			 894
1388 			 1657 			 788
552 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 97
217 			 248 			 53
277 			 251 			 87
218 			 169 			 55
184 			 237 			 53
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3900608000
Epoch [561/1000] took 97.58159112930298s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2729531452291851, train accuracy: 0.5485441620410946
Val mean loss: 1.7332837232729283, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2435 			 1998 			 1210
2085 			 2123 			 1148
2092 			 1966 			 1147
1621 			 1683 			 929
1458 			 1657 			 796
578 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 91
319 			 248 			 72
220 			 251 			 70
230 			 169 			 59
154 			 237 			 48
64 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [562/1000] took 97.51132845878601s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.272585766152058, train accuracy: 0.5428960950433344
Val mean loss: 1.7274287590166417, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2441 			 1998 			 1200
2172 			 2123 			 1164
2105 			 1966 			 1142
1659 			 1683 			 929
1358 			 1657 			 767
534 			 842 			 373
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 88
276 			 248 			 63
240 			 251 			 79
206 			 169 			 54
200 			 237 			 56
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [563/1000] took 97.42906498908997s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.270795223868896, train accuracy: 0.5474729769208296
Val mean loss: 1.7352877913451776, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2387 			 1998 			 1207
2144 			 2123 			 1154
2109 			 1966 			 1137
1618 			 1683 			 924
1462 			 1657 			 810
549 			 842 			 390
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
345 			 263 			 103
257 			 248 			 63
246 			 251 			 76
206 			 169 			 52
155 			 237 			 48
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892578816
Epoch [564/1000] took 97.25540018081665s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2687080885019628, train accuracy: 0.5459148894731717
Val mean loss: 1.7596070272166555, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1208
2149 			 2123 			 1177
2158 			 1966 			 1152
1569 			 1683 			 911
1378 			 1657 			 764
569 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
277 			 263 			 84
308 			 248 			 69
199 			 251 			 68
246 			 169 			 62
176 			 237 			 52
78 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [565/1000] took 97.5862991809845s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2715624552278133, train accuracy: 0.5463044113350862
Val mean loss: 1.7453456302968466, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1201
2207 			 2123 			 1179
2017 			 1966 			 1106
1659 			 1683 			 930
1440 			 1657 			 806
553 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 85
310 			 248 			 71
271 			 251 			 81
202 			 169 			 51
141 			 237 			 44
66 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [566/1000] took 96.90906858444214s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2681830226446609, train accuracy: 0.5486415425065732
Val mean loss: 1.7230107522592313, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2435 			 1998 			 1211
2159 			 2123 			 1174
2175 			 1966 			 1165
1594 			 1683 			 926
1352 			 1657 			 764
554 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 89
295 			 248 			 66
229 			 251 			 74
204 			 169 			 53
189 			 237 			 58
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [567/1000] took 97.55322217941284s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2693073322468458, train accuracy: 0.5486415425065732
Val mean loss: 1.7272364628024217, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2469 			 1998 			 1227
2159 			 2123 			 1173
2087 			 1966 			 1133
1591 			 1683 			 915
1402 			 1657 			 790
561 			 842 			 396
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 95
270 			 248 			 62
237 			 251 			 75
202 			 169 			 51
194 			 237 			 57
76 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [568/1000] took 97.88116335868835s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2708948128319975, train accuracy: 0.549225825299445
Val mean loss: 1.7321472109817877, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2436 			 1998 			 1229
2158 			 2123 			 1172
2098 			 1966 			 1135
1597 			 1683 			 917
1408 			 1657 			 788
572 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 92
280 			 248 			 62
221 			 251 			 70
218 			 169 			 54
189 			 237 			 58
64 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [569/1000] took 97.33406043052673s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2705973622583526, train accuracy: 0.5464017918005648
Val mean loss: 1.7353532343375973, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2462 			 1998 			 1219
2125 			 2123 			 1165
2127 			 1966 			 1147
1572 			 1683 			 908
1438 			 1657 			 791
545 			 842 			 381
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 95
299 			 248 			 65
212 			 251 			 72
223 			 169 			 58
166 			 237 			 51
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892480512
Epoch [570/1000] took 97.3524694442749s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2687056820340616, train accuracy: 0.5501022494887525
Val mean loss: 1.7339928702610294, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2374 			 1998 			 1194
2153 			 2123 			 1182
2091 			 1966 			 1145
1650 			 1683 			 940
1434 			 1657 			 797
567 			 842 			 391
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
332 			 263 			 100
288 			 248 			 69
235 			 251 			 74
210 			 169 			 56
150 			 237 			 48
69 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [571/1000] took 97.22981882095337s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2681530162552808, train accuracy: 0.548933683903009
Val mean loss: 1.7383508042591373, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2429 			 1998 			 1219
2141 			 2123 			 1163
2107 			 1966 			 1150
1640 			 1683 			 932
1426 			 1657 			 796
526 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 101
270 			 248 			 62
238 			 251 			 74
214 			 169 			 54
146 			 237 			 47
80 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3847259648
Epoch [572/1000] took 97.43976664543152s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2686310313943763, train accuracy: 0.551270815074496
Val mean loss: 1.7401851764539393, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2415 			 1998 			 1216
2186 			 2123 			 1187
2110 			 1966 			 1138
1608 			 1683 			 929
1371 			 1657 			 784
579 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 106
257 			 248 			 57
236 			 251 			 77
224 			 169 			 55
157 			 237 			 50
57 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [573/1000] took 97.12700390815735s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2674555176886442, train accuracy: 0.5480572597137015
Val mean loss: 1.726908637256157, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2429 			 1998 			 1210
2121 			 2123 			 1159
2083 			 1966 			 1134
1618 			 1683 			 926
1456 			 1657 			 807
562 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 96
286 			 248 			 66
229 			 251 			 71
223 			 169 			 55
152 			 237 			 47
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [574/1000] took 97.07795786857605s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2674068736138744, train accuracy: 0.5446489434219496
Val mean loss: 1.7456871183907114, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2405 			 1998 			 1205
2230 			 2123 			 1190
2117 			 1966 			 1136
1578 			 1683 			 903
1402 			 1657 			 782
537 			 842 			 377
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
356 			 263 			 105
239 			 248 			 56
233 			 251 			 74
218 			 169 			 57
160 			 237 			 51
78 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [575/1000] took 97.55293774604797s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2684553419689524, train accuracy: 0.5463044113350862
Val mean loss: 1.7291630564666376, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2526 			 1998 			 1227
2144 			 2123 			 1169
2083 			 1966 			 1135
1606 			 1683 			 924
1371 			 1657 			 779
539 			 842 			 376
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 95
278 			 248 			 62
228 			 251 			 71
219 			 169 			 57
176 			 237 			 53
78 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [576/1000] took 97.47335267066956s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2682874288143027, train accuracy: 0.547375596455351
Val mean loss: 1.7397144654902017, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2411 			 1998 			 1200
2171 			 2123 			 1175
2089 			 1966 			 1135
1583 			 1683 			 918
1432 			 1657 			 794
583 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 89
258 			 248 			 59
250 			 251 			 73
219 			 169 			 56
188 			 237 			 58
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [577/1000] took 97.72373032569885s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2702075796335286, train accuracy: 0.5501996299542312
Val mean loss: 1.734646663433168, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2416 			 1998 			 1216
2187 			 2123 			 1188
2095 			 1966 			 1138
1614 			 1683 			 926
1400 			 1657 			 789
557 			 842 			 393
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 100
255 			 248 			 55
246 			 251 			 80
196 			 169 			 50
177 			 237 			 53
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847947776
Epoch [578/1000] took 97.67971801757812s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.268729797776243, train accuracy: 0.5493232057649236
Val mean loss: 1.7327205146231301, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2474 			 1998 			 1232
2153 			 2123 			 1173
2124 			 1966 			 1151
1551 			 1683 			 906
1424 			 1657 			 788
543 			 842 			 391
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 93
299 			 248 			 71
200 			 251 			 69
243 			 169 			 63
161 			 237 			 49
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [579/1000] took 97.24811172485352s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2656148519842796, train accuracy: 0.5503943908851884
Val mean loss: 1.7487558039223277, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2390 			 1998 			 1207
2128 			 2123 			 1167
2152 			 1966 			 1157
1624 			 1683 			 928
1413 			 1657 			 797
562 			 842 			 396
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
349 			 263 			 102
298 			 248 			 68
196 			 251 			 70
201 			 169 			 53
169 			 237 			 54
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [580/1000] took 97.55672407150269s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2656995035777583, train accuracy: 0.5499074885577953
Val mean loss: 1.7162408741509043, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2437 			 1998 			 1222
2203 			 2123 			 1190
2106 			 1966 			 1149
1555 			 1683 			 905
1423 			 1657 			 792
545 			 842 			 389
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 90
297 			 248 			 65
196 			 251 			 65
236 			 169 			 59
176 			 237 			 52
78 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [581/1000] took 97.71176815032959s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.267923186872607, train accuracy: 0.5522446197292823
Val mean loss: 1.741666846159028, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2453 			 1998 			 1224
2185 			 2123 			 1187
2018 			 1966 			 1130
1646 			 1683 			 945
1401 			 1657 			 790
566 			 842 			 395
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 94
259 			 248 			 60
245 			 251 			 79
228 			 169 			 58
175 			 237 			 55
64 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [582/1000] took 97.00259613990784s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2641737700622773, train accuracy: 0.5533158048495472
Val mean loss: 1.7227531962278413, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2391 			 1998 			 1224
2214 			 2123 			 1197
2067 			 1966 			 1142
1600 			 1683 			 926
1433 			 1657 			 798
564 			 842 			 395
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 92
231 			 248 			 52
288 			 251 			 87
203 			 169 			 49
191 			 237 			 56
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [583/1000] took 96.614413022995s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2645654966155317, train accuracy: 0.5508812932125815
Val mean loss: 1.7306319446098515, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1210
2102 			 2123 			 1164
2119 			 1966 			 1149
1601 			 1683 			 921
1492 			 1657 			 815
567 			 842 			 398
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 97
308 			 248 			 70
239 			 251 			 76
215 			 169 			 56
119 			 237 			 39
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [584/1000] took 97.55449676513672s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2664987326039703, train accuracy: 0.5509786736780602
Val mean loss: 1.733459958216039, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2411 			 1998 			 1209
2232 			 2123 			 1201
2085 			 1966 			 1141
1623 			 1683 			 929
1370 			 1657 			 790
548 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
379 			 263 			 108
209 			 248 			 52
270 			 251 			 87
195 			 169 			 50
167 			 237 			 52
64 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [585/1000] took 97.61234545707703s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.26693501780709, train accuracy: 0.5496153471613594
Val mean loss: 1.73316302532103, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2489 			 1998 			 1226
2125 			 2123 			 1165
2099 			 1966 			 1142
1567 			 1683 			 915
1439 			 1657 			 809
550 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 102
251 			 248 			 61
280 			 251 			 81
201 			 169 			 52
159 			 237 			 49
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [586/1000] took 97.15732097625732s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.265462842314414, train accuracy: 0.5470834550589152
Val mean loss: 1.742016629474919, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2448 			 1998 			 1215
2101 			 2123 			 1166
2168 			 1966 			 1155
1584 			 1683 			 910
1411 			 1657 			 788
557 			 842 			 384
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 92
283 			 248 			 68
220 			 251 			 71
237 			 169 			 60
169 			 237 			 51
66 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847685632
Epoch [587/1000] took 97.49841809272766s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2662227903942453, train accuracy: 0.550004869023274
Val mean loss: 1.7254306572239573, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1220
2127 			 2123 			 1165
2091 			 1966 			 1138
1641 			 1683 			 948
1429 			 1657 			 798
536 			 842 			 379
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 84
325 			 248 			 77
235 			 251 			 76
198 			 169 			 53
172 			 237 			 53
69 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3850536448
Epoch [588/1000] took 97.47661757469177s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.264922976865204, train accuracy: 0.5506865322816243
Val mean loss: 1.7188412416272048, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2375 			 1998 			 1197
2212 			 2123 			 1195
2054 			 1966 			 1111
1591 			 1683 			 925
1472 			 1657 			 824
565 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 103
237 			 248 			 55
270 			 251 			 84
230 			 169 			 58
142 			 237 			 45
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [589/1000] took 96.66289830207825s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2629504664293332, train accuracy: 0.5499074885577953
Val mean loss: 1.7519252678243125, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1217
2099 			 2123 			 1163
2100 			 1966 			 1133
1654 			 1683 			 944
1405 			 1657 			 796
565 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 92
301 			 248 			 69
245 			 251 			 76
199 			 169 			 51
158 			 237 			 47
67 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892610560
Epoch [590/1000] took 97.22185778617859s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2635142728918438, train accuracy: 0.5524393806602396
Val mean loss: 1.7238609848952875, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2438 			 1998 			 1226
2180 			 2123 			 1197
2144 			 1966 			 1164
1557 			 1683 			 907
1411 			 1657 			 796
539 			 842 			 383
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 89
279 			 248 			 63
263 			 251 			 79
212 			 169 			 53
153 			 237 			 46
82 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [591/1000] took 97.43983364105225s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2632978493550857, train accuracy: 0.5510760541435388
Val mean loss: 1.7355837443979776, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2458 			 1998 			 1227
2123 			 2123 			 1167
2110 			 1966 			 1149
1602 			 1683 			 919
1404 			 1657 			 797
572 			 842 			 400
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 88
299 			 248 			 68
262 			 251 			 81
204 			 169 			 52
161 			 237 			 49
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [592/1000] took 97.45934891700745s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2648286927154875, train accuracy: 0.550004869023274
Val mean loss: 1.7173516575883074, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2462 			 1998 			 1221
2154 			 2123 			 1177
2066 			 1966 			 1140
1649 			 1683 			 940
1390 			 1657 			 787
548 			 842 			 383
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 95
281 			 248 			 66
257 			 251 			 83
194 			 169 			 52
159 			 237 			 49
78 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [593/1000] took 97.32742047309875s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.264262496496658, train accuracy: 0.5487389229720518
Val mean loss: 1.735289230579283, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2484 			 1998 			 1230
2146 			 2123 			 1164
2075 			 1966 			 1125
1586 			 1683 			 920
1417 			 1657 			 800
561 			 842 			 396
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 90
290 			 248 			 65
276 			 251 			 83
204 			 169 			 51
148 			 237 			 45
71 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846866432
Epoch [594/1000] took 96.92932891845703s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2630062326092586, train accuracy: 0.5524393806602396
Val mean loss: 1.741198792690184, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2414 			 1998 			 1212
2205 			 2123 			 1191
2082 			 1966 			 1147
1604 			 1683 			 930
1396 			 1657 			 788
568 			 842 			 405
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 87
246 			 248 			 55
289 			 251 			 88
215 			 169 			 53
177 			 237 			 55
64 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [595/1000] took 97.50587058067322s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.264344190140008, train accuracy: 0.5516603369364106
Val mean loss: 1.7247096852558415, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2411 			 1998 			 1221
2170 			 2123 			 1178
2112 			 1966 			 1142
1626 			 1683 			 935
1416 			 1657 			 805
534 			 842 			 384
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 97
262 			 248 			 60
258 			 251 			 84
202 			 169 			 54
167 			 237 			 50
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [596/1000] took 97.05086517333984s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2628544585727086, train accuracy: 0.552049858798325
Val mean loss: 1.7406394103678262, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2471 			 1998 			 1220
2088 			 2123 			 1164
2164 			 1966 			 1169
1568 			 1683 			 921
1403 			 1657 			 792
575 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 98
278 			 248 			 64
213 			 251 			 73
250 			 169 			 64
153 			 237 			 46
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [597/1000] took 97.45609331130981s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2631122319497794, train accuracy: 0.5517577174018892
Val mean loss: 1.7419815819437912, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1232
2140 			 2123 			 1177
2040 			 1966 			 1129
1696 			 1683 			 954
1406 			 1657 			 791
542 			 842 			 383
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 95
289 			 248 			 69
264 			 251 			 82
175 			 169 			 47
173 			 237 			 54
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [598/1000] took 97.1375789642334s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2614143370096558, train accuracy: 0.5496153471613594
Val mean loss: 1.7404722876665069, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2438 			 1998 			 1215
2176 			 2123 			 1179
2124 			 1966 			 1149
1575 			 1683 			 919
1403 			 1657 			 796
553 			 842 			 386
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 101
245 			 248 			 59
255 			 251 			 79
224 			 169 			 55
154 			 237 			 46
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892218368
Epoch [599/1000] took 97.10378575325012s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2635648595952542, train accuracy: 0.5482520206446587
Val mean loss: 1.7506464661621466, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2488 			 1998 			 1222
2112 			 2123 			 1161
2047 			 1966 			 1130
1662 			 1683 			 942
1405 			 1657 			 781
555 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 95
278 			 248 			 63
264 			 251 			 82
185 			 169 			 50
168 			 237 			 52
82 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [600/1000] took 97.36316156387329s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2610143423080444, train accuracy: 0.551270815074496
Val mean loss: 1.7338278235458746, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2506 			 1998 			 1235
2153 			 2123 			 1186
2089 			 1966 			 1153
1542 			 1683 			 913
1422 			 1657 			 787
557 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
275 			 263 			 78
281 			 248 			 63
232 			 251 			 73
251 			 169 			 62
170 			 237 			 51
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [601/1000] took 97.61037731170654s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.262177132000433, train accuracy: 0.5499074885577953
Val mean loss: 1.7373550665087816, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2383 			 1998 			 1211
2181 			 2123 			 1182
2145 			 1966 			 1152
1574 			 1683 			 909
1430 			 1657 			 795
556 			 842 			 398
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 95
285 			 248 			 64
211 			 251 			 71
245 			 169 			 59
153 			 237 			 44
69 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [602/1000] took 97.16570210456848s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2618592197649947, train accuracy: 0.5539000876424189
Val mean loss: 1.7334873036640446, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1228
2163 			 2123 			 1193
2057 			 1966 			 1133
1650 			 1683 			 948
1430 			 1657 			 793
552 			 842 			 393
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 96
257 			 248 			 58
265 			 251 			 82
209 			 169 			 51
166 			 237 			 52
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892218368
Epoch [603/1000] took 97.31911063194275s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2606491726878275, train accuracy: 0.5504917713506671
Val mean loss: 1.7315599802063733, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2448 			 1998 			 1220
2105 			 2123 			 1171
2125 			 1966 			 1150
1607 			 1683 			 918
1422 			 1657 			 795
562 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 94
327 			 248 			 76
217 			 251 			 71
207 			 169 			 55
143 			 237 			 44
79 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3850012160
Epoch [604/1000] took 96.93002247810364s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.259732402188013, train accuracy: 0.5532184243840685
Val mean loss: 1.7345941415647181, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2412 			 1998 			 1218
2231 			 2123 			 1201
2064 			 1966 			 1131
1586 			 1683 			 927
1386 			 1657 			 794
590 			 842 			 410
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 95
276 			 248 			 62
224 			 251 			 71
219 			 169 			 56
175 			 237 			 52
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [605/1000] took 97.427565574646s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2616421808706266, train accuracy: 0.5540948485733762
Val mean loss: 1.7513678887995279, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2438 			 1998 			 1223
2117 			 2123 			 1168
2118 			 1966 			 1156
1597 			 1683 			 931
1441 			 1657 			 814
558 			 842 			 398
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 97
325 			 248 			 74
199 			 251 			 69
206 			 169 			 55
150 			 237 			 46
79 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846866432
Epoch [606/1000] took 97.0198016166687s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.261355489956627, train accuracy: 0.5513681955399746
Val mean loss: 1.7406377588830344, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2462 			 1998 			 1218
2181 			 2123 			 1188
2062 			 1966 			 1132
1591 			 1683 			 927
1416 			 1657 			 807
557 			 842 			 390
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 91
296 			 248 			 69
240 			 251 			 75
213 			 169 			 54
163 			 237 			 50
80 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [607/1000] took 97.46026039123535s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2617769137349828, train accuracy: 0.5506865322816243
Val mean loss: 1.7366367055148613, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2439 			 1998 			 1232
2155 			 2123 			 1169
2106 			 1966 			 1143
1617 			 1683 			 934
1389 			 1657 			 781
563 			 842 			 396
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 91
281 			 248 			 63
230 			 251 			 75
220 			 169 			 56
164 			 237 			 48
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [608/1000] took 96.86164498329163s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2601952797898621, train accuracy: 0.5545817509007693
Val mean loss: 1.7437016876732432, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2456 			 1998 			 1233
2187 			 2123 			 1200
2048 			 1966 			 1126
1587 			 1683 			 922
1427 			 1657 			 812
564 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 87
238 			 248 			 55
262 			 251 			 76
221 			 169 			 58
184 			 237 			 59
81 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [609/1000] took 96.47218251228333s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.260588166498321, train accuracy: 0.5529262829876327
Val mean loss: 1.7407888115906134, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1202
2100 			 2123 			 1161
2136 			 1966 			 1157
1650 			 1683 			 939
1463 			 1657 			 820
564 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
332 			 263 			 99
267 			 248 			 59
243 			 251 			 78
195 			 169 			 53
172 			 237 			 54
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3847783936
Epoch [610/1000] took 96.63683748245239s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2600403168119745, train accuracy: 0.5532184243840685
Val mean loss: 1.7444474726188473, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2466 			 1998 			 1227
2155 			 2123 			 1188
2089 			 1966 			 1152
1593 			 1683 			 929
1397 			 1657 			 786
569 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 95
284 			 248 			 65
242 			 251 			 80
209 			 169 			 53
167 			 237 			 50
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [611/1000] took 96.87681031227112s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2600997459851322, train accuracy: 0.552049858798325
Val mean loss: 1.734714258007887, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2448 			 1998 			 1225
2158 			 2123 			 1190
2111 			 1966 			 1148
1586 			 1683 			 924
1409 			 1657 			 797
557 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 88
282 			 248 			 65
237 			 251 			 76
239 			 169 			 59
167 			 237 			 52
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [612/1000] took 97.10321474075317s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2602913236692315, train accuracy: 0.5542896095043335
Val mean loss: 1.7511350643344041, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2431 			 1998 			 1221
2135 			 2123 			 1175
2092 			 1966 			 1142
1627 			 1683 			 942
1422 			 1657 			 812
562 			 842 			 400
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
280 			 263 			 84
317 			 248 			 72
222 			 251 			 70
205 			 169 			 50
184 			 237 			 54
76 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [613/1000] took 97.01296043395996s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2602550081003492, train accuracy: 0.5528289025221541
Val mean loss: 1.7164761874733903, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2389 			 1998 			 1214
2197 			 2123 			 1192
2110 			 1966 			 1146
1584 			 1683 			 929
1439 			 1657 			 804
550 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 97
258 			 248 			 59
283 			 251 			 86
196 			 169 			 50
164 			 237 			 50
84 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [614/1000] took 97.19184279441833s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2596450240069832, train accuracy: 0.5537053267114617
Val mean loss: 1.7330694518438199, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2386 			 1998 			 1211
2107 			 2123 			 1171
2149 			 1966 			 1163
1591 			 1683 			 920
1456 			 1657 			 814
580 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 92
310 			 248 			 69
221 			 251 			 70
228 			 169 			 59
162 			 237 			 48
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [615/1000] took 97.74059247970581s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.258817205547915, train accuracy: 0.552049858798325
Val mean loss: 1.7243200075335618, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1221
2214 			 2123 			 1187
2054 			 1966 			 1142
1612 			 1683 			 937
1445 			 1657 			 802
527 			 842 			 380
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 93
296 			 248 			 67
239 			 251 			 76
207 			 169 			 52
148 			 237 			 44
82 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [616/1000] took 97.41844344139099s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2604336109116814, train accuracy: 0.5480572597137015
Val mean loss: 1.732068518312966, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2419 			 1998 			 1203
2161 			 2123 			 1178
2125 			 1966 			 1147
1593 			 1683 			 916
1394 			 1657 			 792
577 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 91
293 			 248 			 71
244 			 251 			 80
199 			 169 			 54
162 			 237 			 51
80 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [617/1000] took 97.80014133453369s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2607541801030762, train accuracy: 0.5544843704352906
Val mean loss: 1.7246435037473353, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2404 			 1998 			 1221
2180 			 2123 			 1195
2050 			 1966 			 1132
1607 			 1683 			 935
1465 			 1657 			 815
563 			 842 			 396
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 91
289 			 248 			 65
283 			 251 			 87
197 			 169 			 49
138 			 237 			 41
83 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [618/1000] took 97.75971174240112s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.25835914663808, train accuracy: 0.5513681955399746
Val mean loss: 1.7379369328661662, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2407 			 1998 			 1213
2157 			 2123 			 1180
2174 			 1966 			 1169
1603 			 1683 			 924
1332 			 1657 			 765
596 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 101
241 			 248 			 58
242 			 251 			 76
198 			 169 			 54
195 			 237 			 57
72 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [619/1000] took 97.17997574806213s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2563849561311002, train accuracy: 0.5488363034375304
Val mean loss: 1.7671846093201056, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2457 			 1998 			 1221
2131 			 2123 			 1165
2044 			 1966 			 1128
1610 			 1683 			 929
1463 			 1657 			 801
564 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
277 			 263 			 85
263 			 248 			 63
285 			 251 			 87
193 			 169 			 50
184 			 237 			 58
82 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3802628608
Epoch [620/1000] took 97.51044464111328s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2563029089449351, train accuracy: 0.5556529360210342
Val mean loss: 1.7465252323848446, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1224
2142 			 2123 			 1175
2140 			 1966 			 1164
1556 			 1683 			 921
1440 			 1657 			 817
574 			 842 			 405
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 95
269 			 248 			 60
248 			 251 			 81
221 			 169 			 59
160 			 237 			 47
63 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [621/1000] took 97.84316349029541s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2590870250051267, train accuracy: 0.551270815074496
Val mean loss: 1.7478171906820157, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2447 			 1998 			 1230
2127 			 2123 			 1170
2119 			 1966 			 1151
1584 			 1683 			 919
1430 			 1657 			 800
562 			 842 			 391
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
279 			 248 			 63
229 			 251 			 75
231 			 169 			 59
155 			 237 			 47
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892447744
Epoch [622/1000] took 97.40333223342896s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2593832755014533, train accuracy: 0.5554581750900769
Val mean loss: 1.738355526110021, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1212
2164 			 2123 			 1185
2101 			 1966 			 1156
1636 			 1683 			 947
1430 			 1657 			 805
567 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 101
284 			 248 			 64
220 			 251 			 72
213 			 169 			 54
164 			 237 			 48
76 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3900608000
Epoch [623/1000] took 98.11221694946289s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2583869821928744, train accuracy: 0.5511734346090175
Val mean loss: 1.7312408656608769, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2466 			 1998 			 1230
2171 			 2123 			 1184
2079 			 1966 			 1133
1570 			 1683 			 907
1420 			 1657 			 807
563 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 95
270 			 248 			 64
267 			 251 			 82
212 			 169 			 53
151 			 237 			 45
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [624/1000] took 97.79642987251282s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2582193271393345, train accuracy: 0.5561398383484273
Val mean loss: 1.7386652870876034, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2441 			 1998 			 1232
2144 			 2123 			 1181
2135 			 1966 			 1165
1613 			 1683 			 936
1375 			 1657 			 791
561 			 842 			 406
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 96
268 			 248 			 60
251 			 251 			 79
201 			 169 			 54
193 			 237 			 57
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [625/1000] took 97.3602888584137s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2575619140889414, train accuracy: 0.5498101080923167
Val mean loss: 1.7363694615480376, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2452 			 1998 			 1221
2125 			 2123 			 1168
2062 			 1966 			 1125
1595 			 1683 			 920
1476 			 1657 			 818
559 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 97
290 			 248 			 67
246 			 251 			 78
212 			 169 			 53
145 			 237 			 47
72 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [626/1000] took 97.34928011894226s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2566628006759835, train accuracy: 0.5528289025221541
Val mean loss: 1.7504113331073667, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2411 			 1998 			 1221
2164 			 2123 			 1190
2120 			 1966 			 1149
1589 			 1683 			 923
1432 			 1657 			 799
553 			 842 			 395
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 95
251 			 248 			 56
267 			 251 			 82
222 			 169 			 56
155 			 237 			 48
82 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [627/1000] took 97.61894655227661s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.257213726964695, train accuracy: 0.556237218813906
Val mean loss: 1.74022074443538, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2408 			 1998 			 1217
2120 			 2123 			 1179
2120 			 1966 			 1156
1617 			 1683 			 944
1423 			 1657 			 809
581 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 92
316 			 248 			 75
197 			 251 			 68
219 			 169 			 57
168 			 237 			 50
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846735360
Epoch [628/1000] took 96.44237399101257s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.25630575362767, train accuracy: 0.5583795890544356
Val mean loss: 1.7397879914539616, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1229
2157 			 2123 			 1184
2058 			 1966 			 1142
1646 			 1683 			 960
1447 			 1657 			 815
560 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 97
300 			 248 			 68
251 			 251 			 78
187 			 169 			 50
158 			 237 			 51
76 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3850536448
Epoch [629/1000] took 96.75621557235718s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2573837400225465, train accuracy: 0.5517577174018892
Val mean loss: 1.7326578774103305, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2436 			 1998 			 1220
2177 			 2123 			 1188
2079 			 1966 			 1139
1591 			 1683 			 925
1446 			 1657 			 809
540 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 90
259 			 248 			 58
270 			 251 			 84
217 			 169 			 54
155 			 237 			 47
86 			 116 			 34
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [630/1000] took 96.8690459728241s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.256327396984041, train accuracy: 0.5540948485733762
Val mean loss: 1.7382989947388812, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2437 			 1998 			 1222
2165 			 2123 			 1194
2082 			 1966 			 1148
1610 			 1683 			 932
1400 			 1657 			 787
575 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
287 			 263 			 85
266 			 248 			 58
265 			 251 			 81
214 			 169 			 54
183 			 237 			 55
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3926953472
Epoch [631/1000] took 96.87489104270935s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2575502553461497, train accuracy: 0.5580874476579998
Val mean loss: 1.727148538682519, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2381 			 1998 			 1219
2202 			 2123 			 1212
2087 			 1966 			 1155
1632 			 1683 			 945
1415 			 1657 			 808
552 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 94
237 			 248 			 57
256 			 251 			 77
221 			 169 			 54
177 			 237 			 55
77 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [632/1000] took 96.26356649398804s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2539959165537469, train accuracy: 0.5560424578829487
Val mean loss: 1.7215170627687035, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1229
2131 			 2123 			 1176
2099 			 1966 			 1159
1586 			 1683 			 922
1453 			 1657 			 821
566 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 92
260 			 248 			 63
246 			 251 			 76
245 			 169 			 60
154 			 237 			 48
78 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891726848
Epoch [633/1000] took 96.28714561462402s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2538046417206619, train accuracy: 0.5540948485733762
Val mean loss: 1.7321635135790197, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1229
2159 			 2123 			 1180
2090 			 1966 			 1139
1611 			 1683 			 935
1399 			 1657 			 800
576 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 90
274 			 248 			 66
234 			 251 			 75
227 			 169 			 58
183 			 237 			 57
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [634/1000] took 96.96372985839844s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2562709671686, train accuracy: 0.5564319797448631
Val mean loss: 1.743162931465521, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2402 			 1998 			 1225
2152 			 2123 			 1183
2092 			 1966 			 1147
1618 			 1683 			 938
1432 			 1657 			 814
573 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 91
274 			 248 			 66
242 			 251 			 78
207 			 169 			 53
192 			 237 			 60
59 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [635/1000] took 96.70019340515137s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2563097983877236, train accuracy: 0.5539000876424189
Val mean loss: 1.765088537844216, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2449 			 1998 			 1241
2203 			 2123 			 1197
2085 			 1966 			 1142
1581 			 1683 			 928
1388 			 1657 			 782
563 			 842 			 398
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
268 			 263 			 80
253 			 248 			 58
253 			 251 			 77
232 			 169 			 58
203 			 237 			 59
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [636/1000] took 96.12689089775085s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.252868467030867, train accuracy: 0.5550686532281625
Val mean loss: 1.745439977180667, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2320 			 1998 			 1192
2213 			 2123 			 1200
2076 			 1966 			 1141
1607 			 1683 			 928
1483 			 1657 			 835
570 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 101
243 			 248 			 56
266 			 251 			 80
224 			 169 			 59
148 			 237 			 47
66 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [637/1000] took 96.79009079933167s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2539241178384823, train accuracy: 0.5547765118317266
Val mean loss: 1.7348368894763109, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2458 			 1998 			 1231
2108 			 2123 			 1161
2090 			 1966 			 1151
1637 			 1683 			 945
1420 			 1657 			 807
556 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 96
299 			 248 			 67
251 			 251 			 80
204 			 169 			 54
150 			 237 			 46
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [638/1000] took 96.64353060722351s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2543927980731953, train accuracy: 0.5519524783328464
Val mean loss: 1.7406840876835148, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2425 			 1998 			 1215
2225 			 2123 			 1208
2073 			 1966 			 1133
1585 			 1683 			 922
1411 			 1657 			 798
550 			 842 			 392
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 90
193 			 248 			 47
288 			 251 			 89
231 			 169 			 60
175 			 237 			 52
82 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [639/1000] took 96.83998346328735s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.254466032127725, train accuracy: 0.5544843704352906
Val mean loss: 1.7280188740753546, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2399 			 1998 			 1222
2061 			 2123 			 1151
2141 			 1966 			 1156
1671 			 1683 			 956
1432 			 1657 			 805
565 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 99
277 			 248 			 65
242 			 251 			 77
198 			 169 			 53
171 			 237 			 50
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892447744
Epoch [640/1000] took 96.8423523902893s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2533507525363816, train accuracy: 0.5571136430032135
Val mean loss: 1.7437726404608749, val accuracy: 0.29361370716510904

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2465 			 1998 			 1226
2125 			 2123 			 1184
2034 			 1966 			 1143
1622 			 1683 			 936
1452 			 1657 			 829
571 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 94
266 			 248 			 64
306 			 251 			 93
177 			 169 			 47
152 			 237 			 50
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [641/1000] took 97.42557191848755s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.256681103386983, train accuracy: 0.5552634141591197
Val mean loss: 1.7351742285053904, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1239
2140 			 2123 			 1172
2185 			 1966 			 1180
1581 			 1683 			 922
1370 			 1657 			 787
559 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 93
294 			 248 			 69
238 			 251 			 74
200 			 169 			 52
168 			 237 			 51
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891628544
Epoch [642/1000] took 96.76310873031616s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.253399627416676, train accuracy: 0.5550686532281625
Val mean loss: 1.7362388314270392, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2421 			 1998 			 1224
2149 			 2123 			 1177
2064 			 1966 			 1148
1631 			 1683 			 943
1422 			 1657 			 801
582 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 93
253 			 248 			 57
264 			 251 			 82
209 			 169 			 54
174 			 237 			 51
72 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [643/1000] took 96.85318160057068s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2534830111580846, train accuracy: 0.5558476969519914
Val mean loss: 1.7370497540729801, val accuracy: 0.2757009345794392

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2418 			 1998 			 1228
2111 			 2123 			 1178
2096 			 1966 			 1157
1600 			 1683 			 922
1486 			 1657 			 822
558 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
269 			 263 			 79
305 			 248 			 68
265 			 251 			 77
200 			 169 			 50
168 			 237 			 51
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892578816
Epoch [644/1000] took 97.1373679637909s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2548127469615402, train accuracy: 0.55594507741747
Val mean loss: 1.7388641136448557, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2386 			 1998 			 1214
2182 			 2123 			 1199
2115 			 1966 			 1167
1618 			 1683 			 943
1398 			 1657 			 786
570 			 842 			 400
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 90
275 			 248 			 63
238 			 251 			 78
207 			 169 			 55
187 			 237 			 58
67 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3928297984
Epoch [645/1000] took 96.74007058143616s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.254072334907508, train accuracy: 0.5575031648651281
Val mean loss: 1.7458319024341862, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2424 			 1998 			 1236
2149 			 2123 			 1191
2147 			 1966 			 1178
1572 			 1683 			 922
1413 			 1657 			 799
564 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
279 			 248 			 63
215 			 251 			 69
231 			 169 			 60
172 			 237 			 51
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [646/1000] took 97.41866946220398s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2563411985230966, train accuracy: 0.5581848281234785
Val mean loss: 1.729541615742009, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1230
2179 			 2123 			 1205
2057 			 1966 			 1143
1653 			 1683 			 952
1438 			 1657 			 807
554 			 842 			 395
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 94
256 			 248 			 61
255 			 251 			 80
204 			 169 			 52
190 			 237 			 57
69 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [647/1000] took 97.23254776000977s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2540814857987972, train accuracy: 0.5542896095043335
Val mean loss: 1.7408964285036412, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1227
2105 			 2123 			 1166
2108 			 1966 			 1156
1600 			 1683 			 933
1458 			 1657 			 817
552 			 842 			 393
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 97
292 			 248 			 63
234 			 251 			 73
202 			 169 			 52
153 			 237 			 47
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [648/1000] took 96.76606297492981s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.253446036410109, train accuracy: 0.5567241211412991
Val mean loss: 1.7509078688737822, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2403 			 1998 			 1216
2176 			 2123 			 1196
2072 			 1966 			 1142
1622 			 1683 			 943
1422 			 1657 			 810
574 			 842 			 410
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
354 			 263 			 106
283 			 248 			 67
245 			 251 			 74
185 			 169 			 49
152 			 237 			 49
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [649/1000] took 96.79096388816833s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.252784941241006, train accuracy: 0.555166033693641
Val mean loss: 1.735055123887411, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2415 			 1998 			 1222
2140 			 2123 			 1187
2129 			 1966 			 1163
1617 			 1683 			 933
1419 			 1657 			 802
549 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 96
275 			 248 			 62
245 			 251 			 77
194 			 169 			 49
163 			 237 			 48
86 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [650/1000] took 96.82303285598755s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2522579970018144, train accuracy: 0.5558476969519914
Val mean loss: 1.7812584900274508, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2415 			 1998 			 1228
2162 			 2123 			 1186
2111 			 1966 			 1151
1589 			 1683 			 922
1405 			 1657 			 805
587 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 101
332 			 248 			 74
192 			 251 			 65
212 			 169 			 55
142 			 237 			 45
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [651/1000] took 96.68656301498413s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2507061154300179, train accuracy: 0.5517577174018892
Val mean loss: 1.7369604924829996, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2480 			 1998 			 1230
2141 			 2123 			 1186
2043 			 1966 			 1125
1623 			 1683 			 933
1436 			 1657 			 803
546 			 842 			 389
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
287 			 263 			 85
284 			 248 			 64
276 			 251 			 82
209 			 169 			 53
159 			 237 			 48
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [652/1000] took 96.48913836479187s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2527089965677707, train accuracy: 0.557016262537735
Val mean loss: 1.7521239024836843, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2379 			 1998 			 1215
2203 			 2123 			 1199
2135 			 1966 			 1171
1553 			 1683 			 931
1419 			 1657 			 793
580 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 91
252 			 248 			 57
262 			 251 			 81
230 			 169 			 56
157 			 237 			 48
70 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [653/1000] took 96.46456050872803s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2537903074534882, train accuracy: 0.5545817509007693
Val mean loss: 1.7512404918670654, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2438 			 1998 			 1224
2136 			 2123 			 1184
2115 			 1966 			 1152
1599 			 1683 			 939
1430 			 1657 			 808
551 			 842 			 388
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 95
252 			 248 			 56
246 			 251 			 75
232 			 169 			 57
170 			 237 			 56
78 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [654/1000] took 96.83626794815063s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2515824026779223, train accuracy: 0.5530236634531113
Val mean loss: 1.7421314745414547, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2443 			 1998 			 1224
2067 			 2123 			 1152
2097 			 1966 			 1145
1637 			 1683 			 941
1446 			 1657 			 811
579 			 842 			 406
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 89
338 			 248 			 77
237 			 251 			 74
202 			 169 			 53
145 			 237 			 48
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [655/1000] took 96.93970036506653s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.253577916421623, train accuracy: 0.5552634141591197
Val mean loss: 1.7739332332843687, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2341 			 1998 			 1211
2213 			 2123 			 1210
2120 			 1966 			 1157
1609 			 1683 			 927
1411 			 1657 			 796
575 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 100
289 			 248 			 68
259 			 251 			 81
196 			 169 			 51
145 			 237 			 47
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [656/1000] took 96.65766334533691s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2495264518669462, train accuracy: 0.557016262537735
Val mean loss: 1.7332469079552628, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2498 			 1998 			 1244
2154 			 2123 			 1189
2081 			 1966 			 1150
1583 			 1683 			 930
1411 			 1657 			 813
542 			 842 			 394
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
273 			 263 			 79
262 			 248 			 60
249 			 251 			 81
228 			 169 			 59
193 			 237 			 58
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [657/1000] took 97.13379740715027s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2508481776231546, train accuracy: 0.5549712727626838
Val mean loss: 1.7413489585969506, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2359 			 1998 			 1205
2212 			 2123 			 1206
2071 			 1966 			 1149
1623 			 1683 			 934
1451 			 1657 			 816
553 			 842 			 389
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 96
259 			 248 			 62
241 			 251 			 77
228 			 169 			 59
161 			 237 			 51
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [658/1000] took 96.84486103057861s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2531904565211025, train accuracy: 0.5550686532281625
Val mean loss: 1.7400806386296341, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2451 			 1998 			 1230
2163 			 2123 			 1180
2095 			 1966 			 1149
1580 			 1683 			 930
1406 			 1657 			 809
574 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 90
268 			 248 			 62
258 			 251 			 79
227 			 169 			 56
157 			 237 			 47
84 			 116 			 34
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [659/1000] took 96.74920678138733s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2499939680470855, train accuracy: 0.5606193397604441
Val mean loss: 1.7379883411453991, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2333 			 1998 			 1206
2164 			 2123 			 1202
2078 			 1966 			 1155
1688 			 1683 			 964
1430 			 1657 			 817
576 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 99
276 			 248 			 61
258 			 251 			 80
196 			 169 			 51
157 			 237 			 49
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [660/1000] took 96.73500061035156s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2499911272637199, train accuracy: 0.5595481546401792
Val mean loss: 1.7474423298021642, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2433 			 1998 			 1230
2149 			 2123 			 1200
2081 			 1966 			 1154
1592 			 1683 			 938
1433 			 1657 			 816
581 			 842 			 408
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 95
259 			 248 			 63
249 			 251 			 77
218 			 169 			 58
159 			 237 			 47
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [661/1000] took 96.84823775291443s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.250714144602743, train accuracy: 0.5572110234686922
Val mean loss: 1.7491581585349105, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2430 			 1998 			 1233
2102 			 2123 			 1173
2130 			 1966 			 1173
1624 			 1683 			 943
1415 			 1657 			 800
568 			 842 			 400
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 97
281 			 248 			 68
238 			 251 			 74
207 			 169 			 54
145 			 237 			 46
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [662/1000] took 96.58282375335693s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2486487108970357, train accuracy: 0.5569188820722563
Val mean loss: 1.7460540271386868, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2437 			 1998 			 1226
2173 			 2123 			 1210
2073 			 1966 			 1151
1607 			 1683 			 934
1398 			 1657 			 793
581 			 842 			 405
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 95
252 			 248 			 58
238 			 251 			 78
220 			 169 			 58
184 			 237 			 53
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [663/1000] took 97.40567970275879s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2494856328979087, train accuracy: 0.5592560132437433
Val mean loss: 1.7428939138970725, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2474 			 1998 			 1249
2082 			 2123 			 1181
2067 			 1966 			 1158
1638 			 1683 			 954
1459 			 1657 			 812
549 			 842 			 389
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 87
288 			 248 			 64
287 			 251 			 88
193 			 169 			 52
156 			 237 			 47
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [664/1000] took 96.59126496315002s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2492942958605995, train accuracy: 0.5573084039341708
Val mean loss: 1.7369459373194998, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2385 			 1998 			 1228
2266 			 2123 			 1225
2130 			 1966 			 1163
1545 			 1683 			 917
1368 			 1657 			 785
575 			 842 			 405
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 94
217 			 248 			 49
279 			 251 			 88
218 			 169 			 57
180 			 237 			 56
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [665/1000] took 97.20628213882446s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.24966417888986, train accuracy: 0.5564319797448631
Val mean loss: 1.771205087987388, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2420 			 1998 			 1221
2061 			 2123 			 1165
2141 			 1966 			 1168
1601 			 1683 			 932
1489 			 1657 			 827
557 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 99
275 			 248 			 62
255 			 251 			 78
216 			 169 			 57
137 			 237 			 45
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [666/1000] took 96.45097017288208s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.249034144611002, train accuracy: 0.5571136430032135
Val mean loss: 1.7454815230718472, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1226
2146 			 2123 			 1189
2065 			 1966 			 1145
1632 			 1683 			 949
1438 			 1657 			 814
571 			 842 			 398
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 90
274 			 248 			 65
262 			 251 			 83
206 			 169 			 53
168 			 237 			 52
68 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [667/1000] took 96.9714846611023s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2498060817659087, train accuracy: 0.558282208588957
Val mean loss: 1.7528847455978394, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2462 			 1998 			 1234
2115 			 2123 			 1185
2109 			 1966 			 1162
1621 			 1683 			 945
1414 			 1657 			 808
548 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 86
292 			 248 			 67
242 			 251 			 75
202 			 169 			 50
187 			 237 			 56
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [668/1000] took 96.86438918113708s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.248983566077699, train accuracy: 0.5588664913818288
Val mean loss: 1.7265953785035668, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2349 			 1998 			 1201
2225 			 2123 			 1216
2040 			 1966 			 1145
1571 			 1683 			 934
1494 			 1657 			 834
590 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 94
237 			 248 			 56
288 			 251 			 90
221 			 169 			 58
148 			 237 			 48
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3852600832
Epoch [669/1000] took 96.8498842716217s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.249468230384161, train accuracy: 0.5555555555555556
Val mean loss: 1.7327993294087851, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2428 			 1998 			 1228
2151 			 2123 			 1180
2111 			 1966 			 1158
1629 			 1683 			 937
1399 			 1657 			 800
551 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 91
278 			 248 			 63
262 			 251 			 83
205 			 169 			 51
158 			 237 			 47
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3892283904
Epoch [670/1000] took 96.1209020614624s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2457503475875498, train accuracy: 0.5606193397604441
Val mean loss: 1.7672511833470042, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1223
2131 			 2123 			 1192
2133 			 1966 			 1171
1632 			 1683 			 957
1404 			 1657 			 806
568 			 842 			 408
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 86
264 			 248 			 61
251 			 251 			 75
197 			 169 			 52
209 			 237 			 60
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [671/1000] took 96.82617354393005s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2469121782207786, train accuracy: 0.559061252312786
Val mean loss: 1.7283276203202038, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2436 			 1998 			 1241
2129 			 2123 			 1183
2097 			 1966 			 1157
1605 			 1683 			 944
1439 			 1657 			 813
563 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 89
291 			 248 			 67
234 			 251 			 77
203 			 169 			 54
181 			 237 			 53
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [672/1000] took 97.12523198127747s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.248366167612165, train accuracy: 0.5585743499853929
Val mean loss: 1.747130777777695, val accuracy: 0.2967289719626168

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2400 			 1998 			 1228
2129 			 2123 			 1188
2118 			 1966 			 1163
1615 			 1683 			 944
1432 			 1657 			 807
575 			 842 			 406
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 97
275 			 248 			 66
238 			 251 			 78
211 			 169 			 56
168 			 237 			 53
72 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [673/1000] took 97.135577917099s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2486867004094466, train accuracy: 0.5552634141591197
Val mean loss: 1.7601732742495653, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2396 			 1998 			 1222
2226 			 2123 			 1210
2075 			 1966 			 1145
1599 			 1683 			 937
1425 			 1657 			 803
548 			 842 			 385
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 92
245 			 248 			 59
242 			 251 			 79
247 			 169 			 62
166 			 237 			 51
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892218368
Epoch [674/1000] took 97.07513236999512s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2473844941531387, train accuracy: 0.5600350569675723
Val mean loss: 1.767435140726043, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2426 			 1998 			 1240
2115 			 2123 			 1180
2133 			 1966 			 1169
1627 			 1683 			 950
1400 			 1657 			 802
568 			 842 			 410
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 86
262 			 248 			 58
250 			 251 			 80
205 			 169 			 51
201 			 237 			 58
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3800498688
Epoch [675/1000] took 96.63710522651672s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2456303151597115, train accuracy: 0.5591586327782647
Val mean loss: 1.774657534389961, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2465 			 1998 			 1237
2158 			 2123 			 1195
2076 			 1966 			 1152
1581 			 1683 			 945
1469 			 1657 			 826
520 			 842 			 387
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
273 			 263 			 84
259 			 248 			 58
258 			 251 			 80
224 			 169 			 55
178 			 237 			 56
92 			 116 			 35
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [676/1000] took 96.9950590133667s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2477760496911974, train accuracy: 0.5602298178985295
Val mean loss: 1.7655987216205131, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2329 			 1998 			 1212
2160 			 2123 			 1191
2082 			 1966 			 1153
1671 			 1683 			 964
1441 			 1657 			 818
586 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
348 			 263 			 104
298 			 248 			 70
218 			 251 			 73
182 			 169 			 50
162 			 237 			 48
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [677/1000] took 96.39999914169312s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2472692321394092, train accuracy: 0.5576005453306067
Val mean loss: 1.733346287797137, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2430 			 1998 			 1230
2143 			 2123 			 1181
2069 			 1966 			 1154
1608 			 1683 			 945
1450 			 1657 			 815
569 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 94
297 			 248 			 68
262 			 251 			 79
205 			 169 			 52
145 			 237 			 45
70 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891759616
Epoch [678/1000] took 96.14745497703552s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2456951130216367, train accuracy: 0.5576005453306067
Val mean loss: 1.7454550615171107, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2445 			 1998 			 1238
2187 			 2123 			 1202
2067 			 1966 			 1144
1595 			 1683 			 937
1396 			 1657 			 797
579 			 842 			 408
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 91
241 			 248 			 54
257 			 251 			 80
214 			 169 			 52
187 			 237 			 57
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [679/1000] took 96.30224919319153s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2439518821573703, train accuracy: 0.560132437433051
Val mean loss: 1.729446175621777, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2397 			 1998 			 1228
2135 			 2123 			 1189
2132 			 1966 			 1165
1601 			 1683 			 943
1447 			 1657 			 825
557 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 89
263 			 248 			 62
249 			 251 			 79
206 			 169 			 54
188 			 237 			 54
78 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [680/1000] took 96.80001258850098s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2465498610820354, train accuracy: 0.5611062420878372
Val mean loss: 1.7466391179619767, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2441 			 1998 			 1235
2153 			 2123 			 1185
2064 			 1966 			 1163
1604 			 1683 			 953
1430 			 1657 			 812
577 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
272 			 263 			 82
278 			 248 			 63
250 			 251 			 77
237 			 169 			 59
180 			 237 			 55
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [681/1000] took 96.916836977005s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2472827560805086, train accuracy: 0.5567241211412991
Val mean loss: 1.749805683042945, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1222
2174 			 2123 			 1178
2104 			 1966 			 1154
1624 			 1683 			 951
1432 			 1657 			 813
564 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 95
272 			 248 			 61
246 			 251 			 77
211 			 169 			 52
165 			 237 			 51
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [682/1000] took 96.6730854511261s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.244003671723363, train accuracy: 0.5576979257960853
Val mean loss: 1.7276252711691507, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2418 			 1998 			 1231
2188 			 2123 			 1203
2073 			 1966 			 1144
1596 			 1683 			 941
1435 			 1657 			 808
559 			 842 			 400
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 92
244 			 248 			 56
258 			 251 			 83
236 			 169 			 59
167 			 237 			 51
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [683/1000] took 97.23681950569153s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2474911946373937, train accuracy: 0.5606193397604441
Val mean loss: 1.7354243761155663, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2399 			 1998 			 1233
2108 			 2123 			 1182
2093 			 1966 			 1163
1650 			 1683 			 966
1442 			 1657 			 811
577 			 842 			 402
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 90
283 			 248 			 63
241 			 251 			 74
223 			 169 			 55
166 			 237 			 49
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3804758528
Epoch [684/1000] took 96.9729232788086s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2437706550705099, train accuracy: 0.5602298178985295
Val mean loss: 1.7539949388038822, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2375 			 1998 			 1224
2176 			 2123 			 1196
2130 			 1966 			 1172
1599 			 1683 			 934
1432 			 1657 			 823
557 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 109
240 			 248 			 58
242 			 251 			 80
216 			 169 			 55
155 			 237 			 48
65 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [685/1000] took 96.90864515304565s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2459259714664328, train accuracy: 0.5564319797448631
Val mean loss: 1.749653234714415, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2475 			 1998 			 1239
2145 			 2123 			 1185
2080 			 1966 			 1140
1593 			 1683 			 944
1405 			 1657 			 802
571 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 91
265 			 248 			 60
244 			 251 			 76
222 			 169 			 58
181 			 237 			 57
71 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [686/1000] took 96.91794323921204s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2451447189039901, train accuracy: 0.5596455351056578
Val mean loss: 1.734592202233105, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1218
2138 			 2123 			 1191
2098 			 1966 			 1160
1593 			 1683 			 939
1463 			 1657 			 826
584 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 97
250 			 248 			 56
234 			 251 			 73
227 			 169 			 59
187 			 237 			 58
68 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [687/1000] took 96.32044911384583s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.242634380346518, train accuracy: 0.5584769695199143
Val mean loss: 1.7763098914448807, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2389 			 1998 			 1213
2198 			 2123 			 1205
2114 			 1966 			 1168
1593 			 1683 			 938
1408 			 1657 			 806
567 			 842 			 405
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 99
245 			 248 			 57
262 			 251 			 82
213 			 169 			 57
165 			 237 			 53
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [688/1000] took 96.9350254535675s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2439959669410254, train accuracy: 0.5647093193105464
Val mean loss: 1.757312073940184, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2379 			 1998 			 1238
2098 			 2123 			 1181
2117 			 1966 			 1167
1624 			 1683 			 954
1461 			 1657 			 839
590 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
356 			 263 			 107
257 			 248 			 62
230 			 251 			 74
221 			 169 			 58
151 			 237 			 47
69 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [689/1000] took 96.86173915863037s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2436932382925276, train accuracy: 0.5602298178985295
Val mean loss: 1.74076296643513, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2470 			 1998 			 1252
2141 			 2123 			 1185
2047 			 1966 			 1143
1602 			 1683 			 944
1430 			 1657 			 822
579 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 82
258 			 248 			 58
258 			 251 			 78
236 			 169 			 59
182 			 237 			 56
71 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [690/1000] took 96.97732353210449s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2435477513390538, train accuracy: 0.5596455351056578
Val mean loss: 1.7510998394431136, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2419 			 1998 			 1227
2084 			 2123 			 1165
2143 			 1966 			 1175
1628 			 1683 			 963
1423 			 1657 			 813
572 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 94
318 			 248 			 72
216 			 251 			 71
204 			 169 			 53
165 			 237 			 50
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [691/1000] took 96.59813117980957s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2430949706897558, train accuracy: 0.5589638718473074
Val mean loss: 1.7403266982334415, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1232
2215 			 2123 			 1200
2050 			 1966 			 1141
1613 			 1683 			 949
1434 			 1657 			 813
569 			 842 			 405
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 95
237 			 248 			 55
275 			 251 			 84
213 			 169 			 55
172 			 237 			 54
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [692/1000] took 96.68417620658875s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.245513202431046, train accuracy: 0.5619826662771448
Val mean loss: 1.7852781167844447, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1233
2121 			 2123 			 1182
2120 			 1966 			 1175
1599 			 1683 			 950
1459 			 1657 			 830
577 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 92
288 			 248 			 63
231 			 251 			 73
231 			 169 			 55
153 			 237 			 48
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [693/1000] took 97.01808166503906s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.245345762771238, train accuracy: 0.559840296036615
Val mean loss: 1.7504533325753562, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2432 			 1998 			 1239
2164 			 2123 			 1190
2051 			 1966 			 1147
1634 			 1683 			 959
1424 			 1657 			 813
564 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 99
271 			 248 			 63
251 			 251 			 79
200 			 169 			 52
163 			 237 			 51
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [694/1000] took 96.55889058113098s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2423437436421711, train accuracy: 0.5626643295354952
Val mean loss: 1.7577705383300781, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2447 			 1998 			 1246
2151 			 2123 			 1202
2101 			 1966 			 1166
1594 			 1683 			 942
1400 			 1657 			 809
576 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 89
292 			 248 			 65
262 			 251 			 82
209 			 169 			 52
160 			 237 			 53
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [695/1000] took 96.4486927986145s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2420907016855163, train accuracy: 0.5596455351056578
Val mean loss: 1.733134734921339, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2332 			 1998 			 1214
2194 			 2123 			 1209
2130 			 1966 			 1162
1607 			 1683 			 947
1428 			 1657 			 808
578 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 92
257 			 248 			 57
242 			 251 			 73
212 			 169 			 51
185 			 237 			 57
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846800896
Epoch [696/1000] took 96.59971070289612s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2419931786453984, train accuracy: 0.560132437433051
Val mean loss: 1.7420335397487734, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2468 			 1998 			 1251
2117 			 2123 			 1180
2101 			 1966 			 1161
1553 			 1683 			 932
1453 			 1657 			 819
577 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 89
305 			 248 			 67
211 			 251 			 71
233 			 169 			 58
164 			 237 			 50
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [697/1000] took 96.53494811058044s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2422579842564474, train accuracy: 0.5633459927938456
Val mean loss: 1.727911617697739, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2355 			 1998 			 1222
2147 			 2123 			 1198
2066 			 1966 			 1161
1694 			 1683 			 973
1442 			 1657 			 830
565 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
345 			 263 			 104
285 			 248 			 64
238 			 251 			 79
189 			 169 			 51
159 			 237 			 47
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [698/1000] took 96.94814848899841s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2437260897360116, train accuracy: 0.5596455351056578
Val mean loss: 1.7582835976670428, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2455 			 1998 			 1243
2139 			 2123 			 1191
2112 			 1966 			 1162
1583 			 1683 			 938
1414 			 1657 			 805
566 			 842 			 408
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 90
291 			 248 			 67
256 			 251 			 78
209 			 169 			 53
163 			 237 			 48
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [699/1000] took 97.04305577278137s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2435304739764919, train accuracy: 0.5607167202259227
Val mean loss: 1.7482378628195785, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2358 			 1998 			 1210
2196 			 2123 			 1212
2111 			 1966 			 1166
1595 			 1683 			 947
1440 			 1657 			 820
569 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 106
279 			 248 			 62
223 			 251 			 71
206 			 169 			 53
159 			 237 			 46
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [700/1000] took 96.82168984413147s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.241512334421045, train accuracy: 0.5602298178985295
Val mean loss: 1.742488029526501, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2469 			 1998 			 1242
2081 			 2123 			 1180
2089 			 1966 			 1158
1620 			 1683 			 949
1419 			 1657 			 810
591 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 89
291 			 248 			 67
246 			 251 			 79
205 			 169 			 53
168 			 237 			 50
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [701/1000] took 96.254070520401s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2409247330787396, train accuracy: 0.5620800467426235
Val mean loss: 1.7423536195987608, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2370 			 1998 			 1229
2181 			 2123 			 1205
2117 			 1966 			 1172
1595 			 1683 			 942
1443 			 1657 			 818
563 			 842 			 406
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 92
251 			 248 			 59
226 			 251 			 74
232 			 169 			 57
186 			 237 			 54
80 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [702/1000] took 96.93447136878967s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2422456724621425, train accuracy: 0.5620800467426235
Val mean loss: 1.735364062030141, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2396 			 1998 			 1230
2144 			 2123 			 1186
2088 			 1966 			 1169
1620 			 1683 			 943
1424 			 1657 			 817
597 			 842 			 427
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 89
276 			 248 			 60
238 			 251 			 76
211 			 169 			 56
184 			 237 			 56
83 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [703/1000] took 96.73616766929626s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.24292272310762, train accuracy: 0.5619826662771448
Val mean loss: 1.7885908586222952, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2436 			 1998 			 1237
2095 			 2123 			 1181
2095 			 1966 			 1171
1618 			 1683 			 950
1457 			 1657 			 823
568 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 94
314 			 248 			 73
196 			 251 			 66
225 			 169 			 58
170 			 237 			 51
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [704/1000] took 96.31001472473145s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2393175913166035, train accuracy: 0.5622748076735807
Val mean loss: 1.7493597763340647, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2385 			 1998 			 1232
2196 			 2123 			 1212
2052 			 1966 			 1155
1599 			 1683 			 937
1457 			 1657 			 827
580 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 93
237 			 248 			 56
271 			 251 			 81
220 			 169 			 55
171 			 237 			 54
78 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [705/1000] took 96.70812487602234s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2394168087255175, train accuracy: 0.5656831239653326
Val mean loss: 1.756169179590737, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2387 			 1998 			 1234
2189 			 2123 			 1224
2091 			 1966 			 1167
1615 			 1683 			 957
1409 			 1657 			 811
578 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 92
273 			 248 			 64
220 			 251 			 72
229 			 169 			 58
171 			 237 			 54
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [706/1000] took 96.69246029853821s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2412228090369442, train accuracy: 0.563248612328367
Val mean loss: 1.7422428189254389, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2424 			 1998 			 1240
2153 			 2123 			 1202
2071 			 1966 			 1158
1594 			 1683 			 936
1443 			 1657 			 830
584 			 842 			 418
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 97
253 			 248 			 58
256 			 251 			 83
221 			 169 			 60
164 			 237 			 52
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [707/1000] took 97.01273918151855s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2432415262560979, train accuracy: 0.5608141006914013
Val mean loss: 1.748917893665593, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2443 			 1998 			 1236
2099 			 2123 			 1180
2151 			 1966 			 1183
1592 			 1683 			 941
1408 			 1657 			 811
576 			 842 			 408
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 91
329 			 248 			 73
205 			 251 			 69
235 			 169 			 60
155 			 237 			 46
60 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3800498688
Epoch [708/1000] took 97.08048629760742s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.241679581526284, train accuracy: 0.5580874476579998
Val mean loss: 1.7436538295048039, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2441 			 1998 			 1237
2183 			 2123 			 1196
2062 			 1966 			 1141
1613 			 1683 			 941
1409 			 1657 			 812
561 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
265 			 263 			 78
274 			 248 			 62
236 			 251 			 75
223 			 169 			 59
215 			 237 			 63
71 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3850012160
Epoch [709/1000] took 97.18799209594727s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2414572181360002, train accuracy: 0.564027656052196
Val mean loss: 1.7510578079921444, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2376 			 1998 			 1225
2118 			 2123 			 1193
2082 			 1966 			 1167
1645 			 1683 			 966
1470 			 1657 			 831
578 			 842 			 410
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 101
278 			 248 			 63
225 			 251 			 71
202 			 169 			 50
179 			 237 			 53
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [710/1000] took 97.01240491867065s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2404181686145865, train accuracy: 0.5613983834842731
Val mean loss: 1.7342261221350692, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2407 			 1998 			 1232
2163 			 2123 			 1210
2106 			 1966 			 1162
1608 			 1683 			 944
1419 			 1657 			 808
566 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 96
271 			 248 			 62
239 			 251 			 77
208 			 169 			 53
172 			 237 			 53
76 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [711/1000] took 96.93777966499329s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2392995281382884, train accuracy: 0.564027656052196
Val mean loss: 1.742620183200371, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2450 			 1998 			 1255
2123 			 2123 			 1198
2070 			 1966 			 1150
1610 			 1683 			 957
1441 			 1657 			 819
575 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 87
262 			 248 			 59
257 			 251 			 80
224 			 169 			 56
172 			 237 			 53
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [712/1000] took 96.56904816627502s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.238825820873831, train accuracy: 0.5611062420878372
Val mean loss: 1.7367314318331277, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2360 			 1998 			 1216
2178 			 2123 			 1198
2102 			 1966 			 1166
1638 			 1683 			 960
1415 			 1657 			 810
576 			 842 			 412
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 82
252 			 248 			 57
260 			 251 			 80
209 			 169 			 53
207 			 237 			 61
71 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [713/1000] took 96.69189620018005s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.238820627470997, train accuracy: 0.5625669490700166
Val mean loss: 1.737803973802706, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2400 			 1998 			 1225
2095 			 2123 			 1178
2077 			 1966 			 1164
1611 			 1683 			 946
1513 			 1657 			 849
573 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 82
318 			 248 			 72
244 			 251 			 76
214 			 169 			 55
150 			 237 			 49
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [714/1000] took 96.37738180160522s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2376909198419328, train accuracy: 0.5643197974486318
Val mean loss: 1.737874845179116, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2430 			 1998 			 1243
2169 			 2123 			 1211
2080 			 1966 			 1156
1609 			 1683 			 951
1407 			 1657 			 816
574 			 842 			 418
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
265 			 263 			 82
300 			 248 			 66
258 			 251 			 79
211 			 169 			 52
171 			 237 			 52
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892218368
Epoch [715/1000] took 96.61546850204468s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2377843550432508, train accuracy: 0.5652936021034181
Val mean loss: 1.7471034643126697, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2355 			 1998 			 1208
2177 			 2123 			 1216
2097 			 1966 			 1190
1617 			 1683 			 955
1433 			 1657 			 820
590 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 86
218 			 248 			 54
293 			 251 			 92
215 			 169 			 56
188 			 237 			 58
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [716/1000] took 96.37015628814697s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2383344056450318, train accuracy: 0.5630538513974097
Val mean loss: 1.757185473674681, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2360 			 1998 			 1209
2093 			 2123 			 1184
2127 			 1966 			 1179
1655 			 1683 			 964
1463 			 1657 			 835
571 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
366 			 263 			 108
270 			 248 			 63
225 			 251 			 75
192 			 169 			 46
148 			 237 			 47
83 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3891923456
Epoch [717/1000] took 96.75434923171997s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2389936138907698, train accuracy: 0.5618852858116662
Val mean loss: 1.7407131951029708, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2473 			 1998 			 1252
2114 			 2123 			 1188
2058 			 1966 			 1152
1624 			 1683 			 945
1433 			 1657 			 824
567 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 90
293 			 248 			 67
253 			 251 			 77
195 			 169 			 51
163 			 237 			 50
85 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [718/1000] took 97.46687865257263s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2402892712492066, train accuracy: 0.562177427208102
Val mean loss: 1.7320473543027552, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2385 			 1998 			 1230
2211 			 2123 			 1221
2064 			 1966 			 1148
1578 			 1683 			 937
1455 			 1657 			 826
576 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 98
276 			 248 			 64
248 			 251 			 75
218 			 169 			 54
137 			 237 			 43
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [719/1000] took 96.87156534194946s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2382046119817691, train accuracy: 0.5609114811568799
Val mean loss: 1.7416442283769933, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1236
2119 			 2123 			 1189
2120 			 1966 			 1162
1622 			 1683 			 955
1379 			 1657 			 798
583 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 91
307 			 248 			 67
218 			 251 			 71
208 			 169 			 51
181 			 237 			 55
60 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [720/1000] took 96.8660831451416s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2413573341206228, train accuracy: 0.5600350569675723
Val mean loss: 1.7457888766032894, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2389 			 1998 			 1230
2158 			 2123 			 1193
2061 			 1966 			 1150
1632 			 1683 			 958
1468 			 1657 			 819
561 			 842 			 401
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 98
277 			 248 			 61
230 			 251 			 68
225 			 169 			 56
155 			 237 			 46
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [721/1000] took 97.2947690486908s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2397706883718662, train accuracy: 0.5613983834842731
Val mean loss: 1.7772781180172432, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2407 			 1998 			 1228
2145 			 2123 			 1191
2091 			 1966 			 1163
1635 			 1683 			 955
1427 			 1657 			 824
564 			 842 			 404
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 102
293 			 248 			 69
239 			 251 			 78
188 			 169 			 50
147 			 237 			 43
71 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [722/1000] took 96.32689905166626s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2378678353404702, train accuracy: 0.5600350569675723
Val mean loss: 1.770944851200755, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2422 			 1998 			 1229
2195 			 2123 			 1209
2095 			 1966 			 1165
1583 			 1683 			 942
1397 			 1657 			 795
577 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 104
248 			 248 			 57
217 			 251 			 72
232 			 169 			 60
172 			 237 			 53
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [723/1000] took 96.77953052520752s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2372322630288073, train accuracy: 0.5646119388450677
Val mean loss: 1.778078151912224, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2414 			 1998 			 1247
2124 			 2123 			 1193
2072 			 1966 			 1158
1611 			 1683 			 951
1480 			 1657 			 840
568 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 97
309 			 248 			 69
231 			 251 			 71
216 			 169 			 54
130 			 237 			 43
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [724/1000] took 96.36613655090332s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.236807196496803, train accuracy: 0.5650014607069822
Val mean loss: 1.7486155411092246, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2410 			 1998 			 1253
2167 			 2123 			 1195
2044 			 1966 			 1154
1633 			 1683 			 962
1426 			 1657 			 817
589 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 92
274 			 248 			 63
265 			 251 			 79
212 			 169 			 54
146 			 237 			 46
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [725/1000] took 97.14770221710205s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2383004079726627, train accuracy: 0.5622748076735807
Val mean loss: 1.7808145168350964, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2404 			 1998 			 1229
2188 			 2123 			 1218
2108 			 1966 			 1172
1608 			 1683 			 947
1407 			 1657 			 805
554 			 842 			 403
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 92
260 			 248 			 59
251 			 251 			 80
204 			 169 			 49
179 			 237 			 54
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [726/1000] took 97.01050066947937s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2393745267502616, train accuracy: 0.5639302755867173
Val mean loss: 1.7406905569681308, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1222
2109 			 2123 			 1182
2075 			 1966 			 1172
1647 			 1683 			 959
1480 			 1657 			 832
587 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 99
276 			 248 			 63
256 			 251 			 81
202 			 169 			 52
146 			 237 			 45
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [727/1000] took 96.5060658454895s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2348510006506495, train accuracy: 0.564027656052196
Val mean loss: 1.7475889688584862, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2427 			 1998 			 1239
2140 			 2123 			 1187
2096 			 1966 			 1176
1642 			 1683 			 966
1394 			 1657 			 811
570 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
276 			 263 			 80
278 			 248 			 64
261 			 251 			 79
204 			 169 			 52
188 			 237 			 55
77 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [728/1000] took 96.81202697753906s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2359351094266706, train accuracy: 0.5650988411724608
Val mean loss: 1.7285528793567564, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2352 			 1998 			 1227
2228 			 2123 			 1230
2099 			 1966 			 1162
1569 			 1683 			 941
1444 			 1657 			 830
577 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 105
236 			 248 			 56
243 			 251 			 78
218 			 169 			 56
172 			 237 			 51
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [729/1000] took 96.83834624290466s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2372352544021012, train accuracy: 0.5653909825688966
Val mean loss: 1.7435186578006279, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2460 			 1998 			 1258
2090 			 2123 			 1182
2094 			 1966 			 1168
1643 			 1683 			 961
1414 			 1657 			 825
568 			 842 			 412
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
288 			 263 			 86
278 			 248 			 64
261 			 251 			 81
206 			 169 			 51
178 			 237 			 53
73 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [730/1000] took 96.46144342422485s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.236392415015497, train accuracy: 0.5637355146557601
Val mean loss: 1.750317989326105, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2373 			 1998 			 1230
2157 			 2123 			 1201
2074 			 1966 			 1152
1613 			 1683 			 952
1477 			 1657 			 837
575 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 88
311 			 248 			 73
261 			 251 			 79
205 			 169 			 51
140 			 237 			 45
83 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [731/1000] took 96.73621940612793s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2366548366264392, train accuracy: 0.5639302755867173
Val mean loss: 1.7545308950470715, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2364 			 1998 			 1219
2192 			 2123 			 1210
2075 			 1966 			 1159
1676 			 1683 			 972
1388 			 1657 			 816
574 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 96
264 			 248 			 61
240 			 251 			 77
195 			 169 			 52
189 			 237 			 56
67 			 116 			 24
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [732/1000] took 96.72126793861389s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2363995027690662, train accuracy: 0.5627617100009739
Val mean loss: 1.7535014704960148, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2400 			 1998 			 1225
2154 			 2123 			 1201
2108 			 1966 			 1164
1587 			 1683 			 944
1436 			 1657 			 829
584 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
307 			 263 			 89
281 			 248 			 65
212 			 251 			 71
227 			 169 			 59
185 			 237 			 54
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [733/1000] took 96.8412435054779s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2353446723144745, train accuracy: 0.5647093193105464
Val mean loss: 1.7591550989848812, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2408 			 1998 			 1242
2118 			 2123 			 1193
2118 			 1966 			 1172
1620 			 1683 			 949
1429 			 1657 			 828
576 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 86
296 			 248 			 69
214 			 251 			 69
221 			 169 			 56
186 			 237 			 58
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [734/1000] took 97.32276177406311s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2387672601459183, train accuracy: 0.5634433732593241
Val mean loss: 1.7851329634829265, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1226
2173 			 2123 			 1207
2055 			 1966 			 1156
1643 			 1683 			 959
1447 			 1657 			 825
580 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
257 			 248 			 58
264 			 251 			 84
219 			 169 			 56
156 			 237 			 49
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891825152
Epoch [735/1000] took 96.9400839805603s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2368299723414247, train accuracy: 0.5617879053461875
Val mean loss: 1.7610824805934255, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2440 			 1998 			 1245
2163 			 2123 			 1200
2096 			 1966 			 1161
1581 			 1683 			 939
1429 			 1657 			 815
560 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 94
256 			 248 			 61
249 			 251 			 83
221 			 169 			 58
174 			 237 			 53
70 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [736/1000] took 96.57962012290955s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2349572051722684, train accuracy: 0.563248612328367
Val mean loss: 1.7569166974323551, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2369 			 1998 			 1223
2217 			 2123 			 1221
2058 			 1966 			 1155
1613 			 1683 			 961
1444 			 1657 			 817
568 			 842 			 407
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 95
229 			 248 			 55
258 			 251 			 81
230 			 169 			 57
171 			 237 			 52
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [737/1000] took 97.15083575248718s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2372815137340272, train accuracy: 0.5647093193105464
Val mean loss: 1.7651229689760906, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1240
2076 			 2123 			 1178
2129 			 1966 			 1179
1645 			 1683 			 963
1426 			 1657 			 823
576 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 97
310 			 248 			 72
225 			 251 			 75
209 			 169 			 54
156 			 237 			 46
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [738/1000] took 97.36111211776733s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2360092965984641, train accuracy: 0.5641250365176745
Val mean loss: 1.7414389179974068, val accuracy: 0.29517133956386293

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2412 			 1998 			 1233
2147 			 2123 			 1197
2075 			 1966 			 1172
1612 			 1683 			 948
1439 			 1657 			 819
584 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 97
286 			 248 			 69
258 			 251 			 84
207 			 169 			 52
147 			 237 			 47
78 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [739/1000] took 96.92043709754944s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.235858311348615, train accuracy: 0.5641250365176745
Val mean loss: 1.756627670148524, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2389 			 1998 			 1236
2177 			 2123 			 1210
2104 			 1966 			 1175
1582 			 1683 			 947
1425 			 1657 			 808
592 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 90
293 			 248 			 64
200 			 251 			 69
245 			 169 			 61
162 			 237 			 51
70 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [740/1000] took 96.51296830177307s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2324046082214404, train accuracy: 0.5651962216379395
Val mean loss: 1.7612305152706984, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2480 			 1998 			 1258
2102 			 2123 			 1196
2058 			 1966 			 1154
1624 			 1683 			 962
1440 			 1657 			 826
565 			 842 			 408
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
265 			 263 			 82
342 			 248 			 76
225 			 251 			 75
211 			 169 			 55
162 			 237 			 50
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892610560
Epoch [741/1000] took 96.66596913337708s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2330964753560931, train accuracy: 0.5649040802415035
Val mean loss: 1.7615368889599312, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2381 			 1998 			 1227
2125 			 2123 			 1196
2101 			 1966 			 1171
1624 			 1683 			 954
1456 			 1657 			 834
582 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 88
331 			 248 			 76
220 			 251 			 73
215 			 169 			 54
151 			 237 			 48
76 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [742/1000] took 97.37853574752808s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2356621247957058, train accuracy: 0.5638328951212387
Val mean loss: 1.7527463290749528, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2352 			 1998 			 1222
2200 			 2123 			 1212
2055 			 1966 			 1160
1619 			 1683 			 957
1459 			 1657 			 823
584 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 98
245 			 248 			 55
263 			 251 			 82
216 			 169 			 57
164 			 237 			 48
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [743/1000] took 97.09637975692749s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2322833506489097, train accuracy: 0.567143830947512
Val mean loss: 1.746341231392651, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2368 			 1998 			 1235
2163 			 2123 			 1217
2103 			 1966 			 1174
1633 			 1683 			 958
1421 			 1657 			 819
581 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
347 			 263 			 105
246 			 248 			 55
240 			 251 			 80
221 			 169 			 57
162 			 237 			 49
68 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [744/1000] took 97.30638813972473s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2335421066046504, train accuracy: 0.566851689551076
Val mean loss: 1.7758748037059133, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2439 			 1998 			 1245
2159 			 2123 			 1209
2035 			 1966 			 1159
1601 			 1683 			 956
1463 			 1657 			 838
572 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 93
253 			 248 			 60
293 			 251 			 90
218 			 169 			 54
146 			 237 			 46
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [745/1000] took 97.24418950080872s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2335630434324436, train accuracy: 0.5634433732593241
Val mean loss: 1.7452750235069088, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2405 			 1998 			 1233
2121 			 2123 			 1202
2174 			 1966 			 1198
1616 			 1683 			 947
1393 			 1657 			 807
560 			 842 			 399
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 84
283 			 248 			 65
227 			 251 			 72
218 			 169 			 52
184 			 237 			 54
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846866432
Epoch [746/1000] took 97.162428855896s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2317695229595695, train accuracy: 0.5662674067582043
Val mean loss: 1.7403762892978947, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2347 			 1998 			 1221
2127 			 2123 			 1199
2115 			 1966 			 1172
1663 			 1683 			 977
1430 			 1657 			 825
587 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 97
333 			 248 			 77
191 			 251 			 65
198 			 169 			 55
162 			 237 			 49
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [747/1000] took 96.84838938713074s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2314888012372074, train accuracy: 0.564027656052196
Val mean loss: 1.7770969867706299, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2405 			 1998 			 1227
2160 			 2123 			 1205
2043 			 1966 			 1153
1587 			 1683 			 947
1472 			 1657 			 837
602 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 95
278 			 248 			 61
231 			 251 			 73
232 			 169 			 56
159 			 237 			 48
60 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [748/1000] took 96.82198023796082s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2326001898150578, train accuracy: 0.5663647872236829
Val mean loss: 1.7650586163125388, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2364 			 1998 			 1232
2140 			 2123 			 1205
2092 			 1966 			 1175
1650 			 1683 			 970
1442 			 1657 			 819
581 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 94
269 			 248 			 62
233 			 251 			 76
238 			 169 			 60
151 			 237 			 49
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3892513280
Epoch [749/1000] took 96.6941487789154s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2297048089660216, train accuracy: 0.5616905248807089
Val mean loss: 1.7550346386141893, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2426 			 1998 			 1234
2111 			 2123 			 1183
2071 			 1966 			 1157
1648 			 1683 			 964
1432 			 1657 			 816
581 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 89
294 			 248 			 64
249 			 251 			 77
194 			 169 			 51
167 			 237 			 50
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [750/1000] took 97.26688718795776s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2332213615702692, train accuracy: 0.5685071574642127
Val mean loss: 1.7418596700924198, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2372 			 1998 			 1237
2216 			 2123 			 1233
2074 			 1966 			 1170
1584 			 1683 			 953
1455 			 1657 			 832
568 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 94
249 			 248 			 58
256 			 251 			 81
226 			 169 			 57
160 			 237 			 48
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [751/1000] took 96.92424368858337s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2311117663933109, train accuracy: 0.5626643295354952
Val mean loss: 1.7558582904862194, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2369 			 1998 			 1225
2141 			 2123 			 1194
2077 			 1966 			 1164
1648 			 1683 			 953
1441 			 1657 			 819
593 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
267 			 248 			 60
257 			 251 			 78
216 			 169 			 55
154 			 237 			 48
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [752/1000] took 97.1527042388916s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2328275109748603, train accuracy: 0.5661700262927257
Val mean loss: 1.762299441709751, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2375 			 1998 			 1217
2130 			 2123 			 1205
2121 			 1966 			 1180
1607 			 1683 			 960
1450 			 1657 			 837
586 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
302 			 248 			 69
217 			 251 			 70
217 			 169 			 54
154 			 237 			 48
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [753/1000] took 96.44312453269958s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2311678742322596, train accuracy: 0.5677281137403837
Val mean loss: 1.757889160295812, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2355 			 1998 			 1236
2178 			 2123 			 1210
2089 			 1966 			 1172
1630 			 1683 			 964
1431 			 1657 			 826
586 			 842 			 422
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 97
281 			 248 			 68
233 			 251 			 76
200 			 169 			 52
163 			 237 			 50
76 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [754/1000] took 97.02106642723083s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2327027746078754, train accuracy: 0.567922874671341
Val mean loss: 1.7483836121675445, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2385 			 1998 			 1239
2157 			 2123 			 1212
2067 			 1966 			 1165
1672 			 1683 			 983
1415 			 1657 			 820
573 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 99
264 			 248 			 59
255 			 251 			 79
185 			 169 			 49
179 			 237 			 50
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [755/1000] took 96.9385232925415s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2304536633402388, train accuracy: 0.562956470931931
Val mean loss: 1.748087310209507, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2481 			 1998 			 1252
2118 			 2123 			 1191
2106 			 1966 			 1176
1559 			 1683 			 929
1419 			 1657 			 812
586 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 97
262 			 248 			 59
236 			 251 			 74
221 			 169 			 57
178 			 237 			 54
62 			 116 			 25
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [756/1000] took 97.2575113773346s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2334936246322323, train accuracy: 0.5669490700165547
Val mean loss: 1.7359066183974103, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2399 			 1998 			 1238
2128 			 2123 			 1205
2100 			 1966 			 1178
1608 			 1683 			 958
1451 			 1657 			 825
583 			 842 			 418
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 94
273 			 248 			 60
242 			 251 			 77
218 			 169 			 54
170 			 237 			 51
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891890688
Epoch [757/1000] took 96.97161340713501s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2306607606997741, train accuracy: 0.5648066997760249
Val mean loss: 1.775720081678251, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2372 			 1998 			 1224
2159 			 2123 			 1208
2091 			 1966 			 1167
1617 			 1683 			 955
1467 			 1657 			 840
563 			 842 			 406
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 102
292 			 248 			 66
227 			 251 			 74
210 			 169 			 54
139 			 237 			 45
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [758/1000] took 97.23684239387512s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2308010297400929, train accuracy: 0.5650988411724608
Val mean loss: 1.7667526064849481, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2448 			 1998 			 1245
2194 			 2123 			 1217
2070 			 1966 			 1169
1575 			 1683 			 947
1402 			 1657 			 807
580 			 842 			 418
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 87
259 			 248 			 59
255 			 251 			 80
230 			 169 			 57
170 			 237 			 53
69 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [759/1000] took 97.01580739021301s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2317849939857317, train accuracy: 0.5662674067582043
Val mean loss: 1.7544534002862326, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2365 			 1998 			 1223
2150 			 2123 			 1217
2108 			 1966 			 1167
1626 			 1683 			 956
1431 			 1657 			 829
589 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 90
255 			 248 			 58
233 			 251 			 75
226 			 169 			 57
196 			 237 			 60
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [760/1000] took 97.67124390602112s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2314923264527247, train accuracy: 0.5670464504820333
Val mean loss: 1.761917035754134, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2413 			 1998 			 1244
2119 			 2123 			 1199
2075 			 1966 			 1170
1646 			 1683 			 969
1445 			 1657 			 832
571 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
280 			 263 			 82
302 			 248 			 70
258 			 251 			 81
202 			 169 			 52
176 			 237 			 55
66 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3892087296
Epoch [761/1000] took 96.91558718681335s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2312093644498665, train accuracy: 0.5678254942058623
Val mean loss: 1.7810507518489187, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2386 			 1998 			 1239
2177 			 2123 			 1216
2067 			 1966 			 1171
1601 			 1683 			 961
1465 			 1657 			 834
573 			 842 			 410
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 98
264 			 248 			 58
255 			 251 			 79
219 			 169 			 54
153 			 237 			 49
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846931968
Epoch [762/1000] took 96.76006054878235s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.230498987194905, train accuracy: 0.5655857434998539
Val mean loss: 1.769764211119675, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2383 			 1998 			 1231
2141 			 2123 			 1203
2096 			 1966 			 1177
1638 			 1683 			 952
1437 			 1657 			 827
574 			 842 			 418
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
274 			 263 			 81
300 			 248 			 68
245 			 251 			 76
205 			 169 			 52
181 			 237 			 54
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [763/1000] took 96.85601878166199s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2292540689866491, train accuracy: 0.5706495277047424
Val mean loss: 1.7629670457142155, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1242
2137 			 2123 			 1201
2067 			 1966 			 1168
1640 			 1683 			 976
1484 			 1657 			 857
585 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 95
296 			 248 			 67
239 			 251 			 78
197 			 169 			 52
168 			 237 			 51
72 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [764/1000] took 96.89959907531738s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2293118539257584, train accuracy: 0.5650014607069822
Val mean loss: 1.7351650057769403, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2370 			 1998 			 1231
2198 			 2123 			 1219
2108 			 1966 			 1172
1568 			 1683 			 939
1422 			 1657 			 814
603 			 842 			 427
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
338 			 263 			 100
277 			 248 			 64
199 			 251 			 69
247 			 169 			 62
158 			 237 			 49
65 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [765/1000] took 96.80605745315552s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2289353499531375, train accuracy: 0.5673385918784691
Val mean loss: 1.7556742051752603, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2405 			 1998 			 1230
2154 			 2123 			 1213
2061 			 1966 			 1168
1652 			 1683 			 975
1423 			 1657 			 822
574 			 842 			 418
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 92
258 			 248 			 59
261 			 251 			 80
220 			 169 			 54
179 			 237 			 54
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [766/1000] took 96.80581855773926s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2296773535068903, train accuracy: 0.569967864446392
Val mean loss: 1.7636529381682233, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2353 			 1998 			 1237
2149 			 2123 			 1216
2085 			 1966 			 1177
1623 			 1683 			 963
1471 			 1657 			 840
588 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 91
252 			 248 			 59
268 			 251 			 81
225 			 169 			 55
166 			 237 			 56
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [767/1000] took 96.89769196510315s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2294181213943387, train accuracy: 0.5686045379296913
Val mean loss: 1.745378439019366, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2310 			 1998 			 1217
2150 			 2123 			 1209
2138 			 1966 			 1191
1637 			 1683 			 968
1456 			 1657 			 834
578 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 101
284 			 248 			 67
244 			 251 			 76
199 			 169 			 51
150 			 237 			 48
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [768/1000] took 96.80541968345642s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2280446435803565, train accuracy: 0.5685071574642127
Val mean loss: 1.7425349223904494, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2405 			 1998 			 1250
2227 			 2123 			 1237
2018 			 1966 			 1151
1618 			 1683 			 960
1425 			 1657 			 828
576 			 842 			 412
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 82
223 			 248 			 53
316 			 251 			 94
195 			 169 			 51
190 			 237 			 56
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [769/1000] took 97.05100989341736s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.229209577554483, train accuracy: 0.5672412114129906
Val mean loss: 1.73150656281448, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1224
2108 			 2123 			 1192
2159 			 1966 			 1194
1607 			 1683 			 961
1469 			 1657 			 842
570 			 842 			 412
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 96
270 			 248 			 64
258 			 251 			 78
213 			 169 			 53
144 			 237 			 46
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [770/1000] took 96.49338269233704s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2285635935554624, train accuracy: 0.5647093193105464
Val mean loss: 1.7510902561792514, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2392 			 1998 			 1239
2131 			 2123 			 1199
2109 			 1966 			 1175
1616 			 1683 			 948
1412 			 1657 			 817
609 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 99
260 			 248 			 60
229 			 251 			 74
245 			 169 			 60
166 			 237 			 54
65 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [771/1000] took 97.12877583503723s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2292002081128295, train accuracy: 0.5694809621189989
Val mean loss: 1.7610458571736405, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2396 			 1998 			 1240
2136 			 2123 			 1215
2066 			 1966 			 1166
1632 			 1683 			 972
1474 			 1657 			 844
565 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 99
262 			 248 			 57
267 			 251 			 80
192 			 169 			 49
167 			 237 			 48
70 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846702592
Epoch [772/1000] took 96.92199540138245s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.22795919503007, train accuracy: 0.5688966793261272
Val mean loss: 1.7313832335355805, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2409 			 1998 			 1241
2164 			 2123 			 1214
2068 			 1966 			 1176
1621 			 1683 			 974
1428 			 1657 			 822
579 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 96
282 			 248 			 64
245 			 251 			 75
205 			 169 			 52
153 			 237 			 47
78 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [773/1000] took 97.35322380065918s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2265834067469445, train accuracy: 0.564027656052196
Val mean loss: 1.75793577403557, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2407 			 1998 			 1232
2161 			 2123 			 1200
2070 			 1966 			 1156
1618 			 1683 			 958
1423 			 1657 			 821
590 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 94
282 			 248 			 63
251 			 251 			 80
191 			 169 			 50
178 			 237 			 51
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [774/1000] took 96.9104630947113s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2274489666442634, train accuracy: 0.5659752653617685
Val mean loss: 1.768680502728718, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2365 			 1998 			 1230
2155 			 2123 			 1201
2143 			 1966 			 1187
1600 			 1683 			 958
1434 			 1657 			 824
572 			 842 			 412
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 89
282 			 248 			 68
262 			 251 			 80
195 			 169 			 50
166 			 237 			 50
82 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [775/1000] took 97.23841691017151s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2265448822782048, train accuracy: 0.5692862011880416
Val mean loss: 1.7604511511035081, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2341 			 1998 			 1234
2160 			 2123 			 1219
2073 			 1966 			 1169
1623 			 1683 			 957
1466 			 1657 			 832
606 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 94
290 			 248 			 69
254 			 251 			 81
209 			 169 			 54
147 			 237 			 47
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [776/1000] took 96.29071617126465s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2243652763396409, train accuracy: 0.5683123965332554
Val mean loss: 1.7534566826936675, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2416 			 1998 			 1246
2104 			 2123 			 1185
2089 			 1966 			 1176
1624 			 1683 			 966
1444 			 1657 			 839
592 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 96
296 			 248 			 64
223 			 251 			 72
222 			 169 			 56
149 			 237 			 45
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [777/1000] took 96.94691777229309s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2254682095994087, train accuracy: 0.5716233323595287
Val mean loss: 1.7634666198637428, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2392 			 1998 			 1236
2162 			 2123 			 1218
2071 			 1966 			 1184
1612 			 1683 			 965
1445 			 1657 			 842
587 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 98
298 			 248 			 65
235 			 251 			 77
218 			 169 			 58
152 			 237 			 46
62 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [778/1000] took 97.24899435043335s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2299238679193634, train accuracy: 0.5667543090855974
Val mean loss: 1.7546163332171556, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1256
2151 			 2123 			 1210
2052 			 1966 			 1158
1620 			 1683 			 954
1440 			 1657 			 826
560 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 87
244 			 248 			 57
269 			 251 			 82
214 			 169 			 55
177 			 237 			 55
84 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [779/1000] took 97.15162444114685s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2273686790020666, train accuracy: 0.570746908170221
Val mean loss: 1.7665914122651263, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2352 			 1998 			 1234
2131 			 2123 			 1210
2125 			 1966 			 1181
1621 			 1683 			 966
1451 			 1657 			 846
589 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 96
267 			 248 			 60
245 			 251 			 77
209 			 169 			 53
163 			 237 			 49
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [780/1000] took 97.02120590209961s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2265073543768434, train accuracy: 0.5688966793261272
Val mean loss: 1.7442668879904397, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1238
2147 			 2123 			 1212
2054 			 1966 			 1170
1635 			 1683 			 968
1457 			 1657 			 831
583 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 96
286 			 248 			 64
262 			 251 			 81
208 			 169 			 52
148 			 237 			 45
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [781/1000] took 96.72330212593079s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2244457825322017, train accuracy: 0.5704547667737851
Val mean loss: 1.7561664755751447, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2394 			 1998 			 1244
2152 			 2123 			 1229
2089 			 1966 			 1174
1616 			 1683 			 965
1420 			 1657 			 824
598 			 842 			 422
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 91
274 			 248 			 62
235 			 251 			 75
223 			 169 			 55
173 			 237 			 53
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [782/1000] took 97.45195317268372s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2279325058898451, train accuracy: 0.569967864446392
Val mean loss: 1.7579851877398607, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2365 			 1998 			 1238
2182 			 2123 			 1230
2088 			 1966 			 1168
1594 			 1683 			 951
1460 			 1657 			 847
580 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 97
281 			 248 			 67
217 			 251 			 70
248 			 169 			 61
141 			 237 			 44
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [783/1000] took 97.50302028656006s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.228499287014067, train accuracy: 0.5687019183951699
Val mean loss: 1.7438782046480876, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1244
2156 			 2123 			 1211
2030 			 1966 			 1155
1681 			 1683 			 979
1459 			 1657 			 832
572 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 95
290 			 248 			 68
279 			 251 			 82
176 			 169 			 48
139 			 237 			 45
76 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [784/1000] took 97.21945762634277s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.227496211588197, train accuracy: 0.5683123965332554
Val mean loss: 1.7553739954785603, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1237
2140 			 2123 			 1213
2150 			 1966 			 1198
1593 			 1683 			 956
1420 			 1657 			 819
578 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 93
258 			 248 			 57
223 			 251 			 74
234 			 169 			 59
179 			 237 			 54
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [785/1000] took 97.59854626655579s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.228014719634784, train accuracy: 0.5661700262927257
Val mean loss: 1.769958586227603, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2409 			 1998 			 1239
2174 			 2123 			 1213
2029 			 1966 			 1155
1600 			 1683 			 948
1466 			 1657 			 842
591 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 90
253 			 248 			 58
264 			 251 			 84
235 			 169 			 58
172 			 237 			 52
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [786/1000] took 96.82675313949585s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.225575093168336, train accuracy: 0.5692862011880416
Val mean loss: 1.7656826653131625, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1238
2119 			 2123 			 1205
2127 			 1966 			 1181
1666 			 1683 			 982
1430 			 1657 			 831
571 			 842 			 409
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 88
289 			 248 			 62
224 			 251 			 74
226 			 169 			 57
176 			 237 			 52
75 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [787/1000] took 96.96716737747192s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2255133104472888, train accuracy: 0.5678254942058623
Val mean loss: 1.7420256690281193, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2359 			 1998 			 1231
2192 			 2123 			 1221
2046 			 1966 			 1161
1639 			 1683 			 969
1451 			 1657 			 832
582 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 102
286 			 248 			 66
257 			 251 			 75
194 			 169 			 51
148 			 237 			 47
65 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [788/1000] took 96.90451192855835s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2254586141800212, train accuracy: 0.5698704839809134
Val mean loss: 1.7635567042885758, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2375 			 1998 			 1227
2164 			 2123 			 1223
2083 			 1966 			 1174
1615 			 1683 			 968
1449 			 1657 			 838
583 			 842 			 422
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 95
274 			 248 			 61
256 			 251 			 79
208 			 169 			 51
170 			 237 			 53
63 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [789/1000] took 97.68185758590698s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.226951122655304, train accuracy: 0.5701626253773493
Val mean loss: 1.7589692778703643, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2434 			 1998 			 1257
2151 			 2123 			 1220
2077 			 1966 			 1180
1604 			 1683 			 957
1415 			 1657 			 816
588 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 83
250 			 248 			 55
257 			 251 			 80
230 			 169 			 56
185 			 237 			 57
72 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [790/1000] took 97.02775120735168s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2237927568293063, train accuracy: 0.5678254942058623
Val mean loss: 1.7433473307911942, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2387 			 1998 			 1229
2120 			 2123 			 1197
2089 			 1966 			 1166
1640 			 1683 			 980
1452 			 1657 			 839
581 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 86
316 			 248 			 71
246 			 251 			 76
212 			 169 			 54
157 			 237 			 50
72 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3804758528
Epoch [791/1000] took 97.31216192245483s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2267311311956506, train accuracy: 0.5688966793261272
Val mean loss: 1.760130251326212, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2334 			 1998 			 1221
2214 			 2123 			 1233
2054 			 1966 			 1159
1628 			 1683 			 969
1444 			 1657 			 832
595 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 88
269 			 248 			 63
268 			 251 			 81
202 			 169 			 52
189 			 237 			 56
59 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3804758528
Epoch [792/1000] took 96.95592832565308s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2225126659387369, train accuracy: 0.5713311909630928
Val mean loss: 1.7765806767998673, val accuracy: 0.29439252336448596

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2308 			 1998 			 1215
2105 			 2123 			 1215
2147 			 1966 			 1195
1638 			 1683 			 973
1492 			 1657 			 846
579 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
358 			 263 			 106
258 			 248 			 58
218 			 251 			 73
221 			 169 			 60
160 			 237 			 51
69 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [793/1000] took 97.32002353668213s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2258040022998584, train accuracy: 0.5684097769987341
Val mean loss: 1.7726015521258842, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2454 			 1998 			 1267
2147 			 2123 			 1211
2041 			 1966 			 1164
1630 			 1683 			 963
1428 			 1657 			 818
569 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 90
292 			 248 			 66
241 			 251 			 73
208 			 169 			 51
173 			 237 			 50
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [794/1000] took 97.68881797790527s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2277464816503436, train accuracy: 0.5692862011880416
Val mean loss: 1.7381234256232656, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2376 			 1998 			 1241
2139 			 2123 			 1212
2127 			 1966 			 1190
1591 			 1683 			 956
1466 			 1657 			 834
570 			 842 			 413
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 91
310 			 248 			 73
238 			 251 			 75
206 			 169 			 52
156 			 237 			 48
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [795/1000] took 97.03960275650024s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2262807503296207, train accuracy: 0.5696757230499562
Val mean loss: 1.7806638333855607, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2391 			 1998 			 1241
2093 			 2123 			 1192
2104 			 1966 			 1177
1620 			 1683 			 967
1467 			 1657 			 850
594 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
332 			 263 			 100
295 			 248 			 72
254 			 251 			 77
193 			 169 			 51
138 			 237 			 44
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891628544
Epoch [796/1000] took 97.47012591362s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2249406611436624, train accuracy: 0.5695783425844776
Val mean loss: 1.7726238704309232, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2444 			 1998 			 1259
2128 			 2123 			 1203
2061 			 1966 			 1176
1625 			 1683 			 966
1429 			 1657 			 828
582 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
291 			 263 			 86
286 			 248 			 68
272 			 251 			 82
209 			 169 			 53
159 			 237 			 48
67 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [797/1000] took 97.22124075889587s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2242813310890555, train accuracy: 0.5715259518940501
Val mean loss: 1.7469197279069482, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2346 			 1998 			 1231
2241 			 2123 			 1252
2093 			 1966 			 1176
1570 			 1683 			 954
1453 			 1657 			 839
566 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 98
217 			 248 			 53
268 			 251 			 86
232 			 169 			 58
168 			 237 			 51
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [798/1000] took 97.05903792381287s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2218936254673658, train accuracy: 0.5683123965332554
Val mean loss: 1.7592660508504727, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2399 			 1998 			 1254
2038 			 2123 			 1180
2174 			 1966 			 1189
1637 			 1683 			 962
1436 			 1657 			 831
585 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 97
307 			 248 			 71
222 			 251 			 72
213 			 169 			 55
147 			 237 			 49
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [799/1000] took 97.2652108669281s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2240025492100701, train accuracy: 0.5670464504820333
Val mean loss: 1.747877885655659, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1247
2178 			 2123 			 1210
2067 			 1966 			 1172
1611 			 1683 			 958
1436 			 1657 			 825
576 			 842 			 411
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 94
252 			 248 			 59
238 			 251 			 77
224 			 169 			 58
182 			 237 			 54
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [800/1000] took 97.24764800071716s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.223275378300022, train accuracy: 0.5688966793261272
Val mean loss: 1.7668734876120962, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2394 			 1998 			 1253
2146 			 2123 			 1202
2098 			 1966 			 1172
1592 			 1683 			 952
1442 			 1657 			 838
597 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 92
288 			 248 			 65
228 			 251 			 75
219 			 169 			 54
161 			 237 			 49
65 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [801/1000] took 97.3423535823822s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.224737045363845, train accuracy: 0.5676307332749051
Val mean loss: 1.7830296115177433, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2435 			 1998 			 1243
2094 			 2123 			 1196
2041 			 1966 			 1151
1654 			 1683 			 971
1464 			 1657 			 849
581 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 80
324 			 248 			 75
235 			 251 			 73
222 			 169 			 56
137 			 237 			 47
80 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [802/1000] took 96.83726000785828s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2226554019800229, train accuracy: 0.5733761807381439
Val mean loss: 1.736537639687701, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2331 			 1998 			 1241
2175 			 2123 			 1222
2089 			 1966 			 1183
1621 			 1683 			 967
1474 			 1657 			 848
579 			 842 			 427
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 96
282 			 248 			 63
236 			 251 			 72
220 			 169 			 56
153 			 237 			 48
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [803/1000] took 96.98330163955688s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2231821352819046, train accuracy: 0.5681176356022982
Val mean loss: 1.7546566753852657, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1237
2183 			 2123 			 1223
2121 			 1966 			 1174
1592 			 1683 			 947
1433 			 1657 			 829
584 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 96
273 			 248 			 66
224 			 251 			 73
244 			 169 			 61
147 			 237 			 48
79 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3846669824
Epoch [804/1000] took 96.87012100219727s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2231001755530218, train accuracy: 0.5675333528094264
Val mean loss: 1.7577740797182408, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2433 			 1998 			 1256
2112 			 2123 			 1195
2012 			 1966 			 1143
1674 			 1683 			 974
1445 			 1657 			 834
593 			 842 			 426
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 84
266 			 248 			 60
266 			 251 			 84
206 			 169 			 52
172 			 237 			 53
80 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [805/1000] took 97.23992466926575s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.223707530543069, train accuracy: 0.569967864446392
Val mean loss: 1.7522299086175315, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2343 			 1998 			 1231
2172 			 2123 			 1226
2070 			 1966 			 1171
1616 			 1683 			 966
1472 			 1657 			 840
596 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
346 			 263 			 105
246 			 248 			 57
279 			 251 			 85
198 			 169 			 52
148 			 237 			 48
67 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [806/1000] took 97.40480899810791s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2251351288174543, train accuracy: 0.5702600058428279
Val mean loss: 1.7401462909651966, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2420 			 1998 			 1250
2167 			 2123 			 1230
2097 			 1966 			 1178
1616 			 1683 			 962
1406 			 1657 			 820
563 			 842 			 416
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
273 			 263 			 80
260 			 248 			 58
251 			 251 			 79
231 			 169 			 60
195 			 237 			 60
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [807/1000] took 97.12779474258423s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.22088479884317, train accuracy: 0.5692862011880416
Val mean loss: 1.7661297990054619, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2338 			 1998 			 1221
2113 			 2123 			 1199
2088 			 1966 			 1180
1673 			 1683 			 982
1453 			 1657 			 840
604 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 101
323 			 248 			 69
225 			 251 			 73
191 			 169 			 50
153 			 237 			 47
57 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [808/1000] took 97.53494143486023s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2252685186276184, train accuracy: 0.5666569286201188
Val mean loss: 1.7472047195201967, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1240
2220 			 2123 			 1222
2024 			 1966 			 1147
1615 			 1683 			 959
1440 			 1657 			 827
582 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 89
261 			 248 			 58
278 			 251 			 86
196 			 169 			 49
174 			 237 			 51
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [809/1000] took 97.1439847946167s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2237843822838732, train accuracy: 0.5723049956178791
Val mean loss: 1.7821672631473076, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2402 			 1998 			 1261
2104 			 2123 			 1210
2101 			 1966 			 1181
1601 			 1683 			 955
1479 			 1657 			 846
582 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
314 			 263 			 93
293 			 248 			 69
229 			 251 			 72
229 			 169 			 58
150 			 237 			 48
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [810/1000] took 97.54234862327576s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.223001757514811, train accuracy: 0.569188820722563
Val mean loss: 1.752002782937957, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2435 			 1998 			 1257
2127 			 2123 			 1211
2095 			 1966 			 1172
1618 			 1683 			 962
1403 			 1657 			 815
591 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
280 			 263 			 80
285 			 248 			 65
232 			 251 			 73
228 			 169 			 57
187 			 237 			 56
72 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [811/1000] took 97.28110575675964s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.22072140895689, train accuracy: 0.5717207128250072
Val mean loss: 1.7542554547147053, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2276 			 1998 			 1220
2199 			 2123 			 1222
2094 			 1966 			 1187
1625 			 1683 			 963
1477 			 1657 			 849
598 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 99
244 			 248 			 54
272 			 251 			 82
208 			 169 			 52
162 			 237 			 51
61 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [812/1000] took 97.41445088386536s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2201198424877036, train accuracy: 0.5727918979452722
Val mean loss: 1.7643221558594122, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2428 			 1998 			 1255
2119 			 2123 			 1217
2133 			 1966 			 1199
1582 			 1683 			 964
1428 			 1657 			 833
579 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 91
273 			 248 			 63
239 			 251 			 76
237 			 169 			 58
163 			 237 			 51
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [813/1000] took 97.25467801094055s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.221464529587101, train accuracy: 0.5726945174797936
Val mean loss: 1.7443145455383673, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2395 			 1998 			 1247
2143 			 2123 			 1223
2059 			 1966 			 1177
1647 			 1683 			 977
1461 			 1657 			 834
564 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
278 			 263 			 79
268 			 248 			 63
250 			 251 			 79
223 			 169 			 56
184 			 237 			 55
81 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [814/1000] took 96.68128705024719s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2199568300975074, train accuracy: 0.573863083065537
Val mean loss: 1.7502326587351358, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2323 			 1998 			 1244
2099 			 2123 			 1198
2134 			 1966 			 1192
1630 			 1683 			 967
1468 			 1657 			 852
615 			 842 			 440
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 88
286 			 248 			 63
214 			 251 			 71
226 			 169 			 57
183 			 237 			 58
78 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [815/1000] took 97.53848719596863s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2219899586427991, train accuracy: 0.5715259518940501
Val mean loss: 1.750419869655516, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2372 			 1998 			 1238
2172 			 2123 			 1221
2065 			 1966 			 1172
1655 			 1683 			 983
1415 			 1657 			 823
590 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 89
240 			 248 			 55
244 			 251 			 78
207 			 169 			 52
210 			 237 			 61
73 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [816/1000] took 97.30166435241699s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.222977102545563, train accuracy: 0.5718180932904859
Val mean loss: 1.8041921621415673, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2328 			 1998 			 1230
2166 			 2123 			 1216
2062 			 1966 			 1170
1624 			 1683 			 968
1504 			 1657 			 861
585 			 842 			 427
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 97
278 			 248 			 63
258 			 251 			 76
203 			 169 			 52
155 			 237 			 48
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [817/1000] took 97.45994973182678s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.219548495387734, train accuracy: 0.5713311909630928
Val mean loss: 1.7665712164669503, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2433 			 1998 			 1252
2110 			 2123 			 1209
2165 			 1966 			 1201
1539 			 1683 			 949
1428 			 1657 			 827
594 			 842 			 429
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 87
287 			 248 			 66
230 			 251 			 74
260 			 169 			 67
148 			 237 			 46
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [818/1000] took 97.08089399337769s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2202875662444166, train accuracy: 0.5712338104976141
Val mean loss: 1.7456909476256952, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2360 			 1998 			 1232
2160 			 2123 			 1213
2062 			 1966 			 1177
1684 			 1683 			 993
1437 			 1657 			 837
566 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 96
274 			 248 			 61
231 			 251 			 72
219 			 169 			 55
167 			 237 			 48
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845621248
Epoch [819/1000] took 97.04985070228577s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2196625998458388, train accuracy: 0.5753237900477164
Val mean loss: 1.78335514301207, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2351 			 1998 			 1231
2210 			 2123 			 1245
2043 			 1966 			 1178
1626 			 1683 			 984
1435 			 1657 			 837
604 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 94
276 			 248 			 66
253 			 251 			 80
204 			 169 			 52
164 			 237 			 51
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847783936
Epoch [820/1000] took 96.93993616104126s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2181615855463568, train accuracy: 0.5728892784107508
Val mean loss: 1.7704550696582328, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2335 			 1998 			 1227
2153 			 2123 			 1223
2047 			 1966 			 1172
1670 			 1683 			 991
1479 			 1657 			 844
585 			 842 			 426
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 100
290 			 248 			 67
237 			 251 			 72
193 			 169 			 50
168 			 237 			 51
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [821/1000] took 97.2411720752716s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.220479418555524, train accuracy: 0.5720128542214432
Val mean loss: 1.7812628164523985, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2355 			 1998 			 1239
2169 			 2123 			 1221
2130 			 1966 			 1197
1614 			 1683 			 966
1433 			 1657 			 841
568 			 842 			 410
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
353 			 263 			 105
272 			 248 			 64
237 			 251 			 74
197 			 169 			 52
154 			 237 			 47
71 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [822/1000] took 97.4082019329071s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.220071889901087, train accuracy: 0.5729866588762295
Val mean loss: 1.7527934487273054, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2425 			 1998 			 1272
2152 			 2123 			 1214
2114 			 1966 			 1186
1585 			 1683 			 957
1404 			 1657 			 824
589 			 842 			 431
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 87
276 			 248 			 65
239 			 251 			 76
227 			 169 			 56
178 			 237 			 55
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [823/1000] took 97.27706670761108s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2174536049180313, train accuracy: 0.574934268185802
Val mean loss: 1.7697995930183223, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2409 			 1998 			 1244
2078 			 2123 			 1199
2106 			 1966 			 1194
1612 			 1683 			 978
1469 			 1657 			 852
595 			 842 			 437
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 84
294 			 248 			 69
217 			 251 			 71
248 			 169 			 61
159 			 237 			 49
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846800896
Epoch [824/1000] took 97.38063097000122s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.217758883011304, train accuracy: 0.571039049566657
Val mean loss: 1.749955729740422, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2345 			 1998 			 1241
2163 			 2123 			 1216
2052 			 1966 			 1168
1674 			 1683 			 969
1458 			 1657 			 853
577 			 842 			 417
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
332 			 263 			 99
281 			 248 			 65
263 			 251 			 77
206 			 169 			 52
127 			 237 			 40
75 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [825/1000] took 97.24024367332458s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2182406049651149, train accuracy: 0.5717207128250072
Val mean loss: 1.7476700776960792, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2427 			 1998 			 1252
2104 			 2123 			 1202
2154 			 1966 			 1200
1579 			 1683 			 955
1422 			 1657 			 834
583 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 89
332 			 248 			 72
201 			 251 			 67
228 			 169 			 54
155 			 237 			 48
76 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846735360
Epoch [826/1000] took 97.46680951118469s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.219571932825344, train accuracy: 0.5739604635310157
Val mean loss: 1.779450535774231, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2304 			 1998 			 1225
2243 			 2123 			 1249
2042 			 1966 			 1177
1666 			 1683 			 977
1413 			 1657 			 834
601 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 84
265 			 248 			 60
281 			 251 			 81
195 			 169 			 50
187 			 237 			 57
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [827/1000] took 97.12385368347168s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2173286652267907, train accuracy: 0.5720128542214432
Val mean loss: 1.761564263483373, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2382 			 1998 			 1243
2130 			 2123 			 1203
2123 			 1966 			 1198
1581 			 1683 			 952
1481 			 1657 			 855
572 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
319 			 263 			 93
231 			 248 			 53
282 			 251 			 88
208 			 169 			 52
166 			 237 			 49
78 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [828/1000] took 97.01355957984924s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.216887485386798, train accuracy: 0.5736683221345799
Val mean loss: 1.7845527835008574, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2399 			 1998 			 1244
2100 			 2123 			 1201
2109 			 1966 			 1188
1647 			 1683 			 983
1422 			 1657 			 842
592 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
273 			 263 			 83
318 			 248 			 71
237 			 251 			 74
207 			 169 			 51
169 			 237 			 51
80 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [829/1000] took 97.06063175201416s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2190771110332643, train accuracy: 0.5731814198071867
Val mean loss: 1.772527180066923, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2342 			 1998 			 1233
2169 			 2123 			 1228
2108 			 1966 			 1193
1577 			 1683 			 955
1478 			 1657 			 850
595 			 842 			 427
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 94
265 			 248 			 60
234 			 251 			 74
234 			 169 			 61
157 			 237 			 47
76 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [830/1000] took 96.56381177902222s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2144775748995607, train accuracy: 0.5750316486512805
Val mean loss: 1.773993308951215, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2425 			 1998 			 1266
2089 			 2123 			 1211
2076 			 1966 			 1171
1644 			 1683 			 979
1452 			 1657 			 850
583 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 90
299 			 248 			 71
247 			 251 			 76
205 			 169 			 51
154 			 237 			 47
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [831/1000] took 96.71284413337708s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2157667583765641, train accuracy: 0.5720128542214432
Val mean loss: 1.7506875788293235, val accuracy: 0.2757009345794392

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1241
2193 			 2123 			 1233
2086 			 1966 			 1177
1586 			 1683 			 958
1415 			 1657 			 829
601 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
282 			 263 			 81
283 			 248 			 62
252 			 251 			 76
225 			 169 			 55
169 			 237 			 51
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [832/1000] took 97.39294052124023s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2178629545779243, train accuracy: 0.5736683221345799
Val mean loss: 1.7835405628855636, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2339 			 1998 			 1231
2240 			 2123 			 1250
2037 			 1966 			 1173
1588 			 1683 			 959
1486 			 1657 			 855
579 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
288 			 263 			 87
231 			 248 			 56
275 			 251 			 84
239 			 169 			 57
170 			 237 			 51
81 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [833/1000] took 97.51756525039673s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2166061015143943, train accuracy: 0.5719154737559645
Val mean loss: 1.7694588899612427, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2350 			 1998 			 1229
2104 			 2123 			 1190
2115 			 1966 			 1189
1671 			 1683 			 992
1424 			 1657 			 834
605 			 842 			 439
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 98
264 			 248 			 63
256 			 251 			 78
205 			 169 			 53
156 			 237 			 50
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [834/1000] took 97.14991569519043s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2175718222823098, train accuracy: 0.5728892784107508
Val mean loss: 1.7700025599177291, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2409 			 1998 			 1259
2094 			 2123 			 1202
2096 			 1966 			 1182
1609 			 1683 			 969
1471 			 1657 			 846
590 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 98
283 			 248 			 66
243 			 251 			 76
202 			 169 			 53
159 			 237 			 48
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [835/1000] took 97.08299803733826s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.216189348809073, train accuracy: 0.5773687798227676
Val mean loss: 1.7709547862773989, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2339 			 1998 			 1249
2140 			 2123 			 1215
2062 			 1966 			 1179
1665 			 1683 			 990
1469 			 1657 			 861
594 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
332 			 263 			 100
299 			 248 			 68
250 			 251 			 80
196 			 169 			 51
139 			 237 			 46
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [836/1000] took 97.45514988899231s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2179559833164155, train accuracy: 0.5720128542214432
Val mean loss: 1.7590452345406138, val accuracy: 0.2749221183800623

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2415 			 1998 			 1255
2161 			 2123 			 1224
2080 			 1966 			 1182
1646 			 1683 			 975
1390 			 1657 			 819
577 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 81
292 			 248 			 65
260 			 251 			 75
191 			 169 			 49
188 			 237 			 54
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [837/1000] took 97.03473258018494s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.21507754522692, train accuracy: 0.5745447463238874
Val mean loss: 1.7742040273619861, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2391 			 1998 			 1260
2123 			 2123 			 1203
2107 			 1966 			 1196
1597 			 1683 			 964
1467 			 1657 			 851
584 			 842 			 426
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 93
283 			 248 			 65
241 			 251 			 76
203 			 169 			 52
178 			 237 			 52
76 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [838/1000] took 96.6402485370636s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2161914951333375, train accuracy: 0.569967864446392
Val mean loss: 1.7707004430817395, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1246
2161 			 2123 			 1207
2071 			 1966 			 1174
1602 			 1683 			 961
1439 			 1657 			 834
603 			 842 			 431
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 86
287 			 248 			 64
243 			 251 			 75
219 			 169 			 56
173 			 237 			 52
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [839/1000] took 97.05617260932922s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2145601173056249, train accuracy: 0.5772713993572889
Val mean loss: 1.741715572229246, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2394 			 1998 			 1265
2171 			 2123 			 1241
2018 			 1966 			 1153
1668 			 1683 			 990
1443 			 1657 			 851
575 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
282 			 263 			 86
289 			 248 			 68
286 			 251 			 83
197 			 169 			 51
153 			 237 			 48
77 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [840/1000] took 97.34201550483704s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2156740038937126, train accuracy: 0.5731814198071867
Val mean loss: 1.7588911143744863, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1256
2109 			 2123 			 1199
2102 			 1966 			 1187
1627 			 1683 			 979
1459 			 1657 			 845
579 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 89
301 			 248 			 68
255 			 251 			 78
211 			 169 			 53
143 			 237 			 44
78 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [841/1000] took 97.18513655662537s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.216116491693574, train accuracy: 0.573863083065537
Val mean loss: 1.741393257931965, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2343 			 1998 			 1254
2162 			 2123 			 1215
2120 			 1966 			 1190
1627 			 1683 			 975
1418 			 1657 			 827
599 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 83
286 			 248 			 66
245 			 251 			 76
217 			 169 			 54
176 			 237 			 54
81 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [842/1000] took 96.92873048782349s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.215326043118569, train accuracy: 0.5728892784107508
Val mean loss: 1.7561148405075073, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2347 			 1998 			 1241
2202 			 2123 			 1227
2087 			 1966 			 1185
1604 			 1683 			 969
1447 			 1657 			 837
582 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
301 			 263 			 93
262 			 248 			 61
249 			 251 			 78
202 			 169 			 52
187 			 237 			 50
83 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3847521792
Epoch [843/1000] took 96.7435975074768s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2154334066812866, train accuracy: 0.574934268185802
Val mean loss: 1.7579474943440134, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2350 			 1998 			 1246
2172 			 2123 			 1225
2088 			 1966 			 1187
1574 			 1683 			 949
1475 			 1657 			 856
610 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
356 			 263 			 106
223 			 248 			 52
246 			 251 			 77
226 			 169 			 58
169 			 237 			 53
64 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [844/1000] took 97.43829274177551s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2171896892916005, train accuracy: 0.5725971370143149
Val mean loss: 1.7605876631853057, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2410 			 1998 			 1256
2128 			 2123 			 1205
2073 			 1966 			 1174
1633 			 1683 			 973
1445 			 1657 			 846
580 			 842 			 426
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 93
243 			 248 			 55
245 			 251 			 76
245 			 169 			 59
169 			 237 			 51
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [845/1000] took 97.31981039047241s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2174869010752978, train accuracy: 0.5733761807381439
Val mean loss: 1.7501379571309903, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2414 			 1998 			 1259
2138 			 2123 			 1217
2087 			 1966 			 1181
1618 			 1683 			 971
1434 			 1657 			 833
578 			 842 			 427
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
290 			 263 			 89
279 			 248 			 63
248 			 251 			 77
226 			 169 			 55
168 			 237 			 51
73 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [846/1000] took 97.0235002040863s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2160639129695119, train accuracy: 0.5757133119096309
Val mean loss: 1.7789658860462467, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2367 			 1998 			 1247
2162 			 2123 			 1236
2084 			 1966 			 1185
1641 			 1683 			 987
1428 			 1657 			 831
587 			 842 			 426
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 92
276 			 248 			 61
239 			 251 			 77
227 			 169 			 56
164 			 237 			 49
74 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [847/1000] took 96.89990854263306s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2125729138606063, train accuracy: 0.5727918979452722
Val mean loss: 1.7595736137250575, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2396 			 1998 			 1243
2124 			 2123 			 1213
2102 			 1966 			 1186
1621 			 1683 			 973
1431 			 1657 			 838
595 			 842 			 429
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 93
287 			 248 			 64
247 			 251 			 80
204 			 169 			 54
167 			 237 			 49
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [848/1000] took 97.38404083251953s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2149586954220803, train accuracy: 0.5728892784107508
Val mean loss: 1.762308318440507, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2319 			 1998 			 1224
2174 			 2123 			 1228
2049 			 1966 			 1172
1637 			 1683 			 963
1494 			 1657 			 864
596 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 100
249 			 248 			 61
252 			 251 			 79
210 			 169 			 54
165 			 237 			 50
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [849/1000] took 97.14624524116516s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.212847171914169, train accuracy: 0.5772713993572889
Val mean loss: 1.75767978807775, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2402 			 1998 			 1272
2112 			 2123 			 1217
2095 			 1966 			 1186
1621 			 1683 			 968
1420 			 1657 			 834
619 			 842 			 451
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 89
269 			 248 			 62
233 			 251 			 74
242 			 169 			 58
168 			 237 			 51
70 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [850/1000] took 97.64926362037659s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.213402486850168, train accuracy: 0.5724997565488363
Val mean loss: 1.763461807879006, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2395 			 1998 			 1247
2134 			 2123 			 1224
2085 			 1966 			 1179
1630 			 1683 			 964
1465 			 1657 			 851
560 			 842 			 414
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 95
302 			 248 			 66
221 			 251 			 74
231 			 169 			 57
139 			 237 			 43
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [851/1000] took 98.04215216636658s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2157919336330854, train accuracy: 0.5725971370143149
Val mean loss: 1.7449241033414515, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2403 			 1998 			 1253
2177 			 2123 			 1231
2081 			 1966 			 1183
1611 			 1683 			 972
1419 			 1657 			 821
578 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
283 			 263 			 87
291 			 248 			 70
246 			 251 			 77
200 			 169 			 52
176 			 237 			 53
88 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [852/1000] took 97.56578993797302s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.213443871413436, train accuracy: 0.5762975947025026
Val mean loss: 1.7674014684630603, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2378 			 1998 			 1245
2130 			 2123 			 1222
2086 			 1966 			 1182
1594 			 1683 			 968
1463 			 1657 			 852
618 			 842 			 449
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 94
295 			 248 			 66
223 			 251 			 71
213 			 169 			 53
172 			 237 			 52
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [853/1000] took 97.96643877029419s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2142949601942876, train accuracy: 0.574155224461973
Val mean loss: 1.7841865405803774, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2315 			 1998 			 1222
2184 			 2123 			 1236
2094 			 1966 			 1195
1606 			 1683 			 971
1492 			 1657 			 850
578 			 842 			 422
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 103
256 			 248 			 58
231 			 251 			 76
238 			 169 			 63
133 			 237 			 41
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [854/1000] took 97.37825345993042s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2129003229170945, train accuracy: 0.5750316486512805
Val mean loss: 1.7690759548326818, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2486 			 1998 			 1274
2080 			 2123 			 1211
2075 			 1966 			 1184
1612 			 1683 			 962
1426 			 1657 			 843
590 			 842 			 431
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 84
324 			 248 			 74
209 			 251 			 69
225 			 169 			 59
165 			 237 			 50
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [855/1000] took 97.54694533348083s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2133086440719176, train accuracy: 0.5763949751679813
Val mean loss: 1.7792313157058344, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2397 			 1998 			 1261
2192 			 2123 			 1249
2050 			 1966 			 1170
1623 			 1683 			 977
1413 			 1657 			 829
594 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
271 			 263 			 76
250 			 248 			 59
230 			 251 			 73
243 			 169 			 60
216 			 237 			 62
74 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [856/1000] took 97.6133725643158s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2151771198923342, train accuracy: 0.5758106923751095
Val mean loss: 1.7830865354072758, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2370 			 1998 			 1251
2111 			 2123 			 1200
2061 			 1966 			 1180
1633 			 1683 			 988
1495 			 1657 			 862
599 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 83
303 			 248 			 68
270 			 251 			 83
192 			 169 			 50
169 			 237 			 51
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [857/1000] took 96.88424158096313s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2121240247818539, train accuracy: 0.5727918979452722
Val mean loss: 1.781325357716258, val accuracy: 0.2772585669781931

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2357 			 1998 			 1237
2155 			 2123 			 1214
2153 			 1966 			 1210
1590 			 1683 			 966
1440 			 1657 			 835
574 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 83
313 			 248 			 69
193 			 251 			 67
247 			 169 			 62
166 			 237 			 48
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [858/1000] took 96.73184108734131s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2119637413931041, train accuracy: 0.5748368877203233
Val mean loss: 1.7814481490995826, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2353 			 1998 			 1245
2220 			 2123 			 1243
2006 			 1966 			 1164
1641 			 1683 			 976
1466 			 1657 			 854
583 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 92
253 			 248 			 58
258 			 251 			 80
225 			 169 			 57
159 			 237 			 48
81 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [859/1000] took 96.52416896820068s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2130027755027248, train accuracy: 0.5763949751679813
Val mean loss: 1.7411741919633819, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2354 			 1998 			 1248
2134 			 2123 			 1216
2062 			 1966 			 1172
1659 			 1683 			 989
1473 			 1657 			 861
587 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 87
286 			 248 			 66
246 			 251 			 74
217 			 169 			 53
163 			 237 			 50
78 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [860/1000] took 96.63199520111084s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2110183393101084, train accuracy: 0.576979257960853
Val mean loss: 1.787839511545693, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2352 			 1998 			 1250
2180 			 2123 			 1244
2088 			 1966 			 1191
1607 			 1683 			 964
1442 			 1657 			 843
600 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 92
247 			 248 			 57
259 			 251 			 79
218 			 169 			 54
182 			 237 			 50
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [861/1000] took 97.04025435447693s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2131096432884905, train accuracy: 0.5732788002726653
Val mean loss: 1.7619162914229602, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2370 			 1998 			 1249
2129 			 2123 			 1223
2091 			 1966 			 1180
1644 			 1683 			 976
1455 			 1657 			 839
580 			 842 			 420
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 99
284 			 248 			 64
247 			 251 			 75
190 			 169 			 52
161 			 237 			 51
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [862/1000] took 97.06264305114746s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2125066214261397, train accuracy: 0.5783425844775538
Val mean loss: 1.7534066409599491, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2373 			 1998 			 1254
2157 			 2123 			 1239
2089 			 1966 			 1186
1617 			 1683 			 970
1421 			 1657 			 845
612 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 92
278 			 248 			 63
201 			 251 			 68
215 			 169 			 55
198 			 237 			 57
71 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [863/1000] took 96.91705369949341s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.211046696823334, train accuracy: 0.5734735612036226
Val mean loss: 1.746778889400203, val accuracy: 0.2780373831775701

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2404 			 1998 			 1249
2144 			 2123 			 1228
2038 			 1966 			 1166
1612 			 1683 			 959
1475 			 1657 			 857
596 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
282 			 263 			 86
290 			 248 			 66
245 			 251 			 74
228 			 169 			 54
169 			 237 			 49
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [864/1000] took 97.18036198616028s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2124858949786035, train accuracy: 0.5754211705131951
Val mean loss: 1.736051460591758, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1245
2194 			 2123 			 1238
2044 			 1966 			 1180
1611 			 1683 			 969
1434 			 1657 			 842
598 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 89
246 			 248 			 54
258 			 251 			 81
226 			 169 			 54
179 			 237 			 54
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [865/1000] took 96.89362168312073s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2107816269835951, train accuracy: 0.5760054533060668
Val mean loss: 1.7547428927770474, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2326 			 1998 			 1246
2093 			 2123 			 1204
2173 			 1966 			 1212
1626 			 1683 			 973
1490 			 1657 			 861
561 			 842 			 419
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 88
293 			 248 			 67
245 			 251 			 74
222 			 169 			 56
151 			 237 			 47
76 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847521792
Epoch [866/1000] took 96.89305806159973s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.210562006149708, train accuracy: 0.5767844970298958
Val mean loss: 1.8136913049511794, val accuracy: 0.2780373831775701

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2396 			 1998 			 1254
2193 			 2123 			 1249
2048 			 1966 			 1179
1617 			 1683 			 971
1427 			 1657 			 842
588 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 82
263 			 248 			 60
266 			 251 			 81
224 			 169 			 55
164 			 237 			 50
78 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [867/1000] took 96.91730999946594s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2134623013181478, train accuracy: 0.5779530626156393
Val mean loss: 1.7624353519300135, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2305 			 1998 			 1233
2158 			 2123 			 1236
2124 			 1966 			 1209
1600 			 1683 			 969
1496 			 1657 			 860
586 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 101
261 			 248 			 61
231 			 251 			 72
240 			 169 			 59
149 			 237 			 46
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [868/1000] took 96.23272609710693s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.209028238448027, train accuracy: 0.5728892784107508
Val mean loss: 1.7676930311249524, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2385 			 1998 			 1256
2227 			 2123 			 1235
2051 			 1966 			 1170
1622 			 1683 			 976
1388 			 1657 			 815
596 			 842 			 431
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 95
230 			 248 			 53
236 			 251 			 76
240 			 169 			 61
186 			 237 			 55
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [869/1000] took 96.57075524330139s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2111458698537119, train accuracy: 0.5766871165644172
Val mean loss: 1.7637732552319039, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2362 			 1998 			 1262
2130 			 2123 			 1210
2099 			 1966 			 1198
1625 			 1683 			 973
1472 			 1657 			 847
581 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 95
261 			 248 			 61
244 			 251 			 79
231 			 169 			 59
158 			 237 			 49
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [870/1000] took 97.22630739212036s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2123730214214028, train accuracy: 0.5739604635310157
Val mean loss: 1.7421043093611555, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2417 			 1998 			 1247
2111 			 2123 			 1215
2095 			 1966 			 1194
1640 			 1683 			 978
1404 			 1657 			 827
602 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 82
281 			 248 			 64
255 			 251 			 79
217 			 169 			 54
177 			 237 			 56
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3800498688
Epoch [871/1000] took 96.75011706352234s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2090852154749576, train accuracy: 0.5772713993572889
Val mean loss: 1.764459836773756, val accuracy: 0.29283489096573206

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2272 			 1998 			 1220
2152 			 2123 			 1225
2119 			 1966 			 1194
1624 			 1683 			 981
1510 			 1657 			 868
592 			 842 			 440
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
324 			 263 			 97
278 			 248 			 71
228 			 251 			 74
229 			 169 			 59
145 			 237 			 45
80 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3847259648
Epoch [872/1000] took 97.40613889694214s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.213203890487041, train accuracy: 0.5745447463238874
Val mean loss: 1.7566372127067753, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2383 			 1998 			 1257
2097 			 2123 			 1207
2082 			 1966 			 1183
1681 			 1683 			 992
1448 			 1657 			 846
578 			 842 			 415
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
265 			 263 			 79
309 			 248 			 68
242 			 251 			 73
217 			 169 			 56
177 			 237 			 55
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [873/1000] took 97.24046802520752s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2109235034924801, train accuracy: 0.5772713993572889
Val mean loss: 1.7683308473447474, val accuracy: 0.27414330218068533

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2395 			 1998 			 1255
2093 			 2123 			 1213
2107 			 1966 			 1199
1640 			 1683 			 985
1440 			 1657 			 842
594 			 842 			 434
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
241 			 263 			 69
327 			 248 			 70
251 			 251 			 77
192 			 169 			 49
190 			 237 			 55
83 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3802628608
Epoch [874/1000] took 96.48150420188904s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2096739039985562, train accuracy: 0.5765897360989386
Val mean loss: 1.7840759725105473, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2285 			 1998 			 1232
2218 			 2123 			 1248
2111 			 1966 			 1187
1588 			 1683 			 968
1469 			 1657 			 849
598 			 842 			 437
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
326 			 263 			 95
249 			 248 			 59
229 			 251 			 74
235 			 169 			 59
166 			 237 			 52
79 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [875/1000] took 96.80895495414734s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2090399528960944, train accuracy: 0.5801928133216476
Val mean loss: 1.7638489967439233, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2414 			 1998 			 1265
2097 			 2123 			 1211
2099 			 1966 			 1203
1640 			 1683 			 988
1426 			 1657 			 847
593 			 842 			 444
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 93
308 			 248 			 72
202 			 251 			 68
219 			 169 			 55
166 			 237 			 50
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3802628608
Epoch [876/1000] took 96.59484672546387s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.209672408126225, train accuracy: 0.5743499853929301
Val mean loss: 1.762486103104382, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2384 			 1998 			 1249
2149 			 2123 			 1218
2026 			 1966 			 1163
1633 			 1683 			 981
1481 			 1657 			 852
596 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 85
294 			 248 			 65
235 			 251 			 73
214 			 169 			 55
168 			 237 			 49
88 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [877/1000] took 97.09577965736389s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2103173140424806, train accuracy: 0.5781478235465966
Val mean loss: 1.7488958806526371, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2351 			 1998 			 1251
2140 			 2123 			 1228
2089 			 1966 			 1198
1628 			 1683 			 979
1463 			 1657 			 851
598 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 93
283 			 248 			 64
242 			 251 			 75
218 			 169 			 56
154 			 237 			 46
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [878/1000] took 96.35700011253357s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.208004421162828, train accuracy: 0.5767844970298958
Val mean loss: 1.7528874612436063, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2366 			 1998 			 1251
2131 			 2123 			 1218
2138 			 1966 			 1211
1591 			 1683 			 965
1436 			 1657 			 833
607 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 90
253 			 248 			 57
200 			 251 			 68
254 			 169 			 60
188 			 237 			 54
67 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [879/1000] took 96.99315524101257s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.208472899187391, train accuracy: 0.5772713993572889
Val mean loss: 1.7719189277509364, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2308 			 1998 			 1233
2151 			 2123 			 1230
2077 			 1966 			 1193
1653 			 1683 			 990
1481 			 1657 			 857
599 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 101
278 			 248 			 62
230 			 251 			 73
220 			 169 			 56
145 			 237 			 46
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845752320
Epoch [880/1000] took 96.94538950920105s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2110556900686935, train accuracy: 0.5767844970298958
Val mean loss: 1.7426648372557105, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2350 			 1998 			 1240
2171 			 2123 			 1228
2094 			 1966 			 1204
1622 			 1683 			 981
1451 			 1657 			 846
581 			 842 			 424
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
343 			 263 			 100
287 			 248 			 64
202 			 251 			 67
226 			 169 			 58
154 			 237 			 49
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [881/1000] took 96.90182781219482s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2096131421695246, train accuracy: 0.5767844970298958
Val mean loss: 1.7608742859305404, val accuracy: 0.29205607476635514

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2389 			 1998 			 1260
2177 			 2123 			 1239
2015 			 1966 			 1171
1646 			 1683 			 981
1439 			 1657 			 835
603 			 842 			 437
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
316 			 263 			 95
239 			 248 			 58
277 			 251 			 88
203 			 169 			 53
180 			 237 			 55
69 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [882/1000] took 97.03146362304688s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2094334230244717, train accuracy: 0.5811666179764339
Val mean loss: 1.762209505569644, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2393 			 1998 			 1280
2151 			 2123 			 1233
2051 			 1966 			 1184
1596 			 1683 			 970
1479 			 1657 			 867
599 			 842 			 434
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
265 			 263 			 77
257 			 248 			 60
284 			 251 			 84
239 			 169 			 57
160 			 237 			 50
79 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [883/1000] took 96.95651173591614s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2093517934421885, train accuracy: 0.5747395072548447
Val mean loss: 1.7641883855912743, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2316 			 1998 			 1237
2142 			 2123 			 1219
2130 			 1966 			 1199
1662 			 1683 			 984
1423 			 1657 			 833
596 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 93
252 			 248 			 59
242 			 251 			 78
219 			 169 			 55
180 			 237 			 52
69 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891759616
Epoch [884/1000] took 96.8866183757782s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2066348729846634, train accuracy: 0.5782452040120751
Val mean loss: 1.7615072901655988, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2410 			 1998 			 1262
2102 			 2123 			 1210
2062 			 1966 			 1192
1639 			 1683 			 980
1445 			 1657 			 846
611 			 842 			 448
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 87
309 			 248 			 69
242 			 251 			 76
206 			 169 			 52
160 			 237 			 49
64 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [885/1000] took 96.70216035842896s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2083168237751518, train accuracy: 0.5783425844775538
Val mean loss: 1.7466629947104104, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2401 			 1998 			 1268
2154 			 2123 			 1234
2059 			 1966 			 1185
1594 			 1683 			 970
1483 			 1657 			 853
578 			 842 			 429
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 84
241 			 248 			 56
283 			 251 			 85
229 			 169 			 56
171 			 237 			 50
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [886/1000] took 97.2123715877533s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2089629139855644, train accuracy: 0.5762975947025026
Val mean loss: 1.8045311555629824, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2338 			 1998 			 1233
2135 			 2123 			 1225
2105 			 1966 			 1192
1662 			 1683 			 988
1426 			 1657 			 842
603 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
330 			 263 			 98
283 			 248 			 67
240 			 251 			 78
205 			 169 			 54
160 			 237 			 48
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3800498688
Epoch [887/1000] took 96.35414481163025s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.207573782803485, train accuracy: 0.5761028337715455
Val mean loss: 1.7750010025210496, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2382 			 1998 			 1252
2115 			 2123 			 1204
2077 			 1966 			 1185
1636 			 1683 			 988
1459 			 1657 			 851
600 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 91
302 			 248 			 69
237 			 251 			 76
201 			 169 			 53
165 			 237 			 49
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [888/1000] took 97.28230953216553s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2086771002439696, train accuracy: 0.5781478235465966
Val mean loss: 1.7509524909461416, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2423 			 1998 			 1272
2113 			 2123 			 1222
2080 			 1966 			 1199
1611 			 1683 			 976
1463 			 1657 			 847
579 			 842 			 421
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 80
309 			 248 			 74
233 			 251 			 76
209 			 169 			 55
174 			 237 			 53
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [889/1000] took 96.86438059806824s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.206507461464665, train accuracy: 0.5783425844775538
Val mean loss: 1.7478935515008323, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2337 			 1998 			 1251
2234 			 2123 			 1262
2024 			 1966 			 1166
1617 			 1683 			 975
1451 			 1657 			 851
606 			 842 			 434
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 89
252 			 248 			 59
272 			 251 			 84
220 			 169 			 54
163 			 237 			 50
77 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [890/1000] took 97.69616198539734s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2103743814976415, train accuracy: 0.5793163891323401
Val mean loss: 1.7565611746253036, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2344 			 1998 			 1245
2118 			 2123 			 1228
2104 			 1966 			 1203
1646 			 1683 			 984
1462 			 1657 			 849
595 			 842 			 440
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 94
256 			 248 			 57
259 			 251 			 79
212 			 169 			 52
164 			 237 			 51
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891561984
Epoch [891/1000] took 97.32520961761475s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.206297994217026, train accuracy: 0.5767844970298958
Val mean loss: 1.754354386794858, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2345 			 1998 			 1234
2138 			 2123 			 1224
2112 			 1966 			 1198
1638 			 1683 			 987
1429 			 1657 			 844
607 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 95
249 			 248 			 60
223 			 251 			 72
241 			 169 			 59
177 			 237 			 51
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [892/1000] took 96.78080129623413s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2066525089035154, train accuracy: 0.5771740188918103
Val mean loss: 1.7558466399588235, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2354 			 1998 			 1241
2108 			 2123 			 1219
2092 			 1966 			 1190
1649 			 1683 			 997
1458 			 1657 			 841
608 			 842 			 439
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 82
273 			 248 			 59
225 			 251 			 75
235 			 169 			 61
187 			 237 			 55
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [893/1000] took 97.05483484268188s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2070407256530453, train accuracy: 0.5765897360989386
Val mean loss: 1.7765640223898538, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2342 			 1998 			 1249
2185 			 2123 			 1238
2051 			 1966 			 1174
1641 			 1683 			 980
1459 			 1657 			 846
591 			 842 			 434
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 92
300 			 248 			 71
242 			 251 			 76
206 			 169 			 50
160 			 237 			 47
67 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [894/1000] took 96.50726389884949s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2048100493035956, train accuracy: 0.5763949751679813
Val mean loss: 1.799061368151409, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2342 			 1998 			 1234
2113 			 2123 			 1216
2117 			 1966 			 1193
1634 			 1683 			 984
1480 			 1657 			 862
583 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 101
327 			 248 			 73
217 			 251 			 70
193 			 169 			 50
140 			 237 			 45
71 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3906997760
Epoch [895/1000] took 96.7717936038971s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2061547273044646, train accuracy: 0.5760054533060668
Val mean loss: 1.7673954934608647, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2383 			 1998 			 1245
2163 			 2123 			 1239
2074 			 1966 			 1182
1612 			 1683 			 975
1440 			 1657 			 838
597 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
276 			 263 			 81
293 			 248 			 63
236 			 251 			 76
216 			 169 			 54
189 			 237 			 58
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [896/1000] took 96.9336884021759s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2067960175769723, train accuracy: 0.5817509007693057
Val mean loss: 1.7718067663471873, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2326 			 1998 			 1242
2192 			 2123 			 1260
2044 			 1966 			 1186
1632 			 1683 			 983
1476 			 1657 			 866
599 			 842 			 437
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 92
290 			 248 			 69
242 			 251 			 77
210 			 169 			 53
162 			 237 			 49
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [897/1000] took 96.63839507102966s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2054912356944099, train accuracy: 0.5794137695978187
Val mean loss: 1.7614064187538334, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2353 			 1998 			 1256
2139 			 2123 			 1228
2094 			 1966 			 1202
1621 			 1683 			 975
1452 			 1657 			 843
610 			 842 			 446
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 93
256 			 248 			 59
259 			 251 			 81
225 			 169 			 55
161 			 237 			 49
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [898/1000] took 96.70456981658936s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2062186712045164, train accuracy: 0.5763949751679813
Val mean loss: 1.7810390809687173, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2339 			 1998 			 1254
2100 			 2123 			 1209
2112 			 1966 			 1190
1640 			 1683 			 986
1474 			 1657 			 850
604 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 88
285 			 248 			 63
242 			 251 			 78
220 			 169 			 55
174 			 237 			 51
65 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [899/1000] took 96.54960250854492s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2034632721422618, train accuracy: 0.5792190086668614
Val mean loss: 1.7708926055489518, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2318 			 1998 			 1241
2185 			 2123 			 1246
2050 			 1966 			 1183
1638 			 1683 			 984
1493 			 1657 			 869
585 			 842 			 425
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 91
290 			 248 			 68
265 			 251 			 79
199 			 169 			 52
149 			 237 			 45
70 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [900/1000] took 97.12144780158997s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2070247185564487, train accuracy: 0.5778556821501607
Val mean loss: 1.7827367927969955, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2394 			 1998 			 1263
2175 			 2123 			 1236
2080 			 1966 			 1190
1620 			 1683 			 979
1408 			 1657 			 835
592 			 842 			 431
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
299 			 263 			 89
278 			 248 			 64
236 			 251 			 77
229 			 169 			 57
168 			 237 			 53
74 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [901/1000] took 96.9232976436615s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.205307303002319, train accuracy: 0.5812639984419126
Val mean loss: 1.7973166238970872, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2320 			 1998 			 1257
2157 			 2123 			 1226
2083 			 1966 			 1200
1640 			 1683 			 988
1455 			 1657 			 850
614 			 842 			 448
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 97
254 			 248 			 61
242 			 251 			 77
220 			 169 			 56
166 			 237 			 46
80 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3851617792
Epoch [902/1000] took 96.70053124427795s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.205978257262447, train accuracy: 0.5784399649430324
Val mean loss: 1.7816584168410883, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2347 			 1998 			 1253
2135 			 2123 			 1225
2082 			 1966 			 1182
1660 			 1683 			 997
1445 			 1657 			 843
600 			 842 			 440
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
339 			 263 			 100
268 			 248 			 64
226 			 251 			 74
207 			 169 			 53
173 			 237 			 53
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3847390720
Epoch [903/1000] took 96.09324836730957s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2028574904548788, train accuracy: 0.578829486804947
Val mean loss: 1.7660373391174689, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2408 			 1998 			 1265
2161 			 2123 			 1233
2025 			 1966 			 1177
1642 			 1683 			 986
1436 			 1657 			 848
597 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 88
290 			 248 			 64
245 			 251 			 79
199 			 169 			 54
178 			 237 			 53
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3906997760
Epoch [904/1000] took 96.41806817054749s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.203976187750558, train accuracy: 0.5771740188918103
Val mean loss: 1.777154919577808, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2326 			 1998 			 1250
2113 			 2123 			 1201
2138 			 1966 			 1205
1627 			 1683 			 983
1466 			 1657 			 858
599 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
329 			 263 			 100
277 			 248 			 64
249 			 251 			 77
203 			 169 			 55
156 			 237 			 48
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891825152
Epoch [905/1000] took 96.9652009010315s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2060208342899785, train accuracy: 0.5784399649430324
Val mean loss: 1.7980633479792898, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2400 			 1998 			 1262
2154 			 2123 			 1232
2078 			 1966 			 1186
1631 			 1683 			 988
1432 			 1657 			 849
574 			 842 			 423
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
320 			 263 			 95
258 			 248 			 58
260 			 251 			 78
210 			 169 			 52
160 			 237 			 49
76 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [906/1000] took 97.33832955360413s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.20502795627184, train accuracy: 0.5771740188918103
Val mean loss: 1.7696843467107632, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2376 			 1998 			 1246
2182 			 2123 			 1241
2077 			 1966 			 1186
1610 			 1683 			 979
1439 			 1657 			 847
585 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 85
272 			 248 			 60
254 			 251 			 80
216 			 169 			 52
165 			 237 			 52
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [907/1000] took 97.15801620483398s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.203796411043387, train accuracy: 0.5742526049274516
Val mean loss: 1.773388551502693, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1234
2095 			 2123 			 1203
2120 			 1966 			 1198
1604 			 1683 			 975
1481 			 1657 			 851
598 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 84
298 			 248 			 66
234 			 251 			 75
230 			 169 			 55
160 			 237 			 50
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [908/1000] took 96.98005485534668s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.204643603797271, train accuracy: 0.5811666179764339
Val mean loss: 1.7783119911100806, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2365 			 1998 			 1261
2188 			 2123 			 1255
2028 			 1966 			 1178
1674 			 1683 			 998
1426 			 1657 			 846
588 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 93
244 			 248 			 55
273 			 251 			 81
203 			 169 			 50
172 			 237 			 52
81 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [909/1000] took 97.19749784469604s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.204828984455156, train accuracy: 0.5782452040120751
Val mean loss: 1.768191134057394, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2356 			 1998 			 1254
2089 			 2123 			 1216
2136 			 1966 			 1198
1617 			 1683 			 974
1463 			 1657 			 854
608 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
334 			 263 			 96
284 			 248 			 65
232 			 251 			 73
213 			 169 			 55
156 			 237 			 48
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [910/1000] took 96.60132265090942s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2028963248306346, train accuracy: 0.581653520303827
Val mean loss: 1.766471670895088, val accuracy: 0.28894080996884736

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2367 			 1998 			 1265
2132 			 2123 			 1231
2074 			 1966 			 1194
1660 			 1683 			 998
1439 			 1657 			 843
597 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 95
288 			 248 			 65
249 			 251 			 77
198 			 169 			 51
179 			 237 			 56
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [911/1000] took 96.91147923469543s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2059072757435736, train accuracy: 0.5791216282013828
Val mean loss: 1.775689860669578, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2371 			 1998 			 1250
2192 			 2123 			 1249
2053 			 1966 			 1186
1595 			 1683 			 979
1447 			 1657 			 842
611 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 86
265 			 248 			 60
240 			 251 			 76
237 			 169 			 60
180 			 237 			 53
64 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [912/1000] took 96.90724611282349s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.20365249664984, train accuracy: 0.5802901937871263
Val mean loss: 1.7804661640306798, val accuracy: 0.2749221183800623

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2355 			 1998 			 1257
2162 			 2123 			 1249
2095 			 1966 			 1190
1607 			 1683 			 980
1458 			 1657 			 848
592 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
310 			 263 			 89
304 			 248 			 64
197 			 251 			 68
239 			 169 			 56
166 			 237 			 49
68 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [913/1000] took 96.49562668800354s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2010693332859288, train accuracy: 0.5790242477359042
Val mean loss: 1.7659725008941278, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2345 			 1998 			 1246
2147 			 2123 			 1235
2026 			 1966 			 1175
1635 			 1683 			 983
1507 			 1657 			 868
609 			 842 			 439
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 91
296 			 248 			 67
280 			 251 			 80
198 			 169 			 52
133 			 237 			 42
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [914/1000] took 96.89311242103577s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2057955311466229, train accuracy: 0.5798032914597332
Val mean loss: 1.790136043618365, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2369 			 1998 			 1269
2125 			 2123 			 1215
2140 			 1966 			 1215
1626 			 1683 			 983
1418 			 1657 			 837
591 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 96
293 			 248 			 66
245 			 251 			 77
196 			 169 			 52
162 			 237 			 49
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [915/1000] took 97.40447568893433s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.203624124838927, train accuracy: 0.5796085305287759
Val mean loss: 1.7962580628511382, val accuracy: 0.2772585669781931

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2358 			 1998 			 1246
2158 			 2123 			 1240
2078 			 1966 			 1203
1629 			 1683 			 990
1454 			 1657 			 837
592 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 87
296 			 248 			 62
207 			 251 			 68
237 			 169 			 58
170 			 237 			 51
76 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3892316672
Epoch [916/1000] took 97.07641768455505s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.202819477360568, train accuracy: 0.5812639984419126
Val mean loss: 1.7598316640388676, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2322 			 1998 			 1233
2133 			 2123 			 1223
2062 			 1966 			 1193
1652 			 1683 			 994
1493 			 1657 			 885
607 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
356 			 263 			 105
254 			 248 			 58
257 			 251 			 80
206 			 169 			 54
148 			 237 			 48
63 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [917/1000] took 96.88453984260559s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2009558868928119, train accuracy: 0.5799980523906905
Val mean loss: 1.7680421951340466, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2431 			 1998 			 1268
2088 			 2123 			 1221
2113 			 1966 			 1207
1631 			 1683 			 991
1417 			 1657 			 834
589 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
300 			 263 			 91
298 			 248 			 64
223 			 251 			 72
223 			 169 			 55
166 			 237 			 48
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [918/1000] took 96.64657354354858s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2009073486580655, train accuracy: 0.5781478235465966
Val mean loss: 1.7694151227067156, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2348 			 1998 			 1249
2175 			 2123 			 1235
2059 			 1966 			 1180
1651 			 1683 			 991
1442 			 1657 			 849
594 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
315 			 263 			 92
277 			 248 			 59
218 			 251 			 73
217 			 169 			 55
181 			 237 			 54
76 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [919/1000] took 97.03528881072998s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2035730273181404, train accuracy: 0.5801928133216476
Val mean loss: 1.7662131931723617, val accuracy: 0.2780373831775701

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2357 			 1998 			 1254
2136 			 2123 			 1237
2055 			 1966 			 1184
1631 			 1683 			 978
1495 			 1657 			 869
595 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
311 			 263 			 93
292 			 248 			 65
248 			 251 			 77
206 			 169 			 52
146 			 237 			 44
81 			 116 			 26
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [920/1000] took 97.32439637184143s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2022400336844898, train accuracy: 0.5827247054240919
Val mean loss: 1.750110280223009, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2343 			 1998 			 1249
2187 			 2123 			 1257
2111 			 1966 			 1205
1577 			 1683 			 973
1433 			 1657 			 847
618 			 842 			 453
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 97
239 			 248 			 56
232 			 251 			 74
251 			 169 			 60
167 			 237 			 52
64 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3850012160
Epoch [921/1000] took 96.61599445343018s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2003011436105888, train accuracy: 0.5789268672704255
Val mean loss: 1.7671237573391054, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2446 			 1998 			 1272
2121 			 2123 			 1227
2063 			 1966 			 1190
1649 			 1683 			 981
1416 			 1657 			 845
574 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
273 			 263 			 81
290 			 248 			 62
230 			 251 			 74
224 			 169 			 58
188 			 237 			 59
79 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [922/1000] took 97.08644986152649s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.203210108562422, train accuracy: 0.5814587593728698
Val mean loss: 1.7520512575056495, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2302 			 1998 			 1239
2099 			 2123 			 1213
2069 			 1966 			 1201
1698 			 1683 			 1003
1503 			 1657 			 873
598 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 102
263 			 248 			 61
274 			 251 			 79
188 			 169 			 51
149 			 237 			 48
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845719552
Epoch [923/1000] took 97.06762957572937s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2027014128144284, train accuracy: 0.5771740188918103
Val mean loss: 1.773859358415371, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2367 			 1998 			 1246
2169 			 2123 			 1240
2096 			 1966 			 1197
1588 			 1683 			 971
1435 			 1657 			 835
614 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
286 			 263 			 85
263 			 248 			 59
259 			 251 			 83
227 			 169 			 57
176 			 237 			 53
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [924/1000] took 97.09142518043518s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2032113073399504, train accuracy: 0.5792190086668614
Val mean loss: 1.7698299012533047, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2378 			 1998 			 1273
2110 			 2123 			 1209
2090 			 1966 			 1189
1656 			 1683 			 993
1450 			 1657 			 851
585 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 83
284 			 248 			 63
241 			 251 			 75
233 			 169 			 59
167 			 237 			 52
78 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846080000
Epoch [925/1000] took 96.72219443321228s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2009478185407099, train accuracy: 0.5799006719252118
Val mean loss: 1.8061215906608394, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2315 			 1998 			 1241
2181 			 2123 			 1237
2028 			 1966 			 1174
1649 			 1683 			 989
1505 			 1657 			 873
591 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 87
250 			 248 			 58
277 			 251 			 84
227 			 169 			 56
158 			 237 			 48
80 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [926/1000] took 96.06931328773499s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1973878217634755, train accuracy: 0.5827247054240919
Val mean loss: 1.7908072704222144, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2319 			 1998 			 1246
2145 			 2123 			 1233
2076 			 1966 			 1194
1619 			 1683 			 993
1506 			 1657 			 882
604 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 95
264 			 248 			 60
259 			 251 			 81
214 			 169 			 55
142 			 237 			 43
88 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [927/1000] took 97.50955820083618s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2040228106522486, train accuracy: 0.5809718570454767
Val mean loss: 1.766563941792744, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2322 			 1998 			 1246
2178 			 2123 			 1245
2111 			 1966 			 1206
1638 			 1683 			 992
1424 			 1657 			 844
596 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
344 			 263 			 99
249 			 248 			 59
228 			 251 			 76
239 			 169 			 63
159 			 237 			 48
65 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3926167040
Epoch [928/1000] took 96.95029783248901s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2016919778886241, train accuracy: 0.5826273249586134
Val mean loss: 1.7798516401430455, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2422 			 1998 			 1273
2095 			 2123 			 1223
2036 			 1966 			 1182
1645 			 1683 			 1002
1466 			 1657 			 861
605 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
296 			 263 			 84
308 			 248 			 74
239 			 251 			 77
220 			 169 			 55
146 			 237 			 49
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [929/1000] took 97.07574582099915s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2021355495274624, train accuracy: 0.5815561398383484
Val mean loss: 1.7645386224839745, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2373 			 1998 			 1265
2183 			 2123 			 1243
2081 			 1966 			 1206
1615 			 1683 			 985
1427 			 1657 			 844
590 			 842 			 429
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 88
271 			 248 			 64
242 			 251 			 77
238 			 169 			 60
157 			 237 			 49
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846964736
Epoch [930/1000] took 96.32021951675415s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2005262202191576, train accuracy: 0.581945661700263
Val mean loss: 1.7904044098970366, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2373 			 1998 			 1264
2154 			 2123 			 1239
2031 			 1966 			 1178
1660 			 1683 			 1000
1461 			 1657 			 859
590 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 89
264 			 248 			 60
254 			 251 			 74
220 			 169 			 55
170 			 237 			 53
79 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [931/1000] took 97.1342191696167s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2031615853681, train accuracy: 0.5833089882169636
Val mean loss: 1.770191695632004, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2319 			 1998 			 1255
2124 			 2123 			 1230
2102 			 1966 			 1209
1628 			 1683 			 983
1487 			 1657 			 870
609 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 93
277 			 248 			 66
265 			 251 			 82
207 			 169 			 52
145 			 237 			 45
85 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [932/1000] took 96.86294746398926s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.201247743542692, train accuracy: 0.5834063686824423
Val mean loss: 1.7887943634172765, val accuracy: 0.27414330218068533

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2353 			 1998 			 1254
2167 			 2123 			 1253
2125 			 1966 			 1218
1619 			 1683 			 987
1406 			 1657 			 841
599 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
272 			 263 			 78
280 			 248 			 64
267 			 251 			 76
202 			 169 			 49
189 			 237 			 56
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [933/1000] took 97.19769620895386s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2012240803130319, train accuracy: 0.5822378030966988
Val mean loss: 1.7791474592394945, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2286 			 1998 			 1243
2123 			 2123 			 1232
2121 			 1966 			 1203
1642 			 1683 			 993
1496 			 1657 			 865
601 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 99
278 			 248 			 62
230 			 251 			 73
212 			 169 			 52
159 			 237 			 48
74 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [934/1000] took 97.0464289188385s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.200929863244945, train accuracy: 0.5814587593728698
Val mean loss: 1.7822363405692867, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2376 			 1998 			 1252
2112 			 2123 			 1228
2054 			 1966 			 1185
1645 			 1683 			 1000
1483 			 1657 			 871
599 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 93
285 			 248 			 66
247 			 251 			 77
201 			 169 			 51
174 			 237 			 51
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [935/1000] took 96.79067516326904s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1994951380376133, train accuracy: 0.584769695199143
Val mean loss: 1.7877315224670782, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2367 			 1998 			 1264
2148 			 2123 			 1242
2097 			 1966 			 1208
1588 			 1683 			 978
1473 			 1657 			 872
596 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 90
259 			 248 			 61
258 			 251 			 79
215 			 169 			 52
177 			 237 			 55
73 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3906997760
Epoch [936/1000] took 96.82258820533752s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1982966590893231, train accuracy: 0.5836011296133996
Val mean loss: 1.7795022609757214, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2367 			 1998 			 1258
2126 			 2123 			 1229
2100 			 1966 			 1211
1602 			 1683 			 981
1479 			 1657 			 874
595 			 842 			 440
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 93
264 			 248 			 61
231 			 251 			 73
235 			 169 			 61
163 			 237 			 49
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892414976
Epoch [937/1000] took 97.30825543403625s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1994210854869023, train accuracy: 0.5809718570454767
Val mean loss: 1.7779155678865386, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2397 			 1998 			 1273
2135 			 2123 			 1232
2075 			 1966 			 1188
1639 			 1683 			 986
1435 			 1657 			 851
588 			 842 			 436
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
306 			 263 			 89
277 			 248 			 63
232 			 251 			 73
219 			 169 			 54
177 			 237 			 53
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846374912
Epoch [938/1000] took 97.10882806777954s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1985717493797017, train accuracy: 0.5856461193884507
Val mean loss: 1.7761292312203385, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2367 			 1998 			 1271
2145 			 2123 			 1239
2044 			 1966 			 1206
1644 			 1683 			 996
1468 			 1657 			 858
601 			 842 			 444
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
302 			 263 			 88
273 			 248 			 62
245 			 251 			 76
210 			 169 			 55
184 			 237 			 56
70 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [939/1000] took 97.32095551490784s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2008684065854438, train accuracy: 0.5813613789073911
Val mean loss: 1.7745466319526113, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2370 			 1998 			 1266
2184 			 2123 			 1244
2030 			 1966 			 1185
1631 			 1683 			 987
1474 			 1657 			 859
580 			 842 			 429
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
295 			 263 			 87
259 			 248 			 58
252 			 251 			 80
229 			 169 			 57
175 			 237 			 55
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [940/1000] took 96.6839029788971s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1981610668782505, train accuracy: 0.5812639984419126
Val mean loss: 1.7669241253922625, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2343 			 1998 			 1246
2142 			 2123 			 1232
2091 			 1966 			 1204
1612 			 1683 			 977
1479 			 1657 			 865
602 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 94
278 			 248 			 62
244 			 251 			 76
216 			 169 			 55
153 			 237 			 47
68 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846538752
Epoch [941/1000] took 96.98847103118896s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1996015009478989, train accuracy: 0.5832116077514851
Val mean loss: 1.7467624850389434, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2400 			 1998 			 1263
2145 			 2123 			 1237
2077 			 1966 			 1200
1627 			 1683 			 999
1425 			 1657 			 853
595 			 842 			 437
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
258 			 263 			 79
308 			 248 			 68
249 			 251 			 75
209 			 169 			 55
171 			 237 			 53
89 			 116 			 33
Max memory allocated: 9096160768; Memory allocated: 3849487872
Epoch [942/1000] took 97.3273012638092s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.2008223708173567, train accuracy: 0.5846723147336644
Val mean loss: 1.7830902512480573, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2326 			 1998 			 1246
2116 			 2123 			 1229
2110 			 1966 			 1218
1616 			 1683 			 988
1483 			 1657 			 870
618 			 842 			 453
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 81
302 			 248 			 66
245 			 251 			 77
226 			 169 			 58
159 			 237 			 48
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [943/1000] took 96.62930583953857s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.198056745194943, train accuracy: 0.5828220858895705
Val mean loss: 1.7713116203866355, val accuracy: 0.2780373831775701

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2348 			 1998 			 1253
2155 			 2123 			 1240
2045 			 1966 			 1191
1661 			 1683 			 995
1465 			 1657 			 863
595 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
279 			 263 			 80
299 			 248 			 65
242 			 251 			 75
211 			 169 			 53
178 			 237 			 56
75 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [944/1000] took 96.8346266746521s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.197587043696846, train accuracy: 0.5825299444931347
Val mean loss: 1.7765265674125859, val accuracy: 0.2772585669781931

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2369 			 1998 			 1258
2159 			 2123 			 1244
2076 			 1966 			 1190
1627 			 1683 			 984
1439 			 1657 			 861
599 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 82
300 			 248 			 67
260 			 251 			 78
206 			 169 			 51
162 			 237 			 50
75 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [945/1000] took 96.92857074737549s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1975364649778586, train accuracy: 0.5783425844775538
Val mean loss: 1.7634979166635654, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2336 			 1998 			 1238
2176 			 2123 			 1229
2063 			 1966 			 1182
1625 			 1683 			 980
1451 			 1657 			 864
618 			 842 			 446
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 89
265 			 248 			 62
259 			 251 			 80
228 			 169 			 56
162 			 237 			 48
73 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [946/1000] took 97.04557633399963s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.198822173559777, train accuracy: 0.5821404226312201
Val mean loss: 1.7641350903162143, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2324 			 1998 			 1249
2140 			 2123 			 1237
2087 			 1966 			 1201
1656 			 1683 			 986
1481 			 1657 			 870
581 			 842 			 435
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 93
268 			 248 			 65
255 			 251 			 80
215 			 169 			 53
148 			 237 			 48
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [947/1000] took 96.49025535583496s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1953616218403493, train accuracy: 0.5832116077514851
Val mean loss: 1.7541177330947504, val accuracy: 0.2881619937694704

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2310 			 1998 			 1243
2182 			 2123 			 1256
2120 			 1966 			 1213
1600 			 1683 			 975
1436 			 1657 			 848
621 			 842 			 454
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
348 			 263 			 104
244 			 248 			 56
229 			 251 			 74
233 			 169 			 59
156 			 237 			 48
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892152832
Epoch [948/1000] took 97.01384496688843s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.198092280332916, train accuracy: 0.5833089882169636
Val mean loss: 1.7793598436727756, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2383 			 1998 			 1259
2087 			 2123 			 1224
2074 			 1966 			 1197
1637 			 1683 			 996
1489 			 1657 			 873
599 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 99
266 			 248 			 62
251 			 251 			 76
214 			 169 			 55
139 			 237 			 44
73 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3806888448
Epoch [949/1000] took 97.27978658676147s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1989123975376474, train accuracy: 0.5787321063394683
Val mean loss: 1.7571736632323847, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2382 			 1998 			 1259
2157 			 2123 			 1232
2138 			 1966 			 1203
1591 			 1683 			 969
1400 			 1657 			 838
601 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 91
234 			 248 			 53
252 			 251 			 77
231 			 169 			 59
177 			 237 			 55
77 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [950/1000] took 96.88320970535278s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1962546962443914, train accuracy: 0.581653520303827
Val mean loss: 1.797020673751831, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2392 			 1998 			 1267
2115 			 2123 			 1216
2053 			 1966 			 1192
1628 			 1683 			 987
1465 			 1657 			 860
616 			 842 			 451
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 88
241 			 248 			 55
260 			 251 			 80
231 			 169 			 58
167 			 237 			 49
73 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3846211072
Epoch [951/1000] took 96.56472754478455s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1974728876185194, train accuracy: 0.5810692375109553
Val mean loss: 1.7551936347310135, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2341 			 1998 			 1243
2090 			 2123 			 1217
2093 			 1966 			 1206
1649 			 1683 			 997
1494 			 1657 			 861
602 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
287 			 263 			 82
319 			 248 			 73
221 			 251 			 73
222 			 169 			 55
157 			 237 			 49
78 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [952/1000] took 96.50609064102173s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1952418356298287, train accuracy: 0.5828220858895705
Val mean loss: 1.759937335805195, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2312 			 1998 			 1247
2173 			 2123 			 1254
2053 			 1966 			 1187
1621 			 1683 			 986
1484 			 1657 			 857
626 			 842 			 454
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
350 			 263 			 102
268 			 248 			 63
220 			 251 			 71
223 			 169 			 55
153 			 237 			 47
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3811148288
Epoch [953/1000] took 96.63989543914795s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1969986112318307, train accuracy: 0.5831142272860065
Val mean loss: 1.7796995959630826, val accuracy: 0.2772585669781931

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2397 			 1998 			 1266
2160 			 2123 			 1246
2071 			 1966 			 1193
1640 			 1683 			 992
1430 			 1657 			 863
571 			 842 			 428
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
294 			 263 			 85
279 			 248 			 65
242 			 251 			 73
218 			 169 			 53
177 			 237 			 53
74 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [954/1000] took 97.04692196846008s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1984516497341644, train accuracy: 0.5862304021813224
Val mean loss: 1.7868465301467151, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2333 			 1998 			 1270
2159 			 2123 			 1246
2074 			 1966 			 1198
1627 			 1683 			 986
1459 			 1657 			 870
617 			 842 			 450
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
323 			 263 			 98
259 			 248 			 58
245 			 251 			 80
232 			 169 			 59
151 			 237 			 48
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892120064
Epoch [955/1000] took 97.33289575576782s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1982619641725891, train accuracy: 0.5818482812347843
Val mean loss: 1.7573009496781884, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2364 			 1998 			 1256
2090 			 2123 			 1219
2096 			 1966 			 1206
1633 			 1683 			 983
1476 			 1657 			 867
610 			 842 			 444
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 92
295 			 248 			 68
205 			 251 			 67
230 			 169 			 57
168 			 237 			 52
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [956/1000] took 96.92625331878662s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1955766100378422, train accuracy: 0.5851592170610576
Val mean loss: 1.7764873213884307, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2303 			 1998 			 1242
2190 			 2123 			 1261
2080 			 1966 			 1206
1618 			 1683 			 990
1476 			 1657 			 866
602 			 842 			 444
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
358 			 263 			 103
252 			 248 			 58
199 			 251 			 69
236 			 169 			 59
167 			 237 			 51
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [957/1000] took 96.78296232223511s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1952475331654058, train accuracy: 0.5837958905443568
Val mean loss: 1.7523590151856585, val accuracy: 0.28738317757009346

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2326 			 1998 			 1249
2190 			 2123 			 1245
2082 			 1966 			 1203
1635 			 1683 			 996
1440 			 1657 			 857
596 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
336 			 263 			 99
236 			 248 			 55
232 			 251 			 74
211 			 169 			 56
197 			 237 			 56
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [958/1000] took 96.8123676776886s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1960061352200968, train accuracy: 0.5830168468205278
Val mean loss: 1.7893002091384516, val accuracy: 0.2780373831775701

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2386 			 1998 			 1252
2115 			 2123 			 1230
2049 			 1966 			 1197
1621 			 1683 			 985
1495 			 1657 			 878
603 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
258 			 263 			 73
279 			 248 			 64
246 			 251 			 77
241 			 169 			 60
175 			 237 			 53
85 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [959/1000] took 97.04109501838684s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1946496224477654, train accuracy: 0.5872042068361086
Val mean loss: 1.7668252311101773, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2335 			 1998 			 1260
2154 			 2123 			 1252
2088 			 1966 			 1213
1638 			 1683 			 995
1431 			 1657 			 856
623 			 842 			 454
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 86
250 			 248 			 58
270 			 251 			 82
212 			 169 			 54
188 			 237 			 58
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3891661312
Epoch [960/1000] took 97.1730010509491s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1955674390926538, train accuracy: 0.5853539779920148
Val mean loss: 1.7568914599534942, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2335 			 1998 			 1258
2127 			 2123 			 1241
2082 			 1966 			 1205
1668 			 1683 			 1000
1461 			 1657 			 861
596 			 842 			 446
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 85
235 			 248 			 56
253 			 251 			 77
243 			 169 			 58
190 			 237 			 57
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892185600
Epoch [961/1000] took 97.0118498802185s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.196377814187439, train accuracy: 0.5815561398383484
Val mean loss: 1.7726120512659957, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2380 			 1998 			 1256
2072 			 2123 			 1211
2049 			 1966 			 1191
1667 			 1683 			 1004
1507 			 1657 			 876
594 			 842 			 434
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
293 			 263 			 86
320 			 248 			 71
247 			 251 			 73
202 			 169 			 51
154 			 237 			 48
68 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3850536448
Epoch [962/1000] took 96.49473285675049s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1934938493918779, train accuracy: 0.5882753919563736
Val mean loss: 1.767092797814346, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2366 			 1998 			 1277
2165 			 2123 			 1252
2077 			 1966 			 1212
1621 			 1683 			 995
1438 			 1657 			 862
602 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
284 			 263 			 84
285 			 248 			 65
264 			 251 			 79
210 			 169 			 53
166 			 237 			 51
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [963/1000] took 96.95618557929993s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1921879157099025, train accuracy: 0.5873015873015873
Val mean loss: 1.7670685023796269, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2310 			 1998 			 1252
2158 			 2123 			 1253
2100 			 1966 			 1211
1596 			 1683 			 988
1479 			 1657 			 862
626 			 842 			 465
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
318 			 263 			 91
269 			 248 			 62
249 			 251 			 77
228 			 169 			 57
155 			 237 			 47
65 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [964/1000] took 97.00362539291382s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.196920255448588, train accuracy: 0.5853539779920148
Val mean loss: 1.7650570026258143, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2364 			 1998 			 1268
2147 			 2123 			 1247
2072 			 1966 			 1204
1629 			 1683 			 991
1472 			 1657 			 869
585 			 842 			 432
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
297 			 263 			 85
283 			 248 			 66
246 			 251 			 76
226 			 169 			 58
161 			 237 			 47
71 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3823927808
Epoch [965/1000] took 96.2985212802887s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.192716899504914, train accuracy: 0.5840880319407927
Val mean loss: 1.7738390899286038, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2320 			 1998 			 1248
2145 			 2123 			 1241
2046 			 1966 			 1189
1670 			 1683 			 1007
1483 			 1657 			 868
605 			 842 			 445
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
327 			 263 			 95
259 			 248 			 62
252 			 251 			 81
209 			 169 			 54
164 			 237 			 51
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892218368
Epoch [966/1000] took 96.4220495223999s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1937102049682, train accuracy: 0.5846723147336644
Val mean loss: 1.7908703030609503, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2415 			 1998 			 1273
2114 			 2123 			 1229
2065 			 1966 			 1199
1617 			 1683 			 983
1466 			 1657 			 882
592 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
272 			 263 			 83
315 			 248 			 69
232 			 251 			 72
215 			 169 			 53
175 			 237 			 56
75 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [967/1000] took 96.51078844070435s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1936579173970445, train accuracy: 0.5828220858895705
Val mean loss: 1.8012623961378889, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2314 			 1998 			 1241
2196 			 2123 			 1253
2086 			 1966 			 1203
1598 			 1683 			 980
1466 			 1657 			 860
609 			 842 			 448
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 91
291 			 248 			 66
228 			 251 			 74
225 			 169 			 58
143 			 237 			 47
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [968/1000] took 97.13552236557007s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.193849865521226, train accuracy: 0.5875937286980232
Val mean loss: 1.8041301878487193, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2358 			 1998 			 1266
2118 			 2123 			 1244
2104 			 1966 			 1221
1632 			 1683 			 991
1466 			 1657 			 871
591 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 86
301 			 248 			 69
235 			 251 			 74
221 			 169 			 56
159 			 237 			 51
79 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3890513408
Epoch [969/1000] took 97.08328413963318s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1951422568793608, train accuracy: 0.585840880319408
Val mean loss: 1.8066149891876593, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2340 			 1998 			 1256
2180 			 2123 			 1266
2063 			 1966 			 1198
1617 			 1683 			 988
1482 			 1657 			 870
587 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
308 			 263 			 89
263 			 248 			 60
253 			 251 			 77
234 			 169 			 58
153 			 237 			 45
73 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846899200
Epoch [970/1000] took 96.83618378639221s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1959429548536877, train accuracy: 0.5830168468205278
Val mean loss: 1.758672676435331, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2336 			 1998 			 1245
2103 			 2123 			 1229
2083 			 1966 			 1196
1666 			 1683 			 1002
1480 			 1657 			 873
601 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 87
284 			 248 			 65
249 			 251 			 79
207 			 169 			 52
172 			 237 			 54
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3815408128
Epoch [971/1000] took 97.09976840019226s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1947463433319163, train accuracy: 0.583990651475314
Val mean loss: 1.787888526916504, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2352 			 1998 			 1255
2191 			 2123 			 1254
2067 			 1966 			 1198
1597 			 1683 			 979
1454 			 1657 			 869
608 			 842 			 442
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
312 			 263 			 90
259 			 248 			 59
240 			 251 			 78
235 			 169 			 61
159 			 237 			 50
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846833664
Epoch [972/1000] took 96.97026467323303s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1915868260778741, train accuracy: 0.584769695199143
Val mean loss: 1.7863236927404635, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2382 			 1998 			 1267
2096 			 2123 			 1226
2118 			 1966 			 1216
1622 			 1683 			 991
1450 			 1657 			 854
601 			 842 			 451
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
260 			 263 			 76
304 			 248 			 66
228 			 251 			 75
239 			 169 			 57
179 			 237 			 56
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [973/1000] took 97.49153757095337s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1954473919957598, train accuracy: 0.5856461193884507
Val mean loss: 1.7734377035280553, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2363 			 1998 			 1265
2138 			 2123 			 1243
2084 			 1966 			 1204
1642 			 1683 			 998
1449 			 1657 			 863
593 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
321 			 263 			 92
309 			 248 			 71
221 			 251 			 72
207 			 169 			 54
156 			 237 			 50
70 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3926167040
Epoch [974/1000] took 96.32030367851257s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1898003034873914, train accuracy: 0.5909046645242965
Val mean loss: 1.776233268947136, val accuracy: 0.2897196261682243

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2307 			 1998 			 1255
2197 			 2123 			 1275
2047 			 1966 			 1204
1596 			 1683 			 995
1516 			 1657 			 893
606 			 842 			 446
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 98
246 			 248 			 58
238 			 251 			 77
235 			 169 			 60
159 			 237 			 50
78 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846702592
Epoch [975/1000] took 97.20632028579712s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.193636600287904, train accuracy: 0.584769695199143
Val mean loss: 1.7889274882107247, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2341 			 1998 			 1257
2125 			 2123 			 1236
2084 			 1966 			 1197
1632 			 1683 			 997
1469 			 1657 			 863
618 			 842 			 455
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
304 			 263 			 90
290 			 248 			 65
256 			 251 			 76
223 			 169 			 56
142 			 237 			 44
69 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [976/1000] took 96.95983242988586s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1920641530711331, train accuracy: 0.5840880319407927
Val mean loss: 1.7789090551981113, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2385 			 1998 			 1270
2161 			 2123 			 1259
2069 			 1966 			 1190
1617 			 1683 			 989
1443 			 1657 			 842
594 			 842 			 448
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
309 			 263 			 89
258 			 248 			 57
247 			 251 			 77
219 			 169 			 58
181 			 237 			 55
70 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [977/1000] took 96.88727688789368s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1929706121530859, train accuracy: 0.5836011296133996
Val mean loss: 1.7623181750134724, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2348 			 1998 			 1250
2085 			 2123 			 1222
2064 			 1966 			 1200
1680 			 1683 			 1005
1494 			 1657 			 875
598 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
303 			 263 			 92
283 			 248 			 67
269 			 251 			 78
180 			 169 			 48
170 			 237 			 53
79 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3892546048
Epoch [978/1000] took 96.64343190193176s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1932527827696637, train accuracy: 0.5855487389229721
Val mean loss: 1.764924092990596, val accuracy: 0.2819314641744548

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2384 			 1998 			 1275
2166 			 2123 			 1249
2081 			 1966 			 1203
1575 			 1683 			 968
1453 			 1657 			 871
610 			 842 			 447
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
277 			 263 			 82
265 			 248 			 58
248 			 251 			 77
245 			 169 			 60
179 			 237 			 56
70 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3891726848
Epoch [979/1000] took 96.48129796981812s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1908270854073522, train accuracy: 0.5856461193884507
Val mean loss: 1.7884877949226192, val accuracy: 0.2827102803738318

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2304 			 1998 			 1248
2138 			 2123 			 1240
2034 			 1966 			 1189
1694 			 1683 			 1017
1486 			 1657 			 873
613 			 842 			 447
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
317 			 263 			 93
305 			 248 			 69
253 			 251 			 75
186 			 169 			 49
157 			 237 			 49
66 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3817538048
Epoch [980/1000] took 96.91804623603821s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1919252181721625, train accuracy: 0.5853539779920148
Val mean loss: 1.7897585368737943, val accuracy: 0.2834890965732087

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2296 			 1998 			 1243
2119 			 2123 			 1233
2125 			 1966 			 1213
1597 			 1683 			 978
1522 			 1657 			 887
610 			 842 			 457
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
333 			 263 			 94
287 			 248 			 68
240 			 251 			 76
207 			 169 			 52
151 			 237 			 47
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3846800896
Epoch [981/1000] took 97.31302213668823s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1949163014272293, train accuracy: 0.5836985100788782
Val mean loss: 1.7701999559635069, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2388 			 1998 			 1263
2161 			 2123 			 1246
2078 			 1966 			 1204
1638 			 1683 			 991
1419 			 1657 			 852
585 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
292 			 263 			 85
303 			 248 			 67
228 			 251 			 72
212 			 169 			 56
167 			 237 			 52
82 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3855877632
Epoch [982/1000] took 96.70272326469421s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1913130457898908, train accuracy: 0.5859382607848865
Val mean loss: 1.772479970280717, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2340 			 1998 			 1263
2136 			 2123 			 1242
2049 			 1966 			 1189
1659 			 1683 			 995
1475 			 1657 			 882
610 			 842 			 446
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 93
274 			 248 			 62
258 			 251 			 76
209 			 169 			 51
164 			 237 			 51
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3821797888
Epoch [983/1000] took 96.77218341827393s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1921388603445153, train accuracy: 0.5863277826468011
Val mean loss: 1.76743249195378, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2317 			 1998 			 1254
2162 			 2123 			 1246
2095 			 1966 			 1208
1601 			 1683 			 977
1464 			 1657 			 870
630 			 842 			 466
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
331 			 263 			 95
284 			 248 			 66
205 			 251 			 70
245 			 169 			 61
148 			 237 			 43
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [984/1000] took 96.62483859062195s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1925532162375168, train accuracy: 0.5868146849741942
Val mean loss: 1.767177369536423, val accuracy: 0.279595015576324

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2357 			 1998 			 1259
2154 			 2123 			 1253
2043 			 1966 			 1195
1687 			 1683 			 1018
1435 			 1657 			 863
593 			 842 			 438
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
288 			 263 			 83
318 			 248 			 68
212 			 251 			 70
207 			 169 			 55
181 			 237 			 54
78 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3845391872
Epoch [985/1000] took 97.01348662376404s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1932888151700622, train accuracy: 0.5885675333528094
Val mean loss: 1.7991686536044609, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2291 			 1998 			 1250
2216 			 2123 			 1283
2068 			 1966 			 1208
1604 			 1683 			 981
1478 			 1657 			 870
612 			 842 			 452
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
328 			 263 			 98
230 			 248 			 54
258 			 251 			 78
203 			 169 			 49
193 			 237 			 58
72 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3845883392
Epoch [986/1000] took 97.00650215148926s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1912840160996743, train accuracy: 0.5866199240432369
Val mean loss: 1.7954315441410715, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2375 			 1998 			 1269
2096 			 2123 			 1234
2077 			 1966 			 1214
1609 			 1683 			 987
1503 			 1657 			 877
609 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
281 			 263 			 84
322 			 248 			 71
236 			 251 			 75
218 			 169 			 54
152 			 237 			 47
75 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846768128
Epoch [987/1000] took 96.87142205238342s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.193804126476573, train accuracy: 0.5874963482325446
Val mean loss: 1.7874585070261142, val accuracy: 0.2842679127725857

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2370 			 1998 			 1269
2186 			 2123 			 1265
2021 			 1966 			 1186
1616 			 1683 			 993
1481 			 1657 			 877
595 			 842 			 443
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
289 			 263 			 86
261 			 248 			 62
269 			 251 			 82
223 			 169 			 54
159 			 237 			 49
83 			 116 			 32
Max memory allocated: 9096160768; Memory allocated: 3891694080
Epoch [988/1000] took 96.94363760948181s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.190143666349096, train accuracy: 0.5881780114908949
Val mean loss: 1.7780173638971841, val accuracy: 0.278816199376947

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2347 			 1998 			 1268
2140 			 2123 			 1249
2092 			 1966 			 1216
1619 			 1683 			 987
1471 			 1657 			 879
600 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
285 			 263 			 80
305 			 248 			 69
247 			 251 			 79
227 			 169 			 59
144 			 237 			 42
76 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846014464
Epoch [989/1000] took 97.17559170722961s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1916971271654526, train accuracy: 0.5874963482325446
Val mean loss: 1.7797232430155685, val accuracy: 0.29127725856697817

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2237 			 1998 			 1233
2262 			 2123 			 1283
2057 			 1966 			 1202
1651 			 1683 			 1001
1444 			 1657 			 862
618 			 842 			 452
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
341 			 263 			 102
206 			 248 			 51
284 			 251 			 85
223 			 169 			 57
161 			 237 			 51
69 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892349440
Epoch [990/1000] took 96.73017907142639s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1908088060925683, train accuracy: 0.5865225435777583
Val mean loss: 1.7849956547341697, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2373 			 1998 			 1267
2083 			 2123 			 1227
2130 			 1966 			 1217
1609 			 1683 			 983
1458 			 1657 			 873
616 			 842 			 456
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
335 			 263 			 97
263 			 248 			 56
230 			 251 			 74
227 			 169 			 55
163 			 237 			 51
66 			 116 			 27
Max memory allocated: 9096160768; Memory allocated: 3819667968
Epoch [991/1000] took 97.11384582519531s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.191763457665191, train accuracy: 0.5843801733372286
Val mean loss: 1.7748651184686801, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2369 			 1998 			 1264
2153 			 2123 			 1249
2073 			 1966 			 1200
1626 			 1683 			 988
1477 			 1657 			 870
571 			 842 			 430
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
298 			 263 			 85
266 			 248 			 60
232 			 251 			 75
231 			 169 			 59
170 			 237 			 52
87 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3847357952
Epoch [992/1000] took 96.6727466583252s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1900297477609272, train accuracy: 0.5851592170610576
Val mean loss: 1.7665549691130475, val accuracy: 0.2811526479750779

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2342 			 1998 			 1263
2172 			 2123 			 1257
2025 			 1966 			 1183
1630 			 1683 			 997
1474 			 1657 			 857
626 			 842 			 452
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
322 			 263 			 94
306 			 248 			 66
225 			 251 			 72
208 			 169 			 52
151 			 237 			 47
72 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3846047232
Epoch [993/1000] took 97.00836181640625s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1902800753851916, train accuracy: 0.5825299444931347
Val mean loss: 1.758402993039387, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2326 			 1998 			 1247
2174 			 2123 			 1232
2101 			 1966 			 1215
1647 			 1683 			 1002
1441 			 1657 			 853
580 			 842 			 433
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
340 			 263 			 102
246 			 248 			 57
247 			 251 			 78
203 			 169 			 52
164 			 237 			 49
84 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3809018368
Epoch [994/1000] took 96.84678030014038s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1899047592347285, train accuracy: 0.588664913818288
Val mean loss: 1.7886998188204881, val accuracy: 0.2850467289719626

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2348 			 1998 			 1262
2105 			 2123 			 1236
2089 			 1966 			 1212
1632 			 1683 			 1001
1489 			 1657 			 881
606 			 842 			 453
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
325 			 263 			 99
294 			 248 			 63
219 			 251 			 69
210 			 169 			 56
160 			 237 			 48
76 			 116 			 31
Max memory allocated: 9096160768; Memory allocated: 3891595776
Epoch [995/1000] took 96.58553838729858s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1900070849980149, train accuracy: 0.5857434998539293
Val mean loss: 1.8005372721974442, val accuracy: 0.28582554517133957

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2439 			 1998 			 1292
2140 			 2123 			 1244
2058 			 1966 			 1203
1648 			 1683 			 998
1384 			 1657 			 839
600 			 842 			 439
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
305 			 263 			 88
280 			 248 			 62
235 			 251 			 75
208 			 169 			 54
185 			 237 			 58
71 			 116 			 30
Max memory allocated: 9096160768; Memory allocated: 3845555712
Epoch [996/1000] took 96.9293417930603s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1910500270183955, train accuracy: 0.5880806310254163
Val mean loss: 1.8031460628276919, val accuracy: 0.2764797507788162

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2332 			 1998 			 1263
2138 			 2123 			 1246
2022 			 1966 			 1193
1647 			 1683 			 994
1508 			 1657 			 881
622 			 842 			 462
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
287 			 263 			 83
307 			 248 			 68
244 			 251 			 75
199 			 169 			 52
161 			 237 			 48
86 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846997504
Epoch [997/1000] took 96.69858741760254s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1884747741749724, train accuracy: 0.5849644561301003
Val mean loss: 1.7992372541892818, val accuracy: 0.2803738317757009

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2321 			 1998 			 1248
2208 			 2123 			 1265
2046 			 1966 			 1198
1606 			 1683 			 976
1464 			 1657 			 862
624 			 842 			 458
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
267 			 263 			 77
309 			 248 			 70
246 			 251 			 78
233 			 169 			 58
155 			 237 			 49
74 			 116 			 28
Max memory allocated: 9096160768; Memory allocated: 3892316672
Epoch [998/1000] took 96.4196264743805s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1884957286427698, train accuracy: 0.5814587593728698
Val mean loss: 1.7703532969079367, val accuracy: 0.29049844236760125

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2264 			 1998 			 1228
2207 			 2123 			 1251
2109 			 1966 			 1210
1618 			 1683 			 983
1473 			 1657 			 858
598 			 842 			 441
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
337 			 263 			 101
259 			 248 			 61
226 			 251 			 74
235 			 169 			 59
153 			 237 			 49
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3813278208
Epoch [999/1000] took 96.84423875808716s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1909946554175048, train accuracy: 0.5842827928717499
Val mean loss: 1.7728564041416819, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2380 			 1998 			 1261
2113 			 2123 			 1228
2088 			 1966 			 1205
1633 			 1683 			 995
1440 			 1657 			 859
615 			 842 			 452
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
313 			 263 			 94
269 			 248 			 61
254 			 251 			 79
211 			 169 			 54
163 			 237 			 51
74 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3853747712
Epoch [1000/1000] took 96.69418931007385s
Experiment configuration: {'LM': 'LLAMA 2 7B', 'HUGGINGFACE_IMPLEMENTATION': 'AutoModel', 'CLF_HEAD': 'SimplestLinearHead', 'FREEZE_LM': True, 'BATCH_SIZE': 32, 'NUM_EPOCHS': 1000, 'EARLY_STOPPING_AFTER': 'NEVER', 'LEARNING_RATE': 1e-05, 'OPTIMIZER': 'Adam', 'QUANTIZATION': True, 'DATASET': 'Liar', 'DATA_FRAC': 1, 'KEEP_COLUMNS': ['statement', 'label'], 'NUM_CLASSES': 6, 'LABEL_MAPPING': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}}
Train mean loss: 1.1886899751666178, train accuracy: 0.5882753919563736
Val mean loss: 1.7955160867877122, val accuracy: 0.2866043613707165

TRAINING:
Labels predicted: 	 True targets: 	 Correct labels:
2281 			 1998 			 1253
2249 			 2123 			 1273
2076 			 1966 			 1207
1599 			 1683 			 987
1464 			 1657 			 872
600 			 842 			 449
VALIDATION:
Labels predicted: 	 True targets: 	 Correct labels:
345 			 263 			 98
233 			 248 			 55
223 			 251 			 74
242 			 169 			 59
169 			 237 			 53
72 			 116 			 29
Max memory allocated: 9096160768; Memory allocated: 3846211072
Training finished! Training for 1000 epochs took: 97155.57022953033s
Training loss: [1.4365913355461906, 1.435703537545843, 1.43506834737237, 1.4341967206134973, 1.4360991326447958, 1.4336421797951433, 1.433626692243083, 1.4338740218094206, 1.43250511145666, 1.4323560348552336, 1.4327597146465028, 1.4303115613735353, 1.4331644327841073, 1.430407221814925, 1.430299490411705, 1.4312263288973277, 1.4282409428064697, 1.4283220753120114, 1.4296273137921485, 1.4288406468626123, 1.4279460335074927, 1.4297481637877467, 1.42830003013492, 1.4255833306416543, 1.4249343114478565, 1.4258602345101188, 1.4263945138343026, 1.424318101175849, 1.4260116938118623, 1.423292985957731, 1.4251288777199862, 1.4242150311900819, 1.4240526740796098, 1.4229348834801314, 1.423986332067448, 1.4226903146672472, 1.4243264090606356, 1.4217134955516113, 1.4196872874583781, 1.4217172786825543, 1.4207113758425847, 1.4206001561750132, 1.4213918503199783, 1.419940192751424, 1.4179500066602713, 1.4168172981880165, 1.4166834469524872, 1.417288467148754, 1.4178002731079624, 1.417371567907363, 1.417092191467404, 1.4148451899442347, 1.4154200802710941, 1.4158050397474817, 1.4157992846497864, 1.4157865857038172, 1.4131522791407933, 1.4139460789451719, 1.4133475302164429, 1.413488994877658, 1.4139416563176663, 1.4117384970745193, 1.4122225703480087, 1.4103594691582557, 1.4114617521517745, 1.4114420057457184, 1.4105091488621317, 1.4073248366329159, 1.4108667340234062, 1.4084481026896063, 1.409286273602756, 1.4079326502630645, 1.407204565600814, 1.4089079066600383, 1.4080676479131633, 1.4078616130389157, 1.4085535984544368, 1.4067043385401694, 1.4057047660477064, 1.4054537495348685, 1.4044680896206436, 1.4034730311123382, 1.4043445446038172, 1.4036455496077969, 1.4015424262697451, 1.4004269916320515, 1.4032263183890845, 1.4053098182440547, 1.4017063267877168, 1.4019323787956595, 1.4023414723969694, 1.401609337961191, 1.4023706515630086, 1.4004139254026324, 1.4001517039593134, 1.4001494582939742, 1.400598087414774, 1.4000724854870377, 1.3980313305542849, 1.3983092530865535, 1.3981336254939856, 1.399999337032948, 1.3956498910333508, 1.3960407656672587, 1.39640392692661, 1.3946614302578746, 1.3974762649922357, 1.3941980492660189, 1.395068557091591, 1.3947292031528793, 1.394229130581532, 1.3921015685963853, 1.3945644536865092, 1.393401436345228, 1.3939856584198378, 1.3921598600821332, 1.3933348633418574, 1.3908737344533855, 1.390213884668558, 1.3908991902788108, 1.3898942270011545, 1.3889810533909783, 1.3913476474559938, 1.3881027208310421, 1.3897143331272208, 1.3876710179438843, 1.3896831028929382, 1.3872397740681965, 1.3892856825177915, 1.387533265482228, 1.3883678905689085, 1.3860992412329463, 1.3858503794001642, 1.3868672684345662, 1.386967657511108, 1.3875314799424643, 1.3871719161298044, 1.3849595796282046, 1.386140266311503, 1.3834714284195706, 1.3850929012922484, 1.3837241030927758, 1.3837145726628763, 1.3848334840524976, 1.3818079248024295, 1.3827388661672764, 1.382669794225247, 1.3815987084142145, 1.3810750268330083, 1.3798109853750449, 1.3790029101654004, 1.3779856453060733, 1.3829569441507168, 1.3812671747534446, 1.3778698737747572, 1.379896085953044, 1.3780238115898917, 1.3808358168676262, 1.3773990375601985, 1.3770222537614103, 1.3781678617186264, 1.3765907031353388, 1.375873615065839, 1.377441072761084, 1.376927034134434, 1.3751790991453368, 1.3746439103396881, 1.3752263008248398, 1.3743628397166172, 1.375814105120032, 1.3748104044952867, 1.3740477353984322, 1.3746559036855015, 1.3735641270783088, 1.3733568436631531, 1.3717250144370248, 1.3731130042180093, 1.3701116622794083, 1.3711215142520417, 1.372003098885961, 1.3680549016994108, 1.3708855281366366, 1.3700427266296196, 1.372537378210145, 1.3707824484952884, 1.3691752818514624, 1.368811732140657, 1.3671458372817233, 1.3673372810874773, 1.368125223667822, 1.369430247868333, 1.3689213415543982, 1.3661235307235955, 1.3656993079408306, 1.3644418742426458, 1.3666003374295814, 1.3661633774498914, 1.3639447354824743, 1.363635563033392, 1.3666488244154742, 1.3658040446284403, 1.364763455227528, 1.3639767255367148, 1.3651642914501678, 1.3625182659826547, 1.363257147812769, 1.363981003330504, 1.3623563849666036, 1.3612958437928528, 1.3609412448057132, 1.3636061833283613, 1.3619795736865463, 1.3617909548809966, 1.358356398214061, 1.359527418175219, 1.3575554577361015, 1.3586866993027684, 1.3604413853998867, 1.3569917452298221, 1.3585987948925695, 1.3577779650316804, 1.3555997442976337, 1.358724727808872, 1.357117883512907, 1.3564373470169733, 1.35572388981733, 1.3550206092287818, 1.3569789575267803, 1.354857830986427, 1.3555383095488742, 1.3551791353017741, 1.3556955681411649, 1.3559355030178653, 1.3537643215737982, 1.3537541746721833, 1.3547296160106719, 1.3524830189078025, 1.3524837193087997, 1.352489886254165, 1.3512296238420909, 1.3525287533103492, 1.3503410816192627, 1.3503490095940707, 1.350865083691487, 1.350733403476228, 1.351283466704538, 1.3512296167860893, 1.3498155751332315, 1.3509909510612488, 1.3496146592024332, 1.3497415164549402, 1.348150717135159, 1.3488424890137907, 1.3486676353531835, 1.3503058105242958, 1.3478729736025088, 1.3488883106879357, 1.346879300670089, 1.3464230610945513, 1.3471668888103925, 1.3460740014400066, 1.3454318306527777, 1.3486019774018048, 1.3465455670223059, 1.344104336429608, 1.3456840130770318, 1.3432151651085351, 1.3444639604039652, 1.3425355865576556, 1.347058051843138, 1.3432086880704694, 1.3432139886131167, 1.3434231028749934, 1.3439471439409107, 1.3417155092750384, 1.3419244950434128, 1.341987934439353, 1.3406691146416827, 1.3419712726200852, 1.3408323823477248, 1.3416444011195054, 1.3420921879765402, 1.33942512856837, 1.340112060772667, 1.3399931225821236, 1.340977490876694, 1.339772631446149, 1.338774387710191, 1.3393067050945722, 1.3372355016221138, 1.3370528302831441, 1.337926457047091, 1.3363731483061365, 1.338215542730884, 1.3379538705415814, 1.3363008142631745, 1.335229064445258, 1.3352339813642413, 1.3346417247320632, 1.3350253123731999, 1.3368261606893808, 1.3337723544082167, 1.334889875022793, 1.334978470178408, 1.3338703459297014, 1.3348847408532354, 1.3334749632535323, 1.3343023236295515, 1.3319263465679323, 1.3323862946293437, 1.3338184705775846, 1.3307275786949466, 1.3317823755406888, 1.3285147001810163, 1.3332941981490898, 1.3303679729176459, 1.3294899485935674, 1.3287792937406497, 1.330468274351221, 1.3287795659537627, 1.330172950976363, 1.3285120562229573, 1.3269915974400126, 1.3294084722007917, 1.326382600258444, 1.3280398945942102, 1.32768505013249, 1.3265953505893362, 1.3263563630736876, 1.3267222509205898, 1.326014855940394, 1.3274786130661533, 1.3247981101181647, 1.3251896600856958, 1.3253302095092345, 1.3258706505796247, 1.3256994810802543, 1.3268151558076853, 1.3256123827253918, 1.3235633833757443, 1.3246248503711735, 1.3227708503093303, 1.3221486899711632, 1.3220806222095667, 1.3215263242662139, 1.324948779519102, 1.3211392639582031, 1.3222862998273142, 1.3222211005160371, 1.321434107524955, 1.320787857626086, 1.3231695938704542, 1.3201851744518103, 1.3214340618466291, 1.3218585661267195, 1.319454313067261, 1.3197382588252844, 1.3175176898267038, 1.3179292288896078, 1.3173945520154413, 1.321152548366618, 1.3179628574216848, 1.3166995965803152, 1.318588898375027, 1.3175731696815134, 1.3175410986689393, 1.31835245491931, 1.3162343932832141, 1.31746336975573, 1.316364713175646, 1.3156656450200304, 1.3142234187259851, 1.3184574315109727, 1.313862005498179, 1.3156452141818227, 1.3139384557896314, 1.3145503771267948, 1.3159023879844451, 1.3136879391387988, 1.3145555380348848, 1.3108574018300136, 1.3119855049614595, 1.3127369876962585, 1.3123077852332332, 1.3142202520667579, 1.312765567844902, 1.312393123115706, 1.3130908614007113, 1.3131656041397854, 1.3131740223210178, 1.3125403462169327, 1.3093943807566277, 1.309599748281675, 1.3107503544875765, 1.3122821712048254, 1.3094100402523052, 1.310575847313783, 1.3094759150828899, 1.3086632170781167, 1.3082681340963298, 1.308968857812733, 1.30791113829687, 1.3073539759882513, 1.3068796262562832, 1.30878184517596, 1.3069560505519404, 1.3057850086800407, 1.3056236345819967, 1.3050679417785453, 1.3056560407546451, 1.3056650351141101, 1.306316855540528, 1.3071829771327081, 1.3060544920487567, 1.305860730717858, 1.3036234512507359, 1.3061579269040782, 1.303240771976958, 1.3036975045441839, 1.3042724667308487, 1.3024312172351968, 1.3029735242466318, 1.3026420572465083, 1.3035286750377524, 1.3018702621400542, 1.3041575482329848, 1.3024899599336761, 1.3014529765209304, 1.3033088794749845, 1.2994655181314343, 1.3004277647470017, 1.3017416512854745, 1.2983336329831512, 1.301180194471484, 1.2991828593509591, 1.300189487844984, 1.2998532443031716, 1.2993280044597257, 1.2996252359259537, 1.2975571536200812, 1.2981853516673745, 1.2987334899070477, 1.298715975054328, 1.2985613171556658, 1.2998869645632687, 1.299537640865718, 1.2958096470045524, 1.296971856062286, 1.29497920575543, 1.2962010983737458, 1.297482159278846, 1.2962084626111658, 1.2980234270155244, 1.2956350326909454, 1.2954512570877312, 1.2947148890510154, 1.2952921386819762, 1.2932609941729132, 1.294238230149694, 1.29447309083285, 1.2939298242052024, 1.2925524173124556, 1.2943122242098657, 1.2915565702031335, 1.2926573296573673, 1.2924256374902814, 1.2934286512689799, 1.2926626838627635, 1.2940967807145876, 1.2910091342212997, 1.2917814150777562, 1.2912516423103595, 1.292154371181381, 1.2880954946684318, 1.289956757955462, 1.2892417829727458, 1.2897847274382166, 1.2896341987859423, 1.2888921365559658, 1.2882655064264934, 1.289396603902181, 1.2883096775533254, 1.2900646887835683, 1.2868485870391038, 1.2881962105492566, 1.2873370355906144, 1.287538524914382, 1.2892950693394907, 1.2865312066033623, 1.285487726841389, 1.2878840712372017, 1.2887411477781159, 1.2875560462660507, 1.2848393876233204, 1.286546622109933, 1.2847639922041016, 1.287750763685161, 1.2828078986699707, 1.2808153293214484, 1.2822367872033165, 1.2845574342573172, 1.2841000809476384, 1.2840856614513931, 1.2857806229517097, 1.2828917718750665, 1.2832338141503734, 1.2830168521292855, 1.2813512918733734, 1.2815951536006274, 1.2839135453337078, 1.2818535946982672, 1.2809425443875084, 1.2827304839345153, 1.281396330517029, 1.280811575342933, 1.2813086160618197, 1.2817181096270076, 1.2814991882656965, 1.2790260285231927, 1.2797729274565557, 1.2798567971336507, 1.2813083635312374, 1.2799025395206203, 1.278539568092964, 1.279242641829256, 1.2794819290392867, 1.279001827737624, 1.2782054391234092, 1.278121586343581, 1.278744716139226, 1.2776447939352826, 1.2783602444553672, 1.2795322143771566, 1.2785515584678293, 1.2777400883930123, 1.2770596653501565, 1.2767826987204152, 1.2738559899671797, 1.2757826033410997, 1.2744524569897637, 1.274392004325011, 1.2743728547081399, 1.2740562719719433, 1.2746916786532536, 1.2738297143457835, 1.2749462153681341, 1.2716560742565404, 1.2730942804865377, 1.2745233107578717, 1.2731845280463079, 1.2740375698541184, 1.2722833347840472, 1.2713613175900182, 1.2746013727886283, 1.2716055031506073, 1.2716950493809591, 1.2719730354172418, 1.2739126455746708, 1.270033316077473, 1.2707268749813425, 1.2729531452291851, 1.272585766152058, 1.270795223868896, 1.2687080885019628, 1.2715624552278133, 1.2681830226446609, 1.2693073322468458, 1.2708948128319975, 1.2705973622583526, 1.2687056820340616, 1.2681530162552808, 1.2686310313943763, 1.2674555176886442, 1.2674068736138744, 1.2684553419689524, 1.2682874288143027, 1.2702075796335286, 1.268729797776243, 1.2656148519842796, 1.2656995035777583, 1.267923186872607, 1.2641737700622773, 1.2645654966155317, 1.2664987326039703, 1.26693501780709, 1.265462842314414, 1.2662227903942453, 1.264922976865204, 1.2629504664293332, 1.2635142728918438, 1.2632978493550857, 1.2648286927154875, 1.264262496496658, 1.2630062326092586, 1.264344190140008, 1.2628544585727086, 1.2631122319497794, 1.2614143370096558, 1.2635648595952542, 1.2610143423080444, 1.262177132000433, 1.2618592197649947, 1.2606491726878275, 1.259732402188013, 1.2616421808706266, 1.261355489956627, 1.2617769137349828, 1.2601952797898621, 1.260588166498321, 1.2600403168119745, 1.2600997459851322, 1.2602913236692315, 1.2602550081003492, 1.2596450240069832, 1.258817205547915, 1.2604336109116814, 1.2607541801030762, 1.25835914663808, 1.2563849561311002, 1.2563029089449351, 1.2590870250051267, 1.2593832755014533, 1.2583869821928744, 1.2582193271393345, 1.2575619140889414, 1.2566628006759835, 1.257213726964695, 1.25630575362767, 1.2573837400225465, 1.256327396984041, 1.2575502553461497, 1.2539959165537469, 1.2538046417206619, 1.2562709671686, 1.2563097983877236, 1.252868467030867, 1.2539241178384823, 1.2543927980731953, 1.254466032127725, 1.2533507525363816, 1.256681103386983, 1.253399627416676, 1.2534830111580846, 1.2548127469615402, 1.254072334907508, 1.2563411985230966, 1.2540814857987972, 1.253446036410109, 1.252784941241006, 1.2522579970018144, 1.2507061154300179, 1.2527089965677707, 1.2537903074534882, 1.2515824026779223, 1.253577916421623, 1.2495264518669462, 1.2508481776231546, 1.2531904565211025, 1.2499939680470855, 1.2499911272637199, 1.250714144602743, 1.2486487108970357, 1.2494856328979087, 1.2492942958605995, 1.24966417888986, 1.249034144611002, 1.2498060817659087, 1.248983566077699, 1.249468230384161, 1.2457503475875498, 1.2469121782207786, 1.248366167612165, 1.2486867004094466, 1.2473844941531387, 1.2456303151597115, 1.2477760496911974, 1.2472692321394092, 1.2456951130216367, 1.2439518821573703, 1.2465498610820354, 1.2472827560805086, 1.244003671723363, 1.2474911946373937, 1.2437706550705099, 1.2459259714664328, 1.2451447189039901, 1.242634380346518, 1.2439959669410254, 1.2436932382925276, 1.2435477513390538, 1.2430949706897558, 1.245513202431046, 1.245345762771238, 1.2423437436421711, 1.2420907016855163, 1.2419931786453984, 1.2422579842564474, 1.2437260897360116, 1.2435304739764919, 1.241512334421045, 1.2409247330787396, 1.2422456724621425, 1.24292272310762, 1.2393175913166035, 1.2394168087255175, 1.2412228090369442, 1.2432415262560979, 1.241679581526284, 1.2414572181360002, 1.2404181686145865, 1.2392995281382884, 1.238825820873831, 1.238820627470997, 1.2376909198419328, 1.2377843550432508, 1.2383344056450318, 1.2389936138907698, 1.2402892712492066, 1.2382046119817691, 1.2413573341206228, 1.2397706883718662, 1.2378678353404702, 1.2372322630288073, 1.236807196496803, 1.2383004079726627, 1.2393745267502616, 1.2348510006506495, 1.2359351094266706, 1.2372352544021012, 1.236392415015497, 1.2366548366264392, 1.2363995027690662, 1.2353446723144745, 1.2387672601459183, 1.2368299723414247, 1.2349572051722684, 1.2372815137340272, 1.2360092965984641, 1.235858311348615, 1.2324046082214404, 1.2330964753560931, 1.2356621247957058, 1.2322833506489097, 1.2335421066046504, 1.2335630434324436, 1.2317695229595695, 1.2314888012372074, 1.2326001898150578, 1.2297048089660216, 1.2332213615702692, 1.2311117663933109, 1.2328275109748603, 1.2311678742322596, 1.2327027746078754, 1.2304536633402388, 1.2334936246322323, 1.2306607606997741, 1.2308010297400929, 1.2317849939857317, 1.2314923264527247, 1.2312093644498665, 1.230498987194905, 1.2292540689866491, 1.2293118539257584, 1.2289353499531375, 1.2296773535068903, 1.2294181213943387, 1.2280446435803565, 1.229209577554483, 1.2285635935554624, 1.2292002081128295, 1.22795919503007, 1.2265834067469445, 1.2274489666442634, 1.2265448822782048, 1.2243652763396409, 1.2254682095994087, 1.2299238679193634, 1.2273686790020666, 1.2265073543768434, 1.2244457825322017, 1.2279325058898451, 1.228499287014067, 1.227496211588197, 1.228014719634784, 1.225575093168336, 1.2255133104472888, 1.2254586141800212, 1.226951122655304, 1.2237927568293063, 1.2267311311956506, 1.2225126659387369, 1.2258040022998584, 1.2277464816503436, 1.2262807503296207, 1.2249406611436624, 1.2242813310890555, 1.2218936254673658, 1.2240025492100701, 1.223275378300022, 1.224737045363845, 1.2226554019800229, 1.2231821352819046, 1.2231001755530218, 1.223707530543069, 1.2251351288174543, 1.22088479884317, 1.2252685186276184, 1.2237843822838732, 1.223001757514811, 1.22072140895689, 1.2201198424877036, 1.221464529587101, 1.2199568300975074, 1.2219899586427991, 1.222977102545563, 1.219548495387734, 1.2202875662444166, 1.2196625998458388, 1.2181615855463568, 1.220479418555524, 1.220071889901087, 1.2174536049180313, 1.217758883011304, 1.2182406049651149, 1.219571932825344, 1.2173286652267907, 1.216887485386798, 1.2190771110332643, 1.2144775748995607, 1.2157667583765641, 1.2178629545779243, 1.2166061015143943, 1.2175718222823098, 1.216189348809073, 1.2179559833164155, 1.21507754522692, 1.2161914951333375, 1.2145601173056249, 1.2156740038937126, 1.216116491693574, 1.215326043118569, 1.2154334066812866, 1.2171896892916005, 1.2174869010752978, 1.2160639129695119, 1.2125729138606063, 1.2149586954220803, 1.212847171914169, 1.213402486850168, 1.2157919336330854, 1.213443871413436, 1.2142949601942876, 1.2129003229170945, 1.2133086440719176, 1.2151771198923342, 1.2121240247818539, 1.2119637413931041, 1.2130027755027248, 1.2110183393101084, 1.2131096432884905, 1.2125066214261397, 1.211046696823334, 1.2124858949786035, 1.2107816269835951, 1.210562006149708, 1.2134623013181478, 1.209028238448027, 1.2111458698537119, 1.2123730214214028, 1.2090852154749576, 1.213203890487041, 1.2109235034924801, 1.2096739039985562, 1.2090399528960944, 1.209672408126225, 1.2103173140424806, 1.208004421162828, 1.208472899187391, 1.2110556900686935, 1.2096131421695246, 1.2094334230244717, 1.2093517934421885, 1.2066348729846634, 1.2083168237751518, 1.2089629139855644, 1.207573782803485, 1.2086771002439696, 1.206507461464665, 1.2103743814976415, 1.206297994217026, 1.2066525089035154, 1.2070407256530453, 1.2048100493035956, 1.2061547273044646, 1.2067960175769723, 1.2054912356944099, 1.2062186712045164, 1.2034632721422618, 1.2070247185564487, 1.205307303002319, 1.205978257262447, 1.2028574904548788, 1.203976187750558, 1.2060208342899785, 1.20502795627184, 1.203796411043387, 1.204643603797271, 1.204828984455156, 1.2028963248306346, 1.2059072757435736, 1.20365249664984, 1.2010693332859288, 1.2057955311466229, 1.203624124838927, 1.202819477360568, 1.2009558868928119, 1.2009073486580655, 1.2035730273181404, 1.2022400336844898, 1.2003011436105888, 1.203210108562422, 1.2027014128144284, 1.2032113073399504, 1.2009478185407099, 1.1973878217634755, 1.2040228106522486, 1.2016919778886241, 1.2021355495274624, 1.2005262202191576, 1.2031615853681, 1.201247743542692, 1.2012240803130319, 1.200929863244945, 1.1994951380376133, 1.1982966590893231, 1.1994210854869023, 1.1985717493797017, 1.2008684065854438, 1.1981610668782505, 1.1996015009478989, 1.2008223708173567, 1.198056745194943, 1.197587043696846, 1.1975364649778586, 1.198822173559777, 1.1953616218403493, 1.198092280332916, 1.1989123975376474, 1.1962546962443914, 1.1974728876185194, 1.1952418356298287, 1.1969986112318307, 1.1984516497341644, 1.1982619641725891, 1.1955766100378422, 1.1952475331654058, 1.1960061352200968, 1.1946496224477654, 1.1955674390926538, 1.196377814187439, 1.1934938493918779, 1.1921879157099025, 1.196920255448588, 1.192716899504914, 1.1937102049682, 1.1936579173970445, 1.193849865521226, 1.1951422568793608, 1.1959429548536877, 1.1947463433319163, 1.1915868260778741, 1.1954473919957598, 1.1898003034873914, 1.193636600287904, 1.1920641530711331, 1.1929706121530859, 1.1932527827696637, 1.1908270854073522, 1.1919252181721625, 1.1949163014272293, 1.1913130457898908, 1.1921388603445153, 1.1925532162375168, 1.1932888151700622, 1.1912840160996743, 1.193804126476573, 1.190143666349096, 1.1916971271654526, 1.1908088060925683, 1.191763457665191, 1.1900297477609272, 1.1902800753851916, 1.1899047592347285, 1.1900070849980149, 1.1910500270183955, 1.1884747741749724, 1.1884957286427698, 1.1909946554175048, 1.1886899751666178]
Validation loss: [1.6713359704831752, 1.6641664388703137, 1.6771347202905795, 1.6727812406493396, 1.6911757835527745, 1.6662590765371554, 1.6723827472547206, 1.681969165802002, 1.6696376567933617, 1.6770553472565441, 1.6742982864379883, 1.7000730212141828, 1.6704960945175915, 1.665691614151001, 1.6745445786452875, 1.676950414006303, 1.6759050968216687, 1.6967532809187726, 1.671390745697952, 1.6810052045961705, 1.673282364519631, 1.6699076949096308, 1.6629255137792447, 1.6688155924401633, 1.686035569121198, 1.663406616303979, 1.6587551919425405, 1.7006511746383295, 1.6725288193400314, 1.695559585966715, 1.6887839683672277, 1.6817601512118083, 1.670237512123294, 1.6641037958424265, 1.6835051280696218, 1.6695415711984403, 1.6795607369120529, 1.6589303103888906, 1.6779440699554071, 1.685193087996506, 1.6732141593607461, 1.6666309368319627, 1.686114878189273, 1.6712289554316824, 1.6890456909086646, 1.6738702058792114, 1.695309240643571, 1.6744520228083541, 1.6703536248788602, 1.6835650205612183, 1.6956393020909006, 1.6671120335416096, 1.671346350413997, 1.6898230052575833, 1.6760654740217256, 1.677429850508527, 1.669348004387646, 1.6822624206542969, 1.6788961771057873, 1.6845860801092007, 1.6697452213706039, 1.6794887374087077, 1.6761136694652279, 1.6660188727262544, 1.6724894337537812, 1.6776650795122472, 1.678748558207256, 1.681097271965771, 1.6811236114036747, 1.6986641593095733, 1.709243588331269, 1.6774555968075264, 1.7011399472632058, 1.6626280458962046, 1.6753125481489228, 1.668107774199509, 1.6829102330091523, 1.706281757936245, 1.700022156645612, 1.67056760264606, 1.6912676764697563, 1.6932826710910331, 1.6811948811135642, 1.674863396621332, 1.6678294495838444, 1.6881724508797251, 1.6698999579359846, 1.6818384222868012, 1.6815006122356508, 1.7032478524417412, 1.6937134265899658, 1.6849997072685055, 1.687189387112129, 1.6793303780439424, 1.6883968958040563, 1.6850638680341767, 1.6840504698637055, 1.673499889490081, 1.65801833751725, 1.679676733365873, 1.6830062255626772, 1.6809499292838863, 1.6783656928597428, 1.6842841229787686, 1.6925995960468199, 1.6823045684070121, 1.6775685345254294, 1.6796865259728782, 1.6925955138555386, 1.6905549851859487, 1.686600528112272, 1.6913639655927333, 1.6678237202690869, 1.6740435681691983, 1.6725444008664387, 1.6666753815441597, 1.6932399825351994, 1.6955592603218266, 1.6987781844488004, 1.6899310757474202, 1.6837589798904047, 1.690086931717105, 1.6915201065016956, 1.6904149840517741, 1.680567485530202, 1.6850316379128434, 1.7031027340307467, 1.6761353597408388, 1.6918370665573492, 1.6809949467821819, 1.690896955932059, 1.6808291673660278, 1.6956661677942044, 1.6964476341154517, 1.6854112700718205, 1.688385736651537, 1.675865874057863, 1.6857984647518252, 1.6703640484228366, 1.6603754613457657, 1.703169799432522, 1.6760419868841403, 1.6844620559273698, 1.7004763585765188, 1.690658176817545, 1.6894313649433415, 1.696571376265549, 1.6915320623211745, 1.6937197650351175, 1.6983157454467401, 1.6853219125328995, 1.6978251934051514, 1.6857784230534623, 1.6932499583174543, 1.6987056034367258, 1.6850931586288824, 1.6831271968236783, 1.688356754256458, 1.6875712697098895, 1.6929089819512717, 1.6990986742624423, 1.6845100100447492, 1.6852663697265997, 1.7008607532919906, 1.684337383363305, 1.6864174342736966, 1.6953879769255475, 1.6798819070909081, 1.6901141434181026, 1.6901963745675437, 1.6937754590336869, 1.706592545276735, 1.685819875903246, 1.7024512203728281, 1.69114798452796, 1.677873663785981, 1.6961777471914523, 1.7037431757624557, 1.699671521419432, 1.6968802678875807, 1.6761675869546286, 1.6844685920854894, 1.685513353929287, 1.7036333520237992, 1.7123788100917166, 1.7046611192749768, 1.6906833270700967, 1.6875672747449177, 1.6987437155188583, 1.6842546986370552, 1.690094052291498, 1.7061096895031813, 1.6943943442367926, 1.697064236896794, 1.6852315082782652, 1.6888752826830236, 1.7210333027490756, 1.7127855056669654, 1.7094466569947033, 1.6897646770244692, 1.701704318930463, 1.694652324769555, 1.685853452217288, 1.6978289848420678, 1.6883756591052543, 1.68526809971507, 1.7000890127042445, 1.6771292163104545, 1.6941211601582968, 1.7096877098083496, 1.699265323034147, 1.6868280724781315, 1.6838826522594545, 1.6823493910998832, 1.6801580568639243, 1.6912234207478964, 1.6815524508313435, 1.696897169438804, 1.7044661219527082, 1.6777129027901627, 1.6875817077915842, 1.7061075844415805, 1.7172130113694726, 1.7036745868078091, 1.681724118023384, 1.6943953240790017, 1.7022299185031797, 1.695586117302499, 1.6808318364910964, 1.7031947752324545, 1.6819763125442877, 1.7103208070848046, 1.6923268829903952, 1.6870724253538179, 1.7038293350033644, 1.7203273976721414, 1.6808416174679268, 1.6809339087183883, 1.6885953792711583, 1.6958654653735277, 1.7119590363851407, 1.6926785794700063, 1.6812975842778275, 1.7129096984863281, 1.7019210297886918, 1.7049455729926504, 1.7095082794747702, 1.7002736446334095, 1.6945764873085953, 1.7183539198666085, 1.7040237391867288, 1.710601649633268, 1.7128089055782412, 1.6953542610494101, 1.6966368512409489, 1.69641958213434, 1.6912498328743912, 1.6949751260803967, 1.7005135635050332, 1.7065934756907022, 1.6877583323455438, 1.6908510952461056, 1.7042576452580893, 1.7212851890703527, 1.6930183957262737, 1.6990325799802455, 1.7180042470373758, 1.6940280024598284, 1.6941751532438325, 1.7034603328239628, 1.7200590023180333, 1.7151876455400048, 1.7019329681629087, 1.7092104423336867, 1.698983980388176, 1.7144832204027873, 1.700922207134526, 1.684060762568218, 1.700383773664149, 1.7007602104326573, 1.7212512871114218, 1.7040214538574219, 1.7102535497851488, 1.7050046455569383, 1.712901737631821, 1.7091268126557513, 1.7106355864827225, 1.6914691314464663, 1.7160483220728433, 1.728923021293268, 1.680739824364825, 1.706503879733202, 1.7139903394187368, 1.6967556767347383, 1.7019849608584148, 1.7180925927511075, 1.7228332321818283, 1.6841019682767915, 1.6966976043654651, 1.683978135992841, 1.6974194776721117, 1.7018576569673491, 1.706568898224249, 1.7023935230766856, 1.7105678988666069, 1.7188140851695364, 1.7019014707425746, 1.7105522359289773, 1.7167286960090078, 1.6885913436005755, 1.703166374346105, 1.727348144461469, 1.7053741769092838, 1.7258006625059175, 1.7143181649650014, 1.6985770202264554, 1.6962346420055483, 1.7024878321624384, 1.706972886876362, 1.696141786691619, 1.6945285419138467, 1.713166917242655, 1.700618275781957, 1.7336575228993485, 1.6972107974494375, 1.6942002511605985, 1.7025441919885032, 1.7096302974514845, 1.701126307975955, 1.7116720269366008, 1.7143152952194214, 1.7065217582190908, 1.6978159997521378, 1.7117076908669822, 1.731742105832914, 1.6913156015116995, 1.7073720984342622, 1.713058006472704, 1.7329252376789, 1.712187772843896, 1.7094351285841407, 1.7232675290689237, 1.6967725288577196, 1.6979606587712357, 1.7237461805343628, 1.7161861861624368, 1.695359544056218, 1.7372099946184856, 1.6990255350019874, 1.7103293174650611, 1.7161616057884403, 1.7122960759372245, 1.718074249058235, 1.7178643156842488, 1.7007685172848586, 1.7112194910282041, 1.70045382511325, 1.7044243376429489, 1.7258845654929555, 1.708373479726838, 1.7260670516549088, 1.6908368177530242, 1.7038351181076794, 1.704413544840929, 1.716646249701337, 1.7171315216436618, 1.7159811897975643, 1.7031473531955625, 1.717946750361745, 1.7154569160647508, 1.726005304150465, 1.7043267662932233, 1.7012735227259195, 1.7061830555520408, 1.7075128090090868, 1.7089797752659495, 1.715353736063329, 1.711759404438298, 1.7263756641527501, 1.6917051076889038, 1.7227292962190581, 1.7069687261814024, 1.7067036192591598, 1.7082113172949813, 1.6993237908293561, 1.723026205853718, 1.7167818720747785, 1.7006162201485984, 1.7050738043901397, 1.7143492466065942, 1.7007083660218774, 1.7209713749769258, 1.7099027691817865, 1.7242150277626225, 1.7099341095947638, 1.7264081530454682, 1.7194376544254582, 1.7161655193421899, 1.721732802507354, 1.7301228976831204, 1.7116907079045365, 1.7148636957494223, 1.7145089928696795, 1.704067363971617, 1.721898483067024, 1.7176106964669577, 1.7141589507824038, 1.739507009343403, 1.697280046416492, 1.7238704285970547, 1.7296785406950044, 1.7227067947387695, 1.7390318440227974, 1.7274942950504582, 1.707050300225979, 1.7451727739194545, 1.7106958307871005, 1.7122879609829043, 1.7109144955146602, 1.7145199339564254, 1.7310471999935988, 1.7146158014855735, 1.7286507676287395, 1.7246603675004912, 1.7149944857853214, 1.7067439905027064, 1.7206674029187459, 1.712685302990239, 1.7368766970750762, 1.7129719664410847, 1.7126568933812583, 1.7108653493043853, 1.7157512147252152, 1.7309929888422897, 1.7207597290597312, 1.7169255047309688, 1.709249420863826, 1.7137906144304973, 1.7160153679731416, 1.705667553878412, 1.7121834028057936, 1.7132605663159999, 1.709138835348734, 1.7294206561111822, 1.7336139969709443, 1.7372622780683564, 1.7090583341877634, 1.7131977401128629, 1.734466023561431, 1.7257920678068952, 1.7084537599144913, 1.7509267417395986, 1.734835354293265, 1.7366099241303234, 1.7437867972908951, 1.7094786545125449, 1.7185459398641818, 1.7116852329998482, 1.723450582201888, 1.7131769424531518, 1.7155499342011242, 1.7175103106149814, 1.7282199685166522, 1.7208052611932523, 1.7352537440090645, 1.7258968847553904, 1.7164373339676275, 1.7337083089642409, 1.7142277868782603, 1.7442713103643277, 1.7223661440174753, 1.7219919780405557, 1.7119524130007115, 1.7041128757523327, 1.7231442026975679, 1.714271589023311, 1.7287541249903238, 1.709630878960214, 1.7208249249109409, 1.7166738015849416, 1.7212643768729232, 1.73415488440816, 1.7485131752200243, 1.7176789743144338, 1.7251843592015708, 1.7300331069201957, 1.7277252354272983, 1.7287899168526255, 1.724135102295294, 1.745861094172408, 1.7426036945203456, 1.7197863212445887, 1.7250788909632986, 1.7189638236673868, 1.7361400563542435, 1.748230896344999, 1.7076684847110655, 1.703952670097351, 1.73385889937238, 1.754860430229001, 1.7173524833307034, 1.7248074397808169, 1.7112406521308712, 1.7158287967123635, 1.7397439305375262, 1.7595111945780313, 1.7164699885903336, 1.7283442689151298, 1.711886382684475, 1.7370496261410597, 1.7299915086932298, 1.7523507984673106, 1.7271501930748545, 1.735566374732227, 1.728048513575298, 1.7409236838177937, 1.7621470282717449, 1.738431750274286, 1.7574586461230022, 1.74021648779148, 1.7375407625989217, 1.7198254422443668, 1.7347955005924875, 1.7467292111094406, 1.7292755667756243, 1.7298744887840458, 1.7170395298701961, 1.7286416786472971, 1.7246198276194131, 1.7337298800305623, 1.726787276384307, 1.7204084774342978, 1.7247971674291098, 1.7532548468287399, 1.7189427061778744, 1.7215764231798125, 1.7122418168114453, 1.7170096025234316, 1.7410535928679676, 1.7193217335677728, 1.7518331539340135, 1.7442748517524906, 1.71778777750527, 1.721777930492308, 1.7199928208095272, 1.7129783717597402, 1.7341795810839025, 1.7172062862210158, 1.7253743729940274, 1.7266564601805152, 1.7300058254381505, 1.7074760314894886, 1.7305883081947886, 1.7450800232770967, 1.7459996444423025, 1.7299856238248872, 1.7505417713304845, 1.7169460145438589, 1.734643462227612, 1.7169447991906144, 1.7332837232729283, 1.7274287590166417, 1.7352877913451776, 1.7596070272166555, 1.7453456302968466, 1.7230107522592313, 1.7272364628024217, 1.7321472109817877, 1.7353532343375973, 1.7339928702610294, 1.7383508042591373, 1.7401851764539393, 1.726908637256157, 1.7456871183907114, 1.7291630564666376, 1.7397144654902017, 1.734646663433168, 1.7327205146231301, 1.7487558039223277, 1.7162408741509043, 1.741666846159028, 1.7227531962278413, 1.7306319446098515, 1.733459958216039, 1.73316302532103, 1.742016629474919, 1.7254306572239573, 1.7188412416272048, 1.7519252678243125, 1.7238609848952875, 1.7355837443979776, 1.7173516575883074, 1.735289230579283, 1.741198792690184, 1.7247096852558415, 1.7406394103678262, 1.7419815819437912, 1.7404722876665069, 1.7506464661621466, 1.7338278235458746, 1.7373550665087816, 1.7334873036640446, 1.7315599802063733, 1.7345941415647181, 1.7513678887995279, 1.7406377588830344, 1.7366367055148613, 1.7437016876732432, 1.7407888115906134, 1.7444474726188473, 1.734714258007887, 1.7511350643344041, 1.7164761874733903, 1.7330694518438199, 1.7243200075335618, 1.732068518312966, 1.7246435037473353, 1.7379369328661662, 1.7671846093201056, 1.7465252323848446, 1.7478171906820157, 1.738355526110021, 1.7312408656608769, 1.7386652870876034, 1.7363694615480376, 1.7504113331073667, 1.74022074443538, 1.7397879914539616, 1.7326578774103305, 1.7382989947388812, 1.727148538682519, 1.7215170627687035, 1.7321635135790197, 1.743162931465521, 1.765088537844216, 1.745439977180667, 1.7348368894763109, 1.7406840876835148, 1.7280188740753546, 1.7437726404608749, 1.7351742285053904, 1.7362388314270392, 1.7370497540729801, 1.7388641136448557, 1.7458319024341862, 1.729541615742009, 1.7408964285036412, 1.7509078688737822, 1.735055123887411, 1.7812584900274508, 1.7369604924829996, 1.7521239024836843, 1.7512404918670654, 1.7421314745414547, 1.7739332332843687, 1.7332469079552628, 1.7413489585969506, 1.7400806386296341, 1.7379883411453991, 1.7474423298021642, 1.7491581585349105, 1.7460540271386868, 1.7428939138970725, 1.7369459373194998, 1.771205087987388, 1.7454815230718472, 1.7528847455978394, 1.7265953785035668, 1.7327993294087851, 1.7672511833470042, 1.7283276203202038, 1.747130777777695, 1.7601732742495653, 1.767435140726043, 1.774657534389961, 1.7655987216205131, 1.733346287797137, 1.7454550615171107, 1.729446175621777, 1.7466391179619767, 1.749805683042945, 1.7276252711691507, 1.7354243761155663, 1.7539949388038822, 1.749653234714415, 1.734592202233105, 1.7763098914448807, 1.757312073940184, 1.74076296643513, 1.7510998394431136, 1.7403266982334415, 1.7852781167844447, 1.7504533325753562, 1.7577705383300781, 1.733134734921339, 1.7420335397487734, 1.727911617697739, 1.7582835976670428, 1.7482378628195785, 1.742488029526501, 1.7423536195987608, 1.735364062030141, 1.7885908586222952, 1.7493597763340647, 1.756169179590737, 1.7422428189254389, 1.748917893665593, 1.7436538295048039, 1.7510578079921444, 1.7342261221350692, 1.742620183200371, 1.7367314318331277, 1.737803973802706, 1.737874845179116, 1.7471034643126697, 1.757185473674681, 1.7407131951029708, 1.7320473543027552, 1.7416442283769933, 1.7457888766032894, 1.7772781180172432, 1.770944851200755, 1.778078151912224, 1.7486155411092246, 1.7808145168350964, 1.7406905569681308, 1.7475889688584862, 1.7285528793567564, 1.7435186578006279, 1.750317989326105, 1.7545308950470715, 1.7535014704960148, 1.7591550989848812, 1.7851329634829265, 1.7610824805934255, 1.7569166974323551, 1.7651229689760906, 1.7414389179974068, 1.756627670148524, 1.7612305152706984, 1.7615368889599312, 1.7527463290749528, 1.746341231392651, 1.7758748037059133, 1.7452750235069088, 1.7403762892978947, 1.7770969867706299, 1.7650586163125388, 1.7550346386141893, 1.7418596700924198, 1.7558582904862194, 1.762299441709751, 1.757889160295812, 1.7483836121675445, 1.748087310209507, 1.7359066183974103, 1.775720081678251, 1.7667526064849481, 1.7544534002862326, 1.761917035754134, 1.7810507518489187, 1.769764211119675, 1.7629670457142155, 1.7351650057769403, 1.7556742051752603, 1.7636529381682233, 1.745378439019366, 1.7425349223904494, 1.73150656281448, 1.7510902561792514, 1.7610458571736405, 1.7313832335355805, 1.75793577403557, 1.768680502728718, 1.7604511511035081, 1.7534566826936675, 1.7634666198637428, 1.7546163332171556, 1.7665914122651263, 1.7442668879904397, 1.7561664755751447, 1.7579851877398607, 1.7438782046480876, 1.7553739954785603, 1.769958586227603, 1.7656826653131625, 1.7420256690281193, 1.7635567042885758, 1.7589692778703643, 1.7433473307911942, 1.760130251326212, 1.7765806767998673, 1.7726015521258842, 1.7381234256232656, 1.7806638333855607, 1.7726238704309232, 1.7469197279069482, 1.7592660508504727, 1.747877885655659, 1.7668734876120962, 1.7830296115177433, 1.736537639687701, 1.7546566753852657, 1.7577740797182408, 1.7522299086175315, 1.7401462909651966, 1.7661297990054619, 1.7472047195201967, 1.7821672631473076, 1.752002782937957, 1.7542554547147053, 1.7643221558594122, 1.7443145455383673, 1.7502326587351358, 1.750419869655516, 1.8041921621415673, 1.7665712164669503, 1.7456909476256952, 1.78335514301207, 1.7704550696582328, 1.7812628164523985, 1.7527934487273054, 1.7697995930183223, 1.749955729740422, 1.7476700776960792, 1.779450535774231, 1.761564263483373, 1.7845527835008574, 1.772527180066923, 1.773993308951215, 1.7506875788293235, 1.7835405628855636, 1.7694588899612427, 1.7700025599177291, 1.7709547862773989, 1.7590452345406138, 1.7742040273619861, 1.7707004430817395, 1.741715572229246, 1.7588911143744863, 1.741393257931965, 1.7561148405075073, 1.7579474943440134, 1.7605876631853057, 1.7501379571309903, 1.7789658860462467, 1.7595736137250575, 1.762308318440507, 1.75767978807775, 1.763461807879006, 1.7449241033414515, 1.7674014684630603, 1.7841865405803774, 1.7690759548326818, 1.7792313157058344, 1.7830865354072758, 1.781325357716258, 1.7814481490995826, 1.7411741919633819, 1.787839511545693, 1.7619162914229602, 1.7534066409599491, 1.746778889400203, 1.736051460591758, 1.7547428927770474, 1.8136913049511794, 1.7624353519300135, 1.7676930311249524, 1.7637732552319039, 1.7421043093611555, 1.764459836773756, 1.7566372127067753, 1.7683308473447474, 1.7840759725105473, 1.7638489967439233, 1.762486103104382, 1.7488958806526371, 1.7528874612436063, 1.7719189277509364, 1.7426648372557105, 1.7608742859305404, 1.762209505569644, 1.7641883855912743, 1.7615072901655988, 1.7466629947104104, 1.8045311555629824, 1.7750010025210496, 1.7509524909461416, 1.7478935515008323, 1.7565611746253036, 1.754354386794858, 1.7558466399588235, 1.7765640223898538, 1.799061368151409, 1.7673954934608647, 1.7718067663471873, 1.7614064187538334, 1.7810390809687173, 1.7708926055489518, 1.7827367927969955, 1.7973166238970872, 1.7816584168410883, 1.7660373391174689, 1.777154919577808, 1.7980633479792898, 1.7696843467107632, 1.773388551502693, 1.7783119911100806, 1.768191134057394, 1.766471670895088, 1.775689860669578, 1.7804661640306798, 1.7659725008941278, 1.790136043618365, 1.7962580628511382, 1.7598316640388676, 1.7680421951340466, 1.7694151227067156, 1.7662131931723617, 1.750110280223009, 1.7671237573391054, 1.7520512575056495, 1.773859358415371, 1.7698299012533047, 1.8061215906608394, 1.7908072704222144, 1.766563941792744, 1.7798516401430455, 1.7645386224839745, 1.7904044098970366, 1.770191695632004, 1.7887943634172765, 1.7791474592394945, 1.7822363405692867, 1.7877315224670782, 1.7795022609757214, 1.7779155678865386, 1.7761292312203385, 1.7745466319526113, 1.7669241253922625, 1.7467624850389434, 1.7830902512480573, 1.7713116203866355, 1.7765265674125859, 1.7634979166635654, 1.7641350903162143, 1.7541177330947504, 1.7793598436727756, 1.7571736632323847, 1.797020673751831, 1.7551936347310135, 1.759937335805195, 1.7796995959630826, 1.7868465301467151, 1.7573009496781884, 1.7764873213884307, 1.7523590151856585, 1.7893002091384516, 1.7668252311101773, 1.7568914599534942, 1.7726120512659957, 1.767092797814346, 1.7670685023796269, 1.7650570026258143, 1.7738390899286038, 1.7908703030609503, 1.8012623961378889, 1.8041301878487193, 1.8066149891876593, 1.758672676435331, 1.787888526916504, 1.7863236927404635, 1.7734377035280553, 1.776233268947136, 1.7889274882107247, 1.7789090551981113, 1.7623181750134724, 1.764924092990596, 1.7884877949226192, 1.7897585368737943, 1.7701999559635069, 1.772479970280717, 1.76743249195378, 1.767177369536423, 1.7991686536044609, 1.7954315441410715, 1.7874585070261142, 1.7780173638971841, 1.7797232430155685, 1.7849956547341697, 1.7748651184686801, 1.7665549691130475, 1.758402993039387, 1.7886998188204881, 1.8005372721974442, 1.8031460628276919, 1.7992372541892818, 1.7703532969079367, 1.7728564041416819, 1.7955160867877122]
Training accuracy: [0.45544843704352905, 0.45583795890544354, 0.4578829486804947, 0.45651962216379394, 0.4553510565780504, 0.45135845749342685, 0.4601226993865031, 0.4578829486804947, 0.45700652449118706, 0.45739604635310155, 0.4575908072840588, 0.45856461193884507, 0.4577855682150161, 0.45729866588762297, 0.45983055799006717, 0.4599279384555458, 0.4567143830947512, 0.4556431979744863, 0.45875937286980234, 0.45768818774953746, 0.45934365566267404, 0.45729866588762297, 0.4595384165936313, 0.4603174603174603, 0.4594410361281527, 0.46265459148894733, 0.4623624500925114, 0.4583698510078878, 0.4610965040412893, 0.46119388450676796, 0.46070698217937484, 0.46353101567825494, 0.4621676891615542, 0.4657707663842633, 0.4616807868341611, 0.4638231570746908, 0.46421267893660534, 0.4638231570746908, 0.4629467328853832, 0.4623624500925114, 0.46158340636868245, 0.4630441133508618, 0.46440743986756255, 0.4630441133508618, 0.4636283961437336, 0.46664719057357096, 0.4630441133508618, 0.4673288538319213, 0.46937384360697243, 0.4651864835913916, 0.4667445710390496, 0.46742623429739993, 0.47054240919271595, 0.4664524296426137, 0.4691790826760152, 0.46830265848670755, 0.4692764631414938, 0.46878956081410067, 0.4655760054533061, 0.47180835524393805, 0.46859479988314345, 0.4688869412795793, 0.46947122407245107, 0.47161359431298083, 0.4720031161748953, 0.4719057357094167, 0.47093193105463044, 0.47687213944882656, 0.47015288733080146, 0.4732690622261174, 0.4747297692082968, 0.47404810594994645, 0.4699581263998442, 0.4720031161748953, 0.47297692082968157, 0.4722952575713312, 0.47619047619047616, 0.47287954036420293, 0.4750219106047327, 0.4741454864154251, 0.47424286688090367, 0.4746323887428182, 0.4758983347940403, 0.47989093387866394, 0.47287954036420293, 0.4782354659655273, 0.47716428084526247, 0.4758009543285617, 0.47862498782744184, 0.47843022689648457, 0.4747297692082968, 0.4780407050345701, 0.47755380270717696, 0.47794332456909144, 0.48144902132632195, 0.47687213944882656, 0.4785276073619632, 0.4794040315512708, 0.4793066510857922, 0.4809621189989288, 0.4797935534131853, 0.4793066510857922, 0.48057259713701433, 0.47950141201674945, 0.47969617294770667, 0.4800856948096212, 0.4808647385334502, 0.48388353296328757, 0.48144902132632195, 0.4801830752750998, 0.4824228259811082, 0.48388353296328757, 0.48339663063589444, 0.4832992501704158, 0.4834940111013731, 0.48427305482520205, 0.48193592365371507, 0.4850520985490311, 0.48602590320381733, 0.4822280650501509, 0.4841756743597234, 0.48378615249780893, 0.4834940111013731, 0.48252020644658683, 0.4841756743597234, 0.4835913915668517, 0.4841756743597234, 0.48738922972051807, 0.48456519622163796, 0.4855390008764242, 0.48378615249780893, 0.4871944687895608, 0.48738922972051807, 0.48738922972051807, 0.4890446976336547, 0.48602590320381733, 0.48758399065147534, 0.4868049469276463, 0.4863180446002532, 0.4887525562372188, 0.4889473171681761, 0.48797351251338983, 0.4861232836692959, 0.4890446976336547, 0.4910896874087058, 0.48865517577174017, 0.48972636089200505, 0.4905054046158341, 0.49089492647774857, 0.4896289804265264, 0.49216087252897067, 0.49011588275391954, 0.4917713506670562, 0.49410848183854317, 0.4883630343753043, 0.4913818288051417, 0.4938163404421073, 0.49001850228844096, 0.4938163404421073, 0.4924530139254066, 0.4907001655467913, 0.4915765897360989, 0.49196611159801346, 0.49323205764923556, 0.4940111013730646, 0.49547180835524396, 0.4942058623040218, 0.49235563345992794, 0.4961534716135943, 0.4931346771837569, 0.48953159996104784, 0.49479014509689356, 0.4922582529944493, 0.49625085207907293, 0.49303729671827834, 0.4979063199922096, 0.4975167981302951, 0.49761417859577367, 0.4956665692862012, 0.4944006232349791, 0.4959587106826371, 0.4965429934755088, 0.49829584185412407, 0.49479014509689356, 0.49625085207907293, 0.49849060278508134, 0.49849060278508134, 0.4956665692862012, 0.4996591683708248, 0.4985879832505599, 0.4959587106826371, 0.49849060278508134, 0.4973220371993378, 0.4970298958029019, 0.4999513097672607, 0.49907488557795304, 0.4996591683708248, 0.4980037004576882, 0.4992696465089103, 0.5012172558184829, 0.5012172558184829, 0.5000486902327393, 0.5001460706982179, 0.4994644074398676, 0.5000486902327393, 0.49956178790534617, 0.5005355925601325, 0.4981010809231668, 0.5023858214042263, 0.5025805823351835, 0.502483201869705, 0.501704158145876, 0.5009251144220469, 0.5039439088518843, 0.5034570065244912, 0.505307235368585, 0.5010224948875256, 0.5047229525757133, 0.5021910604732691, 0.504528191644756, 0.502483201869705, 0.5054046158340637, 0.50141201674944, 0.5020936800077904, 0.5042360502483202, 0.5041386697828416, 0.5047229525757133, 0.5060862790924141, 0.5079365079365079, 0.5037491479209271, 0.5081312688674652, 0.5060862790924141, 0.5064758009543285, 0.5048203330411919, 0.5072548446781575, 0.5072548446781575, 0.5076443665400721, 0.5052098549031064, 0.5052098549031064, 0.508423410263901, 0.5064758009543285, 0.50637842048885, 0.5079365079365079, 0.5047229525757133, 0.5085207907293797, 0.5075469860745935, 0.5056967572304996, 0.505307235368585, 0.5082286493329438, 0.5064758009543285, 0.5101762586425163, 0.5077417470055506, 0.5108579219008667, 0.5093972149186873, 0.5098841172460804, 0.509494595384166, 0.5090076930567728, 0.5127081507449606, 0.5118317265556529, 0.5112474437627812, 0.5105657805044308, 0.5112474437627812, 0.5099814977115591, 0.5075469860745935, 0.5119291070211316, 0.5110526828318239, 0.5110526828318239, 0.5067679423507644, 0.5124160093485247, 0.5124160093485247, 0.5139740967961827, 0.5142662381926185, 0.511539585159217, 0.5127081507449606, 0.5129029116759178, 0.5113448242282598, 0.5145583795890545, 0.5108579219008667, 0.511539585159217, 0.5110526828318239, 0.5142662381926185, 0.5117343460901743, 0.5158243256402766, 0.515434803778362, 0.5127081507449606, 0.5124160093485247, 0.5149479014509689, 0.513389814003311, 0.513389814003311, 0.5147531405200116, 0.5140714772616614, 0.5146557600545331, 0.5167981302950628, 0.5111500632973026, 0.5146557600545331, 0.5144609991235758, 0.5135845749342682, 0.5157269451747979, 0.5150452819164476, 0.5159217061057552, 0.5155321842438407, 0.5168955107605414, 0.5188431200701139, 0.5169928912260201, 0.5179666958808063, 0.5189405005355926, 0.5174797935534132, 0.5168955107605414, 0.5156295647093193, 0.5168955107605414, 0.5203038270522933, 0.5169928912260201, 0.5197195442594216, 0.5187457396046353, 0.5183562177427208, 0.519330022397507, 0.5182588372772422, 0.5163112279676697, 0.5163112279676697, 0.5200116856558574, 0.5168955107605414, 0.5215697731035155, 0.5196221637939429, 0.5195247833284643, 0.5209854903106437, 0.5208881098451651, 0.5215697731035155, 0.5218619144999513, 0.5180640763462849, 0.5194274028629857, 0.5223488168273445, 0.5182588372772422, 0.5190378810010712, 0.5209854903106437, 0.518550978673678, 0.5186483591391566, 0.5218619144999513, 0.5227383386892589, 0.5215697731035155, 0.5222514363618658, 0.5226409582237803, 0.5220566754309086, 0.5217645340344726, 0.5220566754309086, 0.5236147628785666, 0.5241990456714383, 0.522446197292823, 0.5243938066023955, 0.5215697731035155, 0.5233226214821307, 0.5237121433440451, 0.5237121433440451, 0.5221540558963872, 0.5266335573084039, 0.524296426136917, 0.522446197292823, 0.5266335573084039, 0.5200116856558574, 0.5272178401012757, 0.5215697731035155, 0.5244911870678742, 0.5216671535689941, 0.5219592949654299, 0.5249780893952674, 0.5257571331190963, 0.5251728503262245, 0.5261466549810108, 0.5254649917226605, 0.5244911870678742, 0.5249780893952674, 0.5308209173239848, 0.5259518940500536, 0.5244911870678742, 0.5283864056870192, 0.5248807089297887, 0.5253676112571818, 0.5260492745155322, 0.5264387963774467, 0.5293602103418055, 0.5262440354464895, 0.527120459635797, 0.526341415911968, 0.5258545135845749, 0.5268283182393612, 0.527120459635797, 0.5275099814977116, 0.5241990456714383, 0.5298471126691986, 0.5261466549810108, 0.5283864056870192, 0.5278995033596261, 0.5278995033596261, 0.5293602103418055, 0.5311130587204207, 0.5278021228941474, 0.527412601032233, 0.530236634531113, 0.5266335573084039, 0.5276073619631901, 0.5264387963774467, 0.5297497322037199, 0.5293602103418055, 0.5293602103418055, 0.5280942642905833, 0.5315999610478138, 0.5277047424286688, 0.5331580484954718, 0.5311130587204207, 0.5288733080144123, 0.5314052001168565, 0.5276073619631901, 0.5337423312883436, 0.5315025805823352, 0.5329632875645146, 0.5311130587204207, 0.5323790047716428, 0.5329632875645146, 0.5298471126691986, 0.5321842438406855, 0.530236634531113, 0.5277047424286688, 0.5307235368585062, 0.5319894829097284, 0.5342292336157367, 0.5301392540656344, 0.5325737657026001, 0.5316973415132924, 0.535203038270523, 0.5328659070990359, 0.5323790047716428, 0.5346187554776511, 0.5315999610478138, 0.538319213165839, 0.5335475703573863, 0.5325737657026001, 0.5351056578050443, 0.5362742233907878, 0.5310156782549421, 0.5326711461680786, 0.5355925601324374, 0.5332554289609505, 0.5345213750121726, 0.5334501898919077, 0.5311130587204207, 0.5368585061836596, 0.5327685266335573, 0.5365663647872236, 0.5337423312883436, 0.5357873210633947, 0.5367611257181809, 0.5378323108384458, 0.5336449508228649, 0.5369558866491382, 0.5376375499074886, 0.5325737657026001, 0.5405589638718473, 0.538027071769403, 0.5351056578050443, 0.5343266140812153, 0.5370532671146168, 0.5358847015288734, 0.5347161359431298, 0.5356899405979161, 0.5384165936313176, 0.5366637452527023, 0.5363716038562665, 0.5365663647872236, 0.5394877787515824, 0.5384165936313176, 0.538319213165839, 0.5361768429253092, 0.5376375499074886, 0.5388061154932321, 0.5389034959587107, 0.5377349303729672, 0.5384165936313176, 0.5405589638718473, 0.5378323108384458, 0.5377349303729672, 0.54036420294089, 0.5392930178206252, 0.5370532671146168, 0.5364689843217451, 0.5390008764241893, 0.5402668224754115, 0.5378323108384458, 0.539098256889668, 0.5437725192326419, 0.5388061154932321, 0.5394877787515824, 0.5391956373551465, 0.5397799201480183, 0.5399746810789755, 0.5410458661992404, 0.5392930178206252, 0.5408511052682832, 0.5390008764241893, 0.5412406271301977, 0.5396825396825397, 0.5379296913039244, 0.5409484857337618, 0.5433829973707275, 0.5421170513195053, 0.5398773006134969, 0.5415327685266336, 0.5419222903885481, 0.5404615834063686, 0.5418249099230694, 0.539098256889668, 0.5426039536468984, 0.541143246664719, 0.5428960950433344, 0.5413380075956763, 0.5455253676112571, 0.5451358457493427, 0.5435777583016846, 0.541435388061155, 0.5447463238874282, 0.5454279871457786, 0.5423118122504625, 0.5457201285422144, 0.5436751387671633, 0.5419222903885481, 0.542214431784984, 0.5413380075956763, 0.5435777583016846, 0.5413380075956763, 0.5437725192326419, 0.5446489434219496, 0.5445515629564709, 0.5423118122504625, 0.5427013341123771, 0.5446489434219496, 0.5408511052682832, 0.5446489434219496, 0.5436751387671633, 0.5464991722660434, 0.5478624987827442, 0.5462070308696075, 0.5458175090076931, 0.5455253676112571, 0.5453306066803, 0.5465965527315221, 0.5463044113350862, 0.5505891518161456, 0.5484467815756159, 0.5476677378517869, 0.5471808355243938, 0.5482520206446587, 0.5453306066803, 0.5444541824909923, 0.5476677378517869, 0.5453306066803, 0.5460122699386503, 0.5485441620410946, 0.5452332262148213, 0.5510760541435388, 0.5465965527315221, 0.5485441620410946, 0.5467913136624794, 0.5485441620410946, 0.5428960950433344, 0.5474729769208296, 0.5459148894731717, 0.5463044113350862, 0.5486415425065732, 0.5486415425065732, 0.549225825299445, 0.5464017918005648, 0.5501022494887525, 0.548933683903009, 0.551270815074496, 0.5480572597137015, 0.5446489434219496, 0.5463044113350862, 0.547375596455351, 0.5501996299542312, 0.5493232057649236, 0.5503943908851884, 0.5499074885577953, 0.5522446197292823, 0.5533158048495472, 0.5508812932125815, 0.5509786736780602, 0.5496153471613594, 0.5470834550589152, 0.550004869023274, 0.5506865322816243, 0.5499074885577953, 0.5524393806602396, 0.5510760541435388, 0.550004869023274, 0.5487389229720518, 0.5524393806602396, 0.5516603369364106, 0.552049858798325, 0.5517577174018892, 0.5496153471613594, 0.5482520206446587, 0.551270815074496, 0.5499074885577953, 0.5539000876424189, 0.5504917713506671, 0.5532184243840685, 0.5540948485733762, 0.5513681955399746, 0.5506865322816243, 0.5545817509007693, 0.5529262829876327, 0.5532184243840685, 0.552049858798325, 0.5542896095043335, 0.5528289025221541, 0.5537053267114617, 0.552049858798325, 0.5480572597137015, 0.5544843704352906, 0.5513681955399746, 0.5488363034375304, 0.5556529360210342, 0.551270815074496, 0.5554581750900769, 0.5511734346090175, 0.5561398383484273, 0.5498101080923167, 0.5528289025221541, 0.556237218813906, 0.5583795890544356, 0.5517577174018892, 0.5540948485733762, 0.5580874476579998, 0.5560424578829487, 0.5540948485733762, 0.5564319797448631, 0.5539000876424189, 0.5550686532281625, 0.5547765118317266, 0.5519524783328464, 0.5544843704352906, 0.5571136430032135, 0.5552634141591197, 0.5550686532281625, 0.5558476969519914, 0.55594507741747, 0.5575031648651281, 0.5581848281234785, 0.5542896095043335, 0.5567241211412991, 0.555166033693641, 0.5558476969519914, 0.5517577174018892, 0.557016262537735, 0.5545817509007693, 0.5530236634531113, 0.5552634141591197, 0.557016262537735, 0.5549712727626838, 0.5550686532281625, 0.5606193397604441, 0.5595481546401792, 0.5572110234686922, 0.5569188820722563, 0.5592560132437433, 0.5573084039341708, 0.5564319797448631, 0.5571136430032135, 0.558282208588957, 0.5588664913818288, 0.5555555555555556, 0.5606193397604441, 0.559061252312786, 0.5585743499853929, 0.5552634141591197, 0.5600350569675723, 0.5591586327782647, 0.5602298178985295, 0.5576005453306067, 0.5576005453306067, 0.560132437433051, 0.5611062420878372, 0.5567241211412991, 0.5576979257960853, 0.5606193397604441, 0.5602298178985295, 0.5564319797448631, 0.5596455351056578, 0.5584769695199143, 0.5647093193105464, 0.5602298178985295, 0.5596455351056578, 0.5589638718473074, 0.5619826662771448, 0.559840296036615, 0.5626643295354952, 0.5596455351056578, 0.560132437433051, 0.5633459927938456, 0.5596455351056578, 0.5607167202259227, 0.5602298178985295, 0.5620800467426235, 0.5620800467426235, 0.5619826662771448, 0.5622748076735807, 0.5656831239653326, 0.563248612328367, 0.5608141006914013, 0.5580874476579998, 0.564027656052196, 0.5613983834842731, 0.564027656052196, 0.5611062420878372, 0.5625669490700166, 0.5643197974486318, 0.5652936021034181, 0.5630538513974097, 0.5618852858116662, 0.562177427208102, 0.5609114811568799, 0.5600350569675723, 0.5613983834842731, 0.5600350569675723, 0.5646119388450677, 0.5650014607069822, 0.5622748076735807, 0.5639302755867173, 0.564027656052196, 0.5650988411724608, 0.5653909825688966, 0.5637355146557601, 0.5639302755867173, 0.5627617100009739, 0.5647093193105464, 0.5634433732593241, 0.5617879053461875, 0.563248612328367, 0.5647093193105464, 0.5641250365176745, 0.5641250365176745, 0.5651962216379395, 0.5649040802415035, 0.5638328951212387, 0.567143830947512, 0.566851689551076, 0.5634433732593241, 0.5662674067582043, 0.564027656052196, 0.5663647872236829, 0.5616905248807089, 0.5685071574642127, 0.5626643295354952, 0.5661700262927257, 0.5677281137403837, 0.567922874671341, 0.562956470931931, 0.5669490700165547, 0.5648066997760249, 0.5650988411724608, 0.5662674067582043, 0.5670464504820333, 0.5678254942058623, 0.5655857434998539, 0.5706495277047424, 0.5650014607069822, 0.5673385918784691, 0.569967864446392, 0.5686045379296913, 0.5685071574642127, 0.5672412114129906, 0.5647093193105464, 0.5694809621189989, 0.5688966793261272, 0.564027656052196, 0.5659752653617685, 0.5692862011880416, 0.5683123965332554, 0.5716233323595287, 0.5667543090855974, 0.570746908170221, 0.5688966793261272, 0.5704547667737851, 0.569967864446392, 0.5687019183951699, 0.5683123965332554, 0.5661700262927257, 0.5692862011880416, 0.5678254942058623, 0.5698704839809134, 0.5701626253773493, 0.5678254942058623, 0.5688966793261272, 0.5713311909630928, 0.5684097769987341, 0.5692862011880416, 0.5696757230499562, 0.5695783425844776, 0.5715259518940501, 0.5683123965332554, 0.5670464504820333, 0.5688966793261272, 0.5676307332749051, 0.5733761807381439, 0.5681176356022982, 0.5675333528094264, 0.569967864446392, 0.5702600058428279, 0.5692862011880416, 0.5666569286201188, 0.5723049956178791, 0.569188820722563, 0.5717207128250072, 0.5727918979452722, 0.5726945174797936, 0.573863083065537, 0.5715259518940501, 0.5718180932904859, 0.5713311909630928, 0.5712338104976141, 0.5753237900477164, 0.5728892784107508, 0.5720128542214432, 0.5729866588762295, 0.574934268185802, 0.571039049566657, 0.5717207128250072, 0.5739604635310157, 0.5720128542214432, 0.5736683221345799, 0.5731814198071867, 0.5750316486512805, 0.5720128542214432, 0.5736683221345799, 0.5719154737559645, 0.5728892784107508, 0.5773687798227676, 0.5720128542214432, 0.5745447463238874, 0.569967864446392, 0.5772713993572889, 0.5731814198071867, 0.573863083065537, 0.5728892784107508, 0.574934268185802, 0.5725971370143149, 0.5733761807381439, 0.5757133119096309, 0.5727918979452722, 0.5728892784107508, 0.5772713993572889, 0.5724997565488363, 0.5725971370143149, 0.5762975947025026, 0.574155224461973, 0.5750316486512805, 0.5763949751679813, 0.5758106923751095, 0.5727918979452722, 0.5748368877203233, 0.5763949751679813, 0.576979257960853, 0.5732788002726653, 0.5783425844775538, 0.5734735612036226, 0.5754211705131951, 0.5760054533060668, 0.5767844970298958, 0.5779530626156393, 0.5728892784107508, 0.5766871165644172, 0.5739604635310157, 0.5772713993572889, 0.5745447463238874, 0.5772713993572889, 0.5765897360989386, 0.5801928133216476, 0.5743499853929301, 0.5781478235465966, 0.5767844970298958, 0.5772713993572889, 0.5767844970298958, 0.5767844970298958, 0.5811666179764339, 0.5747395072548447, 0.5782452040120751, 0.5783425844775538, 0.5762975947025026, 0.5761028337715455, 0.5781478235465966, 0.5783425844775538, 0.5793163891323401, 0.5767844970298958, 0.5771740188918103, 0.5765897360989386, 0.5763949751679813, 0.5760054533060668, 0.5817509007693057, 0.5794137695978187, 0.5763949751679813, 0.5792190086668614, 0.5778556821501607, 0.5812639984419126, 0.5784399649430324, 0.578829486804947, 0.5771740188918103, 0.5784399649430324, 0.5771740188918103, 0.5742526049274516, 0.5811666179764339, 0.5782452040120751, 0.581653520303827, 0.5791216282013828, 0.5802901937871263, 0.5790242477359042, 0.5798032914597332, 0.5796085305287759, 0.5812639984419126, 0.5799980523906905, 0.5781478235465966, 0.5801928133216476, 0.5827247054240919, 0.5789268672704255, 0.5814587593728698, 0.5771740188918103, 0.5792190086668614, 0.5799006719252118, 0.5827247054240919, 0.5809718570454767, 0.5826273249586134, 0.5815561398383484, 0.581945661700263, 0.5833089882169636, 0.5834063686824423, 0.5822378030966988, 0.5814587593728698, 0.584769695199143, 0.5836011296133996, 0.5809718570454767, 0.5856461193884507, 0.5813613789073911, 0.5812639984419126, 0.5832116077514851, 0.5846723147336644, 0.5828220858895705, 0.5825299444931347, 0.5783425844775538, 0.5821404226312201, 0.5832116077514851, 0.5833089882169636, 0.5787321063394683, 0.581653520303827, 0.5810692375109553, 0.5828220858895705, 0.5831142272860065, 0.5862304021813224, 0.5818482812347843, 0.5851592170610576, 0.5837958905443568, 0.5830168468205278, 0.5872042068361086, 0.5853539779920148, 0.5815561398383484, 0.5882753919563736, 0.5873015873015873, 0.5853539779920148, 0.5840880319407927, 0.5846723147336644, 0.5828220858895705, 0.5875937286980232, 0.585840880319408, 0.5830168468205278, 0.583990651475314, 0.584769695199143, 0.5856461193884507, 0.5909046645242965, 0.584769695199143, 0.5840880319407927, 0.5836011296133996, 0.5855487389229721, 0.5856461193884507, 0.5853539779920148, 0.5836985100788782, 0.5859382607848865, 0.5863277826468011, 0.5868146849741942, 0.5885675333528094, 0.5866199240432369, 0.5874963482325446, 0.5881780114908949, 0.5874963482325446, 0.5865225435777583, 0.5843801733372286, 0.5851592170610576, 0.5825299444931347, 0.588664913818288, 0.5857434998539293, 0.5880806310254163, 0.5849644561301003, 0.5814587593728698, 0.5842827928717499, 0.5882753919563736]
Validation accuracy: [0.29595015576323985, 0.29361370716510904, 0.2967289719626168, 0.29205607476635514, 0.29127725856697817, 0.29283489096573206, 0.2897196261682243, 0.29205607476635514, 0.2897196261682243, 0.28738317757009346, 0.29361370716510904, 0.29283489096573206, 0.30062305295950154, 0.2998442367601246, 0.29439252336448596, 0.28894080996884736, 0.29127725856697817, 0.2866043613707165, 0.29049844236760125, 0.29205607476635514, 0.29595015576323985, 0.29205607476635514, 0.29906542056074764, 0.29906542056074764, 0.30218068535825543, 0.2982866043613707, 0.29205607476635514, 0.29283489096573206, 0.2967289719626168, 0.29049844236760125, 0.29439252336448596, 0.29205607476635514, 0.28894080996884736, 0.29205607476635514, 0.2967289719626168, 0.29517133956386293, 0.29750778816199375, 0.29361370716510904, 0.29127725856697817, 0.29595015576323985, 0.29517133956386293, 0.28738317757009346, 0.2897196261682243, 0.29205607476635514, 0.29595015576323985, 0.30062305295950154, 0.28738317757009346, 0.30218068535825543, 0.29361370716510904, 0.29906542056074764, 0.29127725856697817, 0.29750778816199375, 0.28738317757009346, 0.29361370716510904, 0.29750778816199375, 0.29361370716510904, 0.29127725856697817, 0.29283489096573206, 0.29205607476635514, 0.29439252336448596, 0.29517133956386293, 0.29361370716510904, 0.29517133956386293, 0.30218068535825543, 0.30218068535825543, 0.29595015576323985, 0.29049844236760125, 0.29906542056074764, 0.30062305295950154, 0.29439252336448596, 0.29127725856697817, 0.28738317757009346, 0.29517133956386293, 0.29906542056074764, 0.2967289719626168, 0.2834890965732087, 0.29439252336448596, 0.29595015576323985, 0.29283489096573206, 0.2967289719626168, 0.2982866043613707, 0.29750778816199375, 0.2967289719626168, 0.2897196261682243, 0.2850467289719626, 0.29439252336448596, 0.29049844236760125, 0.2881619937694704, 0.29595015576323985, 0.29049844236760125, 0.28738317757009346, 0.2982866043613707, 0.29517133956386293, 0.29361370716510904, 0.29361370716510904, 0.29517133956386293, 0.29283489096573206, 0.2850467289719626, 0.29595015576323985, 0.2881619937694704, 0.29517133956386293, 0.29439252336448596, 0.29283489096573206, 0.29595015576323985, 0.28738317757009346, 0.29439252336448596, 0.2967289719626168, 0.29361370716510904, 0.29283489096573206, 0.3029595015576324, 0.29205607476635514, 0.29049844236760125, 0.2897196261682243, 0.29127725856697817, 0.2967289719626168, 0.29283489096573206, 0.2897196261682243, 0.29283489096573206, 0.29595015576323985, 0.29517133956386293, 0.29595015576323985, 0.29439252336448596, 0.29595015576323985, 0.29517133956386293, 0.2881619937694704, 0.29906542056074764, 0.29205607476635514, 0.29517133956386293, 0.2842679127725857, 0.2850467289719626, 0.29750778816199375, 0.29439252336448596, 0.29127725856697817, 0.29439252336448596, 0.29595015576323985, 0.29283489096573206, 0.30218068535825543, 0.29439252336448596, 0.2982866043613707, 0.29750778816199375, 0.28894080996884736, 0.2881619937694704, 0.2881619937694704, 0.29205607476635514, 0.2982866043613707, 0.29439252336448596, 0.2982866043613707, 0.2982866043613707, 0.29205607476635514, 0.29361370716510904, 0.28894080996884736, 0.29049844236760125, 0.2967289719626168, 0.2881619937694704, 0.30062305295950154, 0.2897196261682243, 0.29205607476635514, 0.29205607476635514, 0.29595015576323985, 0.29049844236760125, 0.29283489096573206, 0.29205607476635514, 0.2881619937694704, 0.2967289719626168, 0.29439252336448596, 0.29439252336448596, 0.29127725856697817, 0.2897196261682243, 0.3014018691588785, 0.2897196261682243, 0.29049844236760125, 0.29283489096573206, 0.28894080996884736, 0.28582554517133957, 0.29127725856697817, 0.29906542056074764, 0.29595015576323985, 0.2881619937694704, 0.29049844236760125, 0.28582554517133957, 0.29361370716510904, 0.29439252336448596, 0.29127725856697817, 0.2842679127725857, 0.29439252336448596, 0.29517133956386293, 0.2982866043613707, 0.29361370716510904, 0.29127725856697817, 0.29361370716510904, 0.29127725856697817, 0.2982866043613707, 0.29049844236760125, 0.29127725856697817, 0.2850467289719626, 0.28894080996884736, 0.29205607476635514, 0.29750778816199375, 0.29283489096573206, 0.29361370716510904, 0.29205607476635514, 0.29517133956386293, 0.29517133956386293, 0.29283489096573206, 0.29517133956386293, 0.30062305295950154, 0.29517133956386293, 0.29595015576323985, 0.29517133956386293, 0.29205607476635514, 0.29127725856697817, 0.29283489096573206, 0.29361370716510904, 0.29517133956386293, 0.29361370716510904, 0.29127725856697817, 0.28738317757009346, 0.29439252336448596, 0.29750778816199375, 0.2897196261682243, 0.28894080996884736, 0.29361370716510904, 0.2897196261682243, 0.29439252336448596, 0.29517133956386293, 0.29439252336448596, 0.2967289719626168, 0.2881619937694704, 0.2866043613707165, 0.29517133956386293, 0.29049844236760125, 0.28894080996884736, 0.2897196261682243, 0.29283489096573206, 0.29595015576323985, 0.29283489096573206, 0.29517133956386293, 0.29750778816199375, 0.29283489096573206, 0.2967289719626168, 0.29439252336448596, 0.29127725856697817, 0.29205607476635514, 0.2967289719626168, 0.29439252336448596, 0.29049844236760125, 0.2967289719626168, 0.2866043613707165, 0.29750778816199375, 0.29049844236760125, 0.2897196261682243, 0.2897196261682243, 0.2967289719626168, 0.29361370716510904, 0.28894080996884736, 0.29361370716510904, 0.29750778816199375, 0.29439252336448596, 0.29127725856697817, 0.29283489096573206, 0.29127725856697817, 0.29049844236760125, 0.29517133956386293, 0.29439252336448596, 0.29283489096573206, 0.29595015576323985, 0.28582554517133957, 0.29205607476635514, 0.2897196261682243, 0.29439252336448596, 0.2897196261682243, 0.2897196261682243, 0.29283489096573206, 0.28894080996884736, 0.29283489096573206, 0.2967289719626168, 0.29283489096573206, 0.29517133956386293, 0.29049844236760125, 0.28894080996884736, 0.29517133956386293, 0.29283489096573206, 0.29750778816199375, 0.2850467289719626, 0.29439252336448596, 0.29595015576323985, 0.28582554517133957, 0.29439252336448596, 0.2881619937694704, 0.2827102803738318, 0.29439252336448596, 0.29205607476635514, 0.2803738317757009, 0.29595015576323985, 0.2967289719626168, 0.29517133956386293, 0.29750778816199375, 0.29361370716510904, 0.29127725856697817, 0.29283489096573206, 0.29127725856697817, 0.29049844236760125, 0.28582554517133957, 0.28894080996884736, 0.28894080996884736, 0.2897196261682243, 0.28738317757009346, 0.29906542056074764, 0.29205607476635514, 0.28582554517133957, 0.28894080996884736, 0.29595015576323985, 0.29127725856697817, 0.28738317757009346, 0.29205607476635514, 0.2842679127725857, 0.29127725856697817, 0.29361370716510904, 0.29205607476635514, 0.29439252336448596, 0.2897196261682243, 0.2842679127725857, 0.29750778816199375, 0.2866043613707165, 0.2850467289719626, 0.2897196261682243, 0.28738317757009346, 0.29049844236760125, 0.29361370716510904, 0.28894080996884736, 0.29283489096573206, 0.2897196261682243, 0.2897196261682243, 0.29127725856697817, 0.2866043613707165, 0.29049844236760125, 0.2866043613707165, 0.29127725856697817, 0.2897196261682243, 0.2834890965732087, 0.29205607476635514, 0.29361370716510904, 0.2834890965732087, 0.2897196261682243, 0.28738317757009346, 0.28738317757009346, 0.29361370716510904, 0.28738317757009346, 0.28738317757009346, 0.2834890965732087, 0.28582554517133957, 0.28894080996884736, 0.29049844236760125, 0.2881619937694704, 0.2866043613707165, 0.2827102803738318, 0.2897196261682243, 0.2897196261682243, 0.2897196261682243, 0.29517133956386293, 0.2897196261682243, 0.29517133956386293, 0.2827102803738318, 0.2897196261682243, 0.2850467289719626, 0.28894080996884736, 0.2866043613707165, 0.2881619937694704, 0.29127725856697817, 0.28894080996884736, 0.2866043613707165, 0.29127725856697817, 0.2897196261682243, 0.2897196261682243, 0.29361370716510904, 0.2881619937694704, 0.28738317757009346, 0.2881619937694704, 0.29750778816199375, 0.28582554517133957, 0.2881619937694704, 0.29049844236760125, 0.2866043613707165, 0.2897196261682243, 0.29517133956386293, 0.2842679127725857, 0.29283489096573206, 0.29127725856697817, 0.29283489096573206, 0.29127725856697817, 0.29127725856697817, 0.29283489096573206, 0.2819314641744548, 0.29283489096573206, 0.29283489096573206, 0.29127725856697817, 0.29439252336448596, 0.28582554517133957, 0.2866043613707165, 0.28894080996884736, 0.29127725856697817, 0.29205607476635514, 0.2866043613707165, 0.28738317757009346, 0.28894080996884736, 0.2827102803738318, 0.29049844236760125, 0.2866043613707165, 0.28894080996884736, 0.29127725856697817, 0.28894080996884736, 0.28894080996884736, 0.28738317757009346, 0.29205607476635514, 0.29127725856697817, 0.2866043613707165, 0.29439252336448596, 0.28894080996884736, 0.2866043613707165, 0.2866043613707165, 0.28582554517133957, 0.28894080996884736, 0.28738317757009346, 0.29205607476635514, 0.2850467289719626, 0.28894080996884736, 0.2866043613707165, 0.28738317757009346, 0.2827102803738318, 0.29283489096573206, 0.28738317757009346, 0.2897196261682243, 0.28894080996884736, 0.2897196261682243, 0.2842679127725857, 0.2881619937694704, 0.2897196261682243, 0.28894080996884736, 0.2827102803738318, 0.2881619937694704, 0.29361370716510904, 0.2881619937694704, 0.28738317757009346, 0.29205607476635514, 0.28738317757009346, 0.28894080996884736, 0.2842679127725857, 0.2803738317757009, 0.2842679127725857, 0.2827102803738318, 0.29127725856697817, 0.28894080996884736, 0.28894080996884736, 0.29283489096573206, 0.2842679127725857, 0.2842679127725857, 0.2834890965732087, 0.28738317757009346, 0.28582554517133957, 0.2850467289719626, 0.28738317757009346, 0.28894080996884736, 0.2881619937694704, 0.28582554517133957, 0.2803738317757009, 0.2842679127725857, 0.2834890965732087, 0.29205607476635514, 0.29283489096573206, 0.28894080996884736, 0.28582554517133957, 0.2834890965732087, 0.28582554517133957, 0.279595015576324, 0.2850467289719626, 0.2881619937694704, 0.28894080996884736, 0.28738317757009346, 0.28582554517133957, 0.2866043613707165, 0.29361370716510904, 0.28894080996884736, 0.29205607476635514, 0.29127725856697817, 0.29049844236760125, 0.2897196261682243, 0.2834890965732087, 0.2842679127725857, 0.28582554517133957, 0.2819314641744548, 0.29127725856697817, 0.2834890965732087, 0.2850467289719626, 0.29127725856697817, 0.2881619937694704, 0.28582554517133957, 0.2811526479750779, 0.2819314641744548, 0.29439252336448596, 0.2866043613707165, 0.28582554517133957, 0.28738317757009346, 0.29049844236760125, 0.2772585669781931, 0.29439252336448596, 0.2811526479750779, 0.29283489096573206, 0.2897196261682243, 0.29361370716510904, 0.2850467289719626, 0.28738317757009346, 0.29283489096573206, 0.2850467289719626, 0.2881619937694704, 0.2866043613707165, 0.2881619937694704, 0.28738317757009346, 0.28738317757009346, 0.28582554517133957, 0.29361370716510904, 0.29127725856697817, 0.29049844236760125, 0.29361370716510904, 0.29517133956386293, 0.28894080996884736, 0.28738317757009346, 0.29127725856697817, 0.28738317757009346, 0.29205607476635514, 0.28582554517133957, 0.28738317757009346, 0.28582554517133957, 0.29283489096573206, 0.2780373831775701, 0.2850467289719626, 0.28894080996884736, 0.29127725856697817, 0.2803738317757009, 0.28894080996884736, 0.28582554517133957, 0.2866043613707165, 0.28582554517133957, 0.29205607476635514, 0.29127725856697817, 0.2866043613707165, 0.29205607476635514, 0.29127725856697817, 0.29049844236760125, 0.2819314641744548, 0.28894080996884736, 0.2834890965732087, 0.3014018691588785, 0.2834890965732087, 0.2866043613707165, 0.28582554517133957, 0.28582554517133957, 0.2881619937694704, 0.28582554517133957, 0.2881619937694704, 0.29127725856697817, 0.2866043613707165, 0.2881619937694704, 0.29049844236760125, 0.28582554517133957, 0.2819314641744548, 0.28894080996884736, 0.29049844236760125, 0.2842679127725857, 0.28894080996884736, 0.2897196261682243, 0.2866043613707165, 0.28738317757009346, 0.2819314641744548, 0.29127725856697817, 0.28738317757009346, 0.2834890965732087, 0.28582554517133957, 0.29361370716510904, 0.29361370716510904, 0.2827102803738318, 0.2897196261682243, 0.2842679127725857, 0.2850467289719626, 0.29283489096573206, 0.29049844236760125, 0.28894080996884736, 0.29049844236760125, 0.29049844236760125, 0.2834890965732087, 0.2811526479750779, 0.2850467289719626, 0.29205607476635514, 0.2803738317757009, 0.2866043613707165, 0.29205607476635514, 0.29127725856697817, 0.29439252336448596, 0.28894080996884736, 0.2897196261682243, 0.278816199376947, 0.2827102803738318, 0.2881619937694704, 0.2881619937694704, 0.2842679127725857, 0.28894080996884736, 0.28894080996884736, 0.2819314641744548, 0.2866043613707165, 0.29127725856697817, 0.28894080996884736, 0.2881619937694704, 0.2819314641744548, 0.29205607476635514, 0.2850467289719626, 0.2834890965732087, 0.29517133956386293, 0.2842679127725857, 0.29283489096573206, 0.29283489096573206, 0.28738317757009346, 0.2866043613707165, 0.28582554517133957, 0.2866043613707165, 0.29049844236760125, 0.28738317757009346, 0.2881619937694704, 0.28894080996884736, 0.29049844236760125, 0.28582554517133957, 0.2819314641744548, 0.2842679127725857, 0.28738317757009346, 0.29205607476635514, 0.29283489096573206, 0.2819314641744548, 0.29049844236760125, 0.2881619937694704, 0.2881619937694704, 0.2897196261682243, 0.29361370716510904, 0.28738317757009346, 0.28582554517133957, 0.2757009345794392, 0.29049844236760125, 0.28582554517133957, 0.28738317757009346, 0.2811526479750779, 0.28894080996884736, 0.2842679127725857, 0.28582554517133957, 0.2811526479750779, 0.2827102803738318, 0.28894080996884736, 0.28894080996884736, 0.29205607476635514, 0.2850467289719626, 0.29283489096573206, 0.2866043613707165, 0.28738317757009346, 0.2881619937694704, 0.28738317757009346, 0.28894080996884736, 0.28738317757009346, 0.29127725856697817, 0.2897196261682243, 0.29049844236760125, 0.2842679127725857, 0.29205607476635514, 0.28582554517133957, 0.2842679127725857, 0.2881619937694704, 0.2967289719626168, 0.29127725856697817, 0.2811526479750779, 0.2866043613707165, 0.29205607476635514, 0.2842679127725857, 0.2819314641744548, 0.2866043613707165, 0.2834890965732087, 0.2834890965732087, 0.2897196261682243, 0.279595015576324, 0.29283489096573206, 0.29049844236760125, 0.29127725856697817, 0.29439252336448596, 0.29439252336448596, 0.2842679127725857, 0.2866043613707165, 0.29049844236760125, 0.2811526479750779, 0.29049844236760125, 0.2897196261682243, 0.279595015576324, 0.2834890965732087, 0.29049844236760125, 0.2834890965732087, 0.28582554517133957, 0.28582554517133957, 0.28738317757009346, 0.2881619937694704, 0.29049844236760125, 0.28894080996884736, 0.2866043613707165, 0.29517133956386293, 0.2834890965732087, 0.2866043613707165, 0.2842679127725857, 0.2897196261682243, 0.2842679127725857, 0.2834890965732087, 0.2827102803738318, 0.2819314641744548, 0.29439252336448596, 0.28894080996884736, 0.2866043613707165, 0.2819314641744548, 0.279595015576324, 0.278816199376947, 0.28738317757009346, 0.29205607476635514, 0.2827102803738318, 0.2834890965732087, 0.2827102803738318, 0.28582554517133957, 0.2811526479750779, 0.29127725856697817, 0.28582554517133957, 0.28582554517133957, 0.2850467289719626, 0.2850467289719626, 0.28738317757009346, 0.2881619937694704, 0.29517133956386293, 0.28894080996884736, 0.2897196261682243, 0.29517133956386293, 0.2842679127725857, 0.28738317757009346, 0.28894080996884736, 0.2866043613707165, 0.28894080996884736, 0.2897196261682243, 0.278816199376947, 0.28894080996884736, 0.2803738317757009, 0.2866043613707165, 0.2803738317757009, 0.2866043613707165, 0.2834890965732087, 0.2850467289719626, 0.28894080996884736, 0.2850467289719626, 0.2850467289719626, 0.2842679127725857, 0.28738317757009346, 0.2850467289719626, 0.28738317757009346, 0.2897196261682243, 0.2850467289719626, 0.2819314641744548, 0.2881619937694704, 0.29049844236760125, 0.28582554517133957, 0.2897196261682243, 0.2897196261682243, 0.2850467289719626, 0.2866043613707165, 0.29205607476635514, 0.2827102803738318, 0.2834890965732087, 0.28582554517133957, 0.28582554517133957, 0.29127725856697817, 0.2811526479750779, 0.29049844236760125, 0.28738317757009346, 0.2850467289719626, 0.2850467289719626, 0.2834890965732087, 0.2850467289719626, 0.28738317757009346, 0.28582554517133957, 0.2881619937694704, 0.2811526479750779, 0.28894080996884736, 0.2842679127725857, 0.2811526479750779, 0.28582554517133957, 0.28582554517133957, 0.29439252336448596, 0.278816199376947, 0.2881619937694704, 0.29049844236760125, 0.2842679127725857, 0.29127725856697817, 0.2897196261682243, 0.2897196261682243, 0.2834890965732087, 0.2803738317757009, 0.2834890965732087, 0.29283489096573206, 0.2850467289719626, 0.29283489096573206, 0.28582554517133957, 0.28582554517133957, 0.2827102803738318, 0.2866043613707165, 0.2819314641744548, 0.2834890965732087, 0.2866043613707165, 0.2819314641744548, 0.2866043613707165, 0.2811526479750779, 0.28582554517133957, 0.2881619937694704, 0.2803738317757009, 0.28894080996884736, 0.2866043613707165, 0.28738317757009346, 0.28582554517133957, 0.2827102803738318, 0.2834890965732087, 0.279595015576324, 0.2811526479750779, 0.2850467289719626, 0.2819314641744548, 0.2827102803738318, 0.2850467289719626, 0.2757009345794392, 0.2850467289719626, 0.28894080996884736, 0.28738317757009346, 0.29049844236760125, 0.2749221183800623, 0.2881619937694704, 0.2819314641744548, 0.2834890965732087, 0.2811526479750779, 0.2827102803738318, 0.2842679127725857, 0.29049844236760125, 0.2811526479750779, 0.2850467289719626, 0.2819314641744548, 0.28738317757009346, 0.29127725856697817, 0.2834890965732087, 0.2827102803738318, 0.2897196261682243, 0.2827102803738318, 0.28894080996884736, 0.2842679127725857, 0.2811526479750779, 0.2827102803738318, 0.2772585669781931, 0.2842679127725857, 0.2811526479750779, 0.2811526479750779, 0.28738317757009346, 0.2850467289719626, 0.2780373831775701, 0.2811526479750779, 0.2803738317757009, 0.2780373831775701, 0.2866043613707165, 0.2881619937694704, 0.28894080996884736, 0.2842679127725857, 0.29283489096573206, 0.279595015576324, 0.27414330218068533, 0.28738317757009346, 0.28582554517133957, 0.279595015576324, 0.2819314641744548, 0.278816199376947, 0.28582554517133957, 0.28582554517133957, 0.29205607476635514, 0.278816199376947, 0.2834890965732087, 0.2811526479750779, 0.279595015576324, 0.29049844236760125, 0.2850467289719626, 0.28738317757009346, 0.2850467289719626, 0.2819314641744548, 0.2842679127725857, 0.2811526479750779, 0.2842679127725857, 0.2866043613707165, 0.2819314641744548, 0.28582554517133957, 0.2850467289719626, 0.2827102803738318, 0.2819314641744548, 0.28582554517133957, 0.2866043613707165, 0.2897196261682243, 0.28582554517133957, 0.29049844236760125, 0.2811526479750779, 0.2803738317757009, 0.2803738317757009, 0.2811526479750779, 0.2834890965732087, 0.28894080996884736, 0.2827102803738318, 0.2749221183800623, 0.2811526479750779, 0.28738317757009346, 0.2772585669781931, 0.29049844236760125, 0.279595015576324, 0.2811526479750779, 0.2780373831775701, 0.28582554517133957, 0.2834890965732087, 0.28738317757009346, 0.2850467289719626, 0.2811526479750779, 0.2819314641744548, 0.2842679127725857, 0.2897196261682243, 0.28738317757009346, 0.2850467289719626, 0.2819314641744548, 0.28582554517133957, 0.27414330218068533, 0.2834890965732087, 0.28582554517133957, 0.2834890965732087, 0.2850467289719626, 0.2811526479750779, 0.2842679127725857, 0.2850467289719626, 0.2819314641744548, 0.2827102803738318, 0.279595015576324, 0.2780373831775701, 0.2772585669781931, 0.2819314641744548, 0.28738317757009346, 0.2881619937694704, 0.2850467289719626, 0.2834890965732087, 0.278816199376947, 0.2803738317757009, 0.28582554517133957, 0.2772585669781931, 0.2897196261682243, 0.2842679127725857, 0.28738317757009346, 0.28738317757009346, 0.2780373831775701, 0.2850467289719626, 0.2819314641744548, 0.278816199376947, 0.2811526479750779, 0.2819314641744548, 0.2803738317757009, 0.2897196261682243, 0.2827102803738318, 0.2834890965732087, 0.2834890965732087, 0.278816199376947, 0.2850467289719626, 0.28582554517133957, 0.279595015576324, 0.2850467289719626, 0.2897196261682243, 0.2803738317757009, 0.2827102803738318, 0.28582554517133957, 0.2819314641744548, 0.2827102803738318, 0.2834890965732087, 0.2811526479750779, 0.2811526479750779, 0.2842679127725857, 0.279595015576324, 0.2842679127725857, 0.2803738317757009, 0.2842679127725857, 0.278816199376947, 0.29127725856697817, 0.2803738317757009, 0.2811526479750779, 0.2811526479750779, 0.2866043613707165, 0.2850467289719626, 0.28582554517133957, 0.2764797507788162, 0.2803738317757009, 0.29049844236760125, 0.2866043613707165, 0.2866043613707165]
Accuracy plot saved at 'now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572/accuracy_now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572.png'
Loss plot saved at 'now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572/loss_now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572.png'
Checkpoint saved at 'now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572/checkpoint_now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572.pth'
Best checkpoint saved at 'now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572/best_checkpoint_now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572.pth'
Output logfile saved at now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572/output_log_now_really_EXP2contd_Llama-7b_FULL_SimpleLinearHead_1710514184.3864572.txt
